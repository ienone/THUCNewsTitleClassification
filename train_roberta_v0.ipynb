{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c9c81ff-102e-42d0-8126-3ce4c6e65131",
   "metadata": {},
   "source": [
    "# 中文新闻文本标题分类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8eceb6-4ca9-4100-b137-5125eaea3c58",
   "metadata": {},
   "source": [
    "## 环境准备、路径与超参数总配置 (Global Configuration: Environment, Paths & Hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4afef16-c2e7-47f6-b304-3b12e21ad507",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 16:52:41,917 - __main__ - INFO - 环境准备与日志配置完成。\n",
      "2025-05-18 16:52:41,918 - __main__ - INFO - 文本日志将保存到: ./files/logs/training_log_20250518_165241.log\n",
      "2025-05-18 16:52:41,919 - __main__ - INFO - TensorBoard 日志将保存到: ./files/logs/tensorboard_runs/20250518-165241\n",
      "2025-05-18 16:52:41,919 - __main__ - INFO - PyTorch 版本: 2.5.1+cu124\n",
      "2025-05-18 16:52:41,919 - __main__ - INFO - Transformers 版本: 4.51.3\n",
      "2025-05-18 16:52:41,920 - __main__ - INFO - Datasets 版本: 3.3.2\n",
      "2025-05-18 16:52:41,920 - __main__ - INFO - Evaluate 版本: 0.4.3\n",
      "2025-05-18 16:52:41,943 - __main__ - INFO - 将使用设备: cuda\n",
      "2025-05-18 16:52:41,960 - __main__ - INFO - CUDA 可用. GPU数量: 1\n",
      "2025-05-18 16:52:41,963 - __main__ - INFO -   GPU 0: NVIDIA GeForce RTX 3090\n",
      "2025-05-18 16:52:41,963 - __main__ - INFO - 当前默认GPU: NVIDIA GeForce RTX 3090\n",
      "2025-05-18 16:52:41,965 - __main__ - INFO - 所有配置参数定义完毕。\n",
      "2025-05-18 16:52:41,966 - __main__ - INFO - 文本日志文件: ./files/logs/training_log_20250518_165241.log\n",
      "2025-05-18 16:52:41,966 - __main__ - INFO - TensorBoard 日志目录: ./files/logs/tensorboard_runs/20250518-165241\n",
      "2025-05-18 16:52:41,966 - __main__ - INFO - 原始数据目录: ./files/raw_data\n",
      "2025-05-18 16:52:41,967 - __main__ - INFO -   增强数据文件: ./files/raw_data/train_add.txt\n",
      "2025-05-18 16:52:41,967 - __main__ - INFO - 缓存数据目录: ./files/processed_data_cache\n",
      "2025-05-18 16:52:41,967 - __main__ - INFO - 模型保存目录: ./files/saved_models\n",
      "2025-05-18 16:52:41,968 - __main__ - INFO -   初次最佳模型: ./files/saved_models/best_roberta_model.pt\n",
      "2025-05-18 16:52:41,968 - __main__ - INFO -   二次最佳模型: ./files/saved_models/best_augmented_model.pt\n",
      "2025-05-18 16:52:41,968 - __main__ - INFO - 最终预测结果目录: ./files/results\n",
      "2025-05-18 16:52:41,968 - __main__ - INFO -   最终预测JSON: ./files/results/final_test_top1_predictions.json\n",
      "2025-05-18 16:52:41,969 - __main__ - INFO -   最终预测TXT: ./files/results/result.txt\n",
      "2025-05-18 16:52:41,969 - __main__ - INFO - 初步分析结果目录: ./files/analysis_results\n",
      "2025-05-18 16:52:41,969 - __main__ - INFO -   初步分析JSON: ./files/analysis_results/all_test_analysis/test_top1_predictions.json\n",
      "2025-05-18 16:52:41,970 - __main__ - INFO - 最终预测分析图表/CSV目录: ./files/results/prediction_analysis_charts_csv\n",
      "2025-05-18 16:52:41,970 - __main__ - INFO -   最终分析图: ./files/results/prediction_analysis_charts_csv/final_test_prob_distribution.png\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 0. 导入库 (Import Libraries)\n",
    "# ==============================================================================\n",
    "import os\n",
    "import csv # 用于读写CSV文件\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import datasets\n",
    "import logging\n",
    "import evaluate\n",
    "import subprocess\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image \n",
    "from tqdm.auto import tqdm\n",
    "from datasets import Dataset \n",
    "from torch.optim import AdamW\n",
    "from datetime import datetime\n",
    "from functools import partial \n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. 基础环境与日志配置 (Basic Environment & Logging Setup)\n",
    "# ==============================================================================\n",
    "\n",
    "# --- 1.1 网络代理设置 (Network Proxy Setup) ---\n",
    "# autodl代理加速配置\n",
    "# try:\n",
    "#     result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True, check=True)\n",
    "#     output = result.stdout\n",
    "#     for line in output.splitlines():\n",
    "#         if '=' in line:\n",
    "#             var, value = line.split('=', 1)\n",
    "#             os.environ[var] = value\n",
    "#     print(\"代理环境变量设置成功。\") # 使用 print 因为 log 可能还未配置\n",
    "# except subprocess.CalledProcessError as e:\n",
    "#     print(f\"设置代理环境变量失败: {e}\")\n",
    "# except FileNotFoundError:\n",
    "#     print(\"无法执行 bash 命令，可能不在 Linux 环境或未安装 bash。\")\n",
    "\n",
    "# --- 1.2 日志配置 (Logging Configuration) ---\n",
    "LOG_DIR = \"./files/logs\"\n",
    "if not os.path.exists(LOG_DIR):\n",
    "    os.makedirs(LOG_DIR)\n",
    "\n",
    "# TensorBoard Writer - 保留时间戳以区分不同的运行\n",
    "TENSORBOARD_RUN_NAME = datetime.now().strftime(\"%Y%m%d-%H%M%S\") \n",
    "TENSORBOARD_LOG_DIR = os.path.join(LOG_DIR, \"tensorboard_runs\", TENSORBOARD_RUN_NAME)\n",
    "writer = SummaryWriter(TENSORBOARD_LOG_DIR)\n",
    "\n",
    "# 文件日志 \n",
    "LOG_FILE_NAME = f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "LOG_FILE_PATH = os.path.join(LOG_DIR, LOG_FILE_NAME)\n",
    "\n",
    "# 清除已存在的handlers，避免重复日志\n",
    "if logging.root.handlers:\n",
    "    for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "        handler.close()\n",
    "\n",
    "if not logging.getLogger().hasHandlers(): # 只在没有 handler 的时候配置\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', # 添加了 %(name)s\n",
    "        handlers=[\n",
    "            logging.FileHandler(LOG_FILE_PATH, mode='w', encoding='utf-8'), # mode='w' 每次覆盖\n",
    "            logging.StreamHandler() # 同时输出到控制台\n",
    "        ]\n",
    "    )\n",
    "log = logging.getLogger(__name__) # 获取一个logger实例\n",
    "\n",
    "log.info(\"环境准备与日志配置完成。\")\n",
    "log.info(f\"文本日志将保存到: {LOG_FILE_PATH}\")\n",
    "log.info(f\"TensorBoard 日志将保存到: {TENSORBOARD_LOG_DIR}\")\n",
    "\n",
    "\n",
    "# --- 1.3 版本与设备信息 (Version & Device Information) ---\n",
    "log.info(f\"PyTorch 版本: {torch.__version__}\")\n",
    "log.info(f\"Transformers 版本: {transformers.__version__}\")\n",
    "log.info(f\"Datasets 版本: {datasets.__version__}\") # 需要 import datasets\n",
    "log.info(f\"Evaluate 版本: {evaluate.__version__}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "log.info(f\"将使用设备: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    log.info(f\"CUDA 可用. GPU数量: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        log.info(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    log.info(f\"当前默认GPU: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "else:\n",
    "    log.info(\"CUDA 不可用, 将使用 CPU.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. 路径与文件名配置 (Paths & Filenames Configuration)\n",
    "# ==============================================================================\n",
    "# 确保主文件目录存在\n",
    "if not os.path.exists(\"./files\"):\n",
    "    os.makedirs(\"./files\")\n",
    "\n",
    "# --- 2.1 原始数据路径 (Raw Data Paths) ---\n",
    "RAW_DATA_DIR = \"./files/raw_data\"\n",
    "if not os.path.exists(RAW_DATA_DIR): \n",
    "    os.makedirs(RAW_DATA_DIR)\n",
    "    \n",
    "TRAIN_FILE_NAME = \"train.txt\"\n",
    "DEV_FILE_NAME = \"dev.txt\"\n",
    "TEST_FILE_NAME = \"test.txt\"\n",
    "TRAIN_ADD_FILE_NAME = \"train_add.txt\" # 用于数据增强的文件名\n",
    "\n",
    "# 完整路径\n",
    "TRAIN_FILE_PATH = os.path.join(RAW_DATA_DIR, TRAIN_FILE_NAME)\n",
    "DEV_FILE_PATH = os.path.join(RAW_DATA_DIR, DEV_FILE_NAME)\n",
    "TEST_FILE_PATH = os.path.join(RAW_DATA_DIR, TEST_FILE_NAME)\n",
    "AUG_TRAIN_FILE_PATH = os.path.join(RAW_DATA_DIR, TRAIN_ADD_FILE_NAME) # 增强数据文件路径\n",
    "\n",
    "# --- 2.2 预处理数据缓存路径 (Processed Data Cache Paths) ---\n",
    "CACHE_DATA_DIR = \"./files/processed_data_cache\"\n",
    "if not os.path.exists(CACHE_DATA_DIR): \n",
    "    os.makedirs(CACHE_DATA_DIR)\n",
    "    \n",
    "CACHE_TRAIN_PATH = os.path.join(CACHE_DATA_DIR, \"train_dataset_cache\")\n",
    "CACHE_DEV_PATH = os.path.join(CACHE_DATA_DIR, \"dev_dataset_cache\")\n",
    "CACHE_TEST_PATH = os.path.join(CACHE_DATA_DIR, \"test_dataset_cache\")\n",
    "CACHE_AUG_TRAIN_PATH = os.path.join(CACHE_DATA_DIR, \"aug_train_dataset_cache\") # 缓存增强数据\n",
    "\n",
    "# --- 2.3 模型保存路径 (Model Saving Paths) ---\n",
    "SAVED_MODELS_DIR = \"./files/saved_models\"\n",
    "if not os.path.exists(SAVED_MODELS_DIR): \n",
    "    os.makedirs(SAVED_MODELS_DIR)\n",
    "    \n",
    "BEST_MODEL_NAME = \"best_roberta_model.pt\" # 初次微调的最佳模型\n",
    "BEST_MODEL_PATH = os.path.join(SAVED_MODELS_DIR, BEST_MODEL_NAME)\n",
    "\n",
    "BEST_AUG_MODEL_NAME = \"best_augmented_model.pt\" # 二次微调后的最佳模型\n",
    "BEST_AUG_MODEL_PATH = os.path.join(SAVED_MODELS_DIR, BEST_AUG_MODEL_NAME)\n",
    "\n",
    "# --- 2.4 最终预测结果输出路径 (Final Prediction Results Output Paths) ---\n",
    "FINAL_RESULTS_DIR = \"./files/results\"\n",
    "if not os.path.exists(FINAL_RESULTS_DIR): \n",
    "    os.makedirs(FINAL_RESULTS_DIR)\n",
    "\n",
    "FINAL_TOP1_JSON_FILE_NAME = \"final_test_top1_predictions.json\" \n",
    "FINAL_RESULT_TXT_FILE_NAME = \"result.txt\"  \n",
    "\n",
    "FINAL_TOP1_JSON_PATH = os.path.join(FINAL_RESULTS_DIR, FINAL_TOP1_JSON_FILE_NAME)\n",
    "FINAL_RESULT_TXT_PATH = os.path.join(FINAL_RESULTS_DIR, FINAL_RESULT_TXT_FILE_NAME)\n",
    "\n",
    "# --- 2.5 初步分析结果输出路径 (Initial Analysis Results Output Paths - after 1st fine-tuning) ---\n",
    "ANALYSIS_DIR = \"./files/analysis_results\" \n",
    "if not os.path.exists(ANALYSIS_DIR): \n",
    "    os.makedirs(ANALYSIS_DIR)\n",
    "    \n",
    "TRAIN_ADD_FILE_PATH = AUG_TRAIN_FILE_PATH\n",
    "\n",
    "ALL_TEST_ANALYSIS_OUTPUT_DIR = os.path.join(ANALYSIS_DIR, \"all_test_analysis\")\n",
    "if not os.path.exists(ALL_TEST_ANALYSIS_OUTPUT_DIR): \n",
    "    os.makedirs(ALL_TEST_ANALYSIS_OUTPUT_DIR)\n",
    "    \n",
    "TOP1_JSON_FILE_NAME = \"test_top1_predictions.json\" \n",
    "PROB_DIST_PLOT_FILE_NAME = \"test_prob_distribution.png\" \n",
    "PROB_DIST_CSV_FILE_NAME = \"test_prob_distribution_data.csv\" \n",
    "\n",
    "TOP1_JSON_PATH = os.path.join(ALL_TEST_ANALYSIS_OUTPUT_DIR, TOP1_JSON_FILE_NAME)\n",
    "PROB_DIST_PLOT_PATH = os.path.join(ALL_TEST_ANALYSIS_OUTPUT_DIR, PROB_DIST_PLOT_FILE_NAME)\n",
    "PROB_DIST_CSV_PATH = os.path.join(ALL_TEST_ANALYSIS_OUTPUT_DIR, PROB_DIST_CSV_FILE_NAME)\n",
    "\n",
    "# --- 2.6 最终预测分析的图表/CSV输出路径 (Final Prediction Analysis Plot/CSV Output Paths) ---\n",
    "FINAL_ANALYSIS_SUBDIR_NAME = \"prediction_analysis_charts_csv\" # 子目录名\n",
    "FINAL_ANALYSIS_OUTPUT_PATH = os.path.join(FINAL_RESULTS_DIR, FINAL_ANALYSIS_SUBDIR_NAME)\n",
    "if not os.path.exists(FINAL_ANALYSIS_OUTPUT_PATH): \n",
    "    os.makedirs(FINAL_ANALYSIS_OUTPUT_PATH)\n",
    "\n",
    "FINAL_PROB_DIST_PLOT_FILE_NAME = \"final_test_prob_distribution.png\"  \n",
    "FINAL_PROB_DIST_CSV_FILE_NAME = \"final_test_prob_distribution_data.csv\" \n",
    "\n",
    "FINAL_PROB_DIST_PLOT_PATH = os.path.join(FINAL_ANALYSIS_OUTPUT_PATH, FINAL_PROB_DIST_PLOT_FILE_NAME)\n",
    "FINAL_PROB_DIST_CSV_PATH = os.path.join(FINAL_ANALYSIS_OUTPUT_PATH, FINAL_PROB_DIST_CSV_FILE_NAME)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. 模型与 Tokenizer 配置 (Model & Tokenizer Configuration)\n",
    "# ==============================================================================\n",
    "PRETRAINED_MODEL_NAME = 'hfl/chinese-roberta-wwm-ext-large'\n",
    "# 优先使用本地缓存。如果 LOCAL_MODEL_PATH 为 None、空字符串或无效路径，\n",
    "# AutoTokenizer/AutoModel.from_pretrained 会自动尝试从 PRETRAINED_MODEL_NAME 下载或使用Hub缓存。\n",
    "LOCAL_MODEL_PATH = '/root/.cache/huggingface/hub/models--hfl--chinese-roberta-wwm-ext-large/snapshots/a25cc9e05974bd9687e528edd516f2cfdb3f5db9'\n",
    "# 建议：可以检查 LOCAL_MODEL_PATH 是否有效，如果无效则设为 None\n",
    "if not (LOCAL_MODEL_PATH and os.path.exists(LOCAL_MODEL_PATH)):\n",
    "    log.warning(f\"配置的本地模型路径 '{LOCAL_MODEL_PATH}' 无效或不存在。将依赖HuggingFace Hub加载 '{PRETRAINED_MODEL_NAME}'。\")\n",
    "    LOCAL_MODEL_PATH = None # 设为None，让transformers库自动处理\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. 训练与数据处理超参数 (Training & Data Processing Hyperparameters)\n",
    "# ==============================================================================\n",
    "\n",
    "# --- 4.1 数据处理 (Data Processing) ---\n",
    "MAX_SEQ_LENGTH = 28  # 文本最大序列长度\n",
    "NUM_PROC_FOR_MAP = 12 # Dataset.map() 使用的并行进程数\n",
    "\n",
    "# --- 4.2 DataLoader ---\n",
    "BATCH_SIZE = 256\n",
    "#NUM_WORKERS = 6  # DataLoader 的 num_workers。Linux上可尝试增加, Windows或Jupyter中建议为0或较小值\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "# --- 4.3 训练过程 (Training Process) ---\n",
    "# 初次微调的超参数\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 6\n",
    "WARMUP_PROPORTION = 0.1\n",
    "WEIGHT_DECAY = 0.01\n",
    "EVAL_STRATEGY = \"steps\"             # \"steps\" 或 \"epoch\"\n",
    "EVAL_FREQUENCY_FRAC_EPOCH = 0.2     # 如果 EVAL_STRATEGY=\"steps\", 每训练X比例的epoch数据后验证\n",
    "EARLY_STOPPING_ENABLED = True       # 是否启用早停 (初次微调)\n",
    "EARLY_STOPPING_PATIENCE = 3        # 验证指标连续多少次评估没有改善后停止 (这里的“次”对应评估的次数)\n",
    "EARLY_STOPPING_METRIC = 'val_loss'  # 早停基于的指标: 'val_loss', 'val_accuracy', 'val_f1_weighted'\n",
    "EARLY_STOPPING_MIN_DELTA = 0.001    # 指标改善的最小阈值\n",
    "# EARLY_STOPPING_MODE 会根据 EARLY_STOPPING_METRIC 自动推断 (含'loss'为'min', 否则'max')\n",
    "\n",
    "# 二次微调的超参数\n",
    "AUG_LEARNING_RATE = 2e-5\n",
    "AUG_NUM_EPOCHS = 6\n",
    "AUG_WARMUP_PROPORTION = 0.1\n",
    "AUG_WEIGHT_DECAY = 0.01\n",
    "# 二次微调的 EVAL_STRATEGY 和 EVAL_FREQUENCY_FRAC_EPOCH 将复用初次微调的设置。\n",
    "# 如果需要不同，可以定义 AUG_EVAL_STRATEGY 和 AUG_EVAL_FREQUENCY_FRAC_EPOCH。\n",
    "AUG_EVAL_STRATEGY = \"steps\"\n",
    "AUG_EVAL_FREQUENCY_FRAC_EPOCH = 1\n",
    "# --- 4.4 分析参数 (Analysis Parameters) ---\n",
    "ANALYSIS_TOP_P_THRESHOLD = 0.9       # 用于单元格19的 top-p 分析的阈值 (如果该单元格仍在使用)\n",
    "PROB_DIST_PLOT_STEP = 0.001        # 概率分布图的x轴分度值\n",
    "TOP_PERCENT_FOR_AUGMENTATION = 0.80  # 用于数据增强的最高概率样本比例\n",
    "\n",
    "log.info(\"所有配置参数定义完毕。\")\n",
    "log.info(f\"文本日志文件: {LOG_FILE_PATH}\")\n",
    "log.info(f\"TensorBoard 日志目录: {TENSORBOARD_LOG_DIR}\")\n",
    "log.info(f\"原始数据目录: {RAW_DATA_DIR}\")\n",
    "log.info(f\"  增强数据文件: {AUG_TRAIN_FILE_PATH}\")\n",
    "log.info(f\"缓存数据目录: {CACHE_DATA_DIR}\")\n",
    "log.info(f\"模型保存目录: {SAVED_MODELS_DIR}\")\n",
    "log.info(f\"  初次最佳模型: {BEST_MODEL_PATH}\")\n",
    "log.info(f\"  二次最佳模型: {BEST_AUG_MODEL_PATH}\")\n",
    "log.info(f\"最终预测结果目录: {FINAL_RESULTS_DIR}\")\n",
    "log.info(f\"  最终预测JSON: {FINAL_TOP1_JSON_PATH}\")\n",
    "log.info(f\"  最终预测TXT: {FINAL_RESULT_TXT_PATH}\")\n",
    "log.info(f\"初步分析结果目录: {ANALYSIS_DIR}\")\n",
    "log.info(f\"  初步分析JSON: {TOP1_JSON_PATH}\") # (示例)\n",
    "log.info(f\"最终预测分析图表/CSV目录: {FINAL_ANALYSIS_OUTPUT_PATH}\")\n",
    "log.info(f\"  最终分析图: {FINAL_PROB_DIST_PLOT_PATH}\") # (示例)\n",
    "\n",
    "# ==============================================================================\n",
    "# 单元格 3 结束\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c47c6c2-e40c-4744-86f2-7ae52767da93",
   "metadata": {},
   "source": [
    "## 数据加载与标签映射 (Data Loading & Label Mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2f4204c-cdad-4e39-8adb-043771a3909c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 16:52:45,033 - __main__ - INFO - ============================== 开始数据加载与标签映射 ==============================\n",
      "2025-05-18 16:52:45,034 - __main__ - INFO - -------------------- 加载训练集 --------------------\n",
      "2025-05-18 16:52:45,035 - __main__ - INFO - 开始从文件加载数据: ./files/raw_data/train.txt\n",
      "2025-05-18 16:52:45,692 - __main__ - INFO - 从 ./files/raw_data/train.txt 成功加载 752471 条数据\n",
      "2025-05-18 16:52:45,692 - __main__ - INFO - -------------------- 加载验证集 --------------------\n",
      "2025-05-18 16:52:45,693 - __main__ - INFO - 开始从文件加载数据: ./files/raw_data/dev.txt\n",
      "2025-05-18 16:52:45,762 - __main__ - INFO - 从 ./files/raw_data/dev.txt 成功加载 80000 条数据\n",
      "2025-05-18 16:52:45,763 - __main__ - INFO - -------------------- 加载测试集 --------------------\n",
      "2025-05-18 16:52:45,764 - __main__ - INFO - 开始从文件加载数据: ./files/raw_data/test.txt\n",
      "2025-05-18 16:52:45,814 - __main__ - INFO - 从 ./files/raw_data/test.txt 成功加载 83599 条数据\n",
      "2025-05-18 16:52:45,815 - __main__ - INFO - 开始从训练集构建类别标签映射...\n",
      "2025-05-18 16:52:45,885 - __main__ - INFO - 共找到 14 个唯一类别: ['体育', '娱乐', '家居', '彩票', '房产', '教育', '时尚', '时政', '星座', '游戏', '社会', '科技', '股票', '财经']\n",
      "2025-05-18 16:52:45,886 - __main__ - INFO - 类别到ID的映射 (label_to_id): {'体育': 0, '娱乐': 1, '家居': 2, '彩票': 3, '房产': 4, '教育': 5, '时尚': 6, '时政': 7, '星座': 8, '游戏': 9, '社会': 10, '科技': 11, '股票': 12, '财经': 13}\n",
      "2025-05-18 16:52:45,886 - __main__ - INFO - ID到类别的映射 (id_to_label): {0: '体育', 1: '娱乐', 2: '家居', 3: '彩票', 4: '房产', 5: '教育', 6: '时尚', 7: '时政', 8: '星座', 9: '游戏', 10: '社会', 11: '科技', 12: '股票', 13: '财经'}\n",
      "2025-05-18 16:52:45,887 - __main__ - INFO - 原始数据加载和标签映射完成\n",
      "2025-05-18 16:52:45,887 - __main__ - INFO - ============================== 数据加载与标签映射结束 ==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "\n",
    "log.info(\"=\"*30 + \" 开始数据加载与标签映射 \" + \"=\"*30)\n",
    "\n",
    "# --- 5.1 数据加载函数定义 (Data Loading Function Definition) ---\n",
    "def load_data_from_file(file_path: str, is_test_set: bool = False, delimiter: str = '\\t') -> list:\n",
    "    \"\"\"\n",
    "    从指定的文本文件加载数据到字典列表\n",
    "\n",
    "    参数:\n",
    "        file_path (str): 数据文件的完整路径\n",
    "        is_test_set (bool): 指示是否为测试集（测试集没有标签）\n",
    "        delimiter (str): 文本和标签之间的分隔符（如果适用）\n",
    "\n",
    "    返回:\n",
    "        list: 包含数据样本的列表，每个样本是一个字典\n",
    "              对于训练/验证集: [{'text': str, 'label_text': str}, ...]\n",
    "              对于测试集: [{'text': str, 'id': int}, ...]\n",
    "    \"\"\"\n",
    "    data_samples = []\n",
    "    if not os.path.exists(file_path):\n",
    "        log.error(f\"数据文件不存在: {file_path}\")\n",
    "        raise FileNotFoundError(f\"Data file not found at {file_path}\")\n",
    "\n",
    "    log.info(f\"开始从文件加载数据: {file_path}\")\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                line = line.strip()\n",
    "                if not line: # 跳过空行\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    if is_test_set:\n",
    "                        data_samples.append({\"text\": line, \"id\": i}) # 为测试集样本添加id\n",
    "                    else:\n",
    "                        parts = line.split(delimiter)\n",
    "                        if len(parts) == 2:\n",
    "                            text, label_text = parts\n",
    "                            data_samples.append({\"text\": text, \"label_text\": label_text})\n",
    "                        else:\n",
    "                            log.warning(f\"跳过格式错误的行 {i+1} (0-indexed) in {file_path}: '{line}'. 期望格式: text{delimiter}label\")\n",
    "                except Exception as e_line: # 处理单行解析错误\n",
    "                    log.warning(f\"处理行 {i+1} (0-indexed) 时出错 in {file_path}: '{line}' - 错误: {e_line}\")\n",
    "    except Exception as e_file: # 处理文件读取错误\n",
    "        log.error(f\"读取文件 {file_path} 时发生严重错误: {e_file}\")\n",
    "        raise # 重新抛出异常，让上层处理\n",
    "\n",
    "    log.info(f\"从 {file_path} 成功加载 {len(data_samples)} 条数据\")\n",
    "    return data_samples\n",
    "\n",
    "# --- 5.2 确保原始数据目录存在 (Ensure Raw Data Directory Exists) ---\n",
    "# RAW_DATA_DIR 在单元格3中定义\n",
    "if not os.path.exists(RAW_DATA_DIR):\n",
    "    os.makedirs(RAW_DATA_DIR)\n",
    "    log.warning(f\"原始数据目录 {RAW_DATA_DIR} 不存在，已自动创建\")\n",
    "    log.warning(f\"请确保以下文件已放入该目录中才能继续:\")\n",
    "    log.warning(f\"  - {TRAIN_FILE_NAME}\")\n",
    "    log.warning(f\"  - {DEV_FILE_NAME}\")\n",
    "    log.warning(f\"  - {TEST_FILE_NAME}\")\n",
    "    # 在实际场景中，如果数据是脚本运行的先决条件，这里可能应该直接 exit()\n",
    "    # exit(\"错误：原始数据文件缺失，请准备数据后重试\")\n",
    "\n",
    "\n",
    "# --- 5.3 加载所有数据集 (Load All Datasets) ---\n",
    "# 文件路径 TRAIN_FILE_PATH, DEV_FILE_PATH, TEST_FILE_PATH 在单元格3中定义\n",
    "try:\n",
    "    log.info(\"-\" * 20 + \" 加载训练集 \" + \"-\" * 20)\n",
    "    raw_train_data = load_data_from_file(TRAIN_FILE_PATH, is_test_set=False)\n",
    "\n",
    "    log.info(\"-\" * 20 + \" 加载验证集 \" + \"-\" * 20)\n",
    "    raw_dev_data = load_data_from_file(DEV_FILE_PATH, is_test_set=False)\n",
    "\n",
    "    log.info(\"-\" * 20 + \" 加载测试集 \" + \"-\" * 20)\n",
    "    raw_test_data = load_data_from_file(TEST_FILE_PATH, is_test_set=True)\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    log.critical(f\"关键数据文件加载失败: {e}\")\n",
    "    log.critical(\"程序无法继续，请检查文件路径和数据是否准备就绪\")\n",
    "    exit(1) # 使用非零退出码表示错误\n",
    "except Exception as e:\n",
    "    log.critical(f\"加载数据时发生未知严重错误: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# --- 5.4 构建类别标签映射 (Build Class Label Mapping) ---\n",
    "log.info(\"开始从训练集构建类别标签映射...\")\n",
    "try:\n",
    "    # 从 raw_train_data 获取标签\n",
    "    all_train_labels = [item['label_text'] for item in raw_train_data if 'label_text' in item]\n",
    "\n",
    "    if not raw_train_data:\n",
    "        log.critical(\"错误：训练数据集为空，无法构建标签映射\")\n",
    "        exit(1)\n",
    "    if not all_train_labels:\n",
    "        log.critical(\"错误：无法从训练数据中提取到任何标签请检查训练文件格式 (应为 text\\\\tlabel) 和内容\")\n",
    "        log.critical(f\"检查文件: {TRAIN_FILE_PATH}\")\n",
    "        exit(1)\n",
    "\n",
    "    unique_labels = sorted(list(set(all_train_labels)))\n",
    "    label_to_id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    id_to_label = {idx: label for label, idx in label_to_id.items()}\n",
    "    num_classes = len(unique_labels)\n",
    "\n",
    "    if num_classes == 0:\n",
    "        log.critical(\"错误：识别到的类别数量为0，无法继续训练\")\n",
    "        exit(1)\n",
    "    elif num_classes == 1:\n",
    "        log.warning(\"警告：只识别到一个类别模型可能无法有效学习分类任务\")\n",
    "\n",
    "\n",
    "    log.info(f\"共找到 {num_classes} 个唯一类别: {unique_labels}\")\n",
    "    log.info(f\"类别到ID的映射 (label_to_id): {label_to_id}\")\n",
    "    log.info(f\"ID到类别的映射 (id_to_label): {id_to_label}\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.critical(f\"构建标签映射时发生未知错误: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "log.info(\"原始数据加载和标签映射完成\")\n",
    "log.info(\"=\"*30 + \" 数据加载与标签映射结束 \" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "# --- 检查关键变量是否已定义，为后续单元格做准备 ---\n",
    "# (这些变量应该在此单元格的 try-except 块成功执行后被定义)\n",
    "assert 'raw_train_data' in locals(), \"raw_train_data 未定义\"\n",
    "assert 'raw_dev_data' in locals(), \"raw_dev_data 未定义\"\n",
    "assert 'raw_test_data' in locals(), \"raw_test_data 未定义\"\n",
    "assert 'label_to_id' in locals(), \"label_to_id 未定义\"\n",
    "assert 'id_to_label' in locals(), \"id_to_label 未定义\"\n",
    "assert 'num_classes' in locals(), \"num_classes 未定义\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb10bf3-3de6-47b1-9f14-c185a2cf8dbc",
   "metadata": {},
   "source": [
    "## 数据预处理 (Tokenization & Dataset Creation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2252266b-c8ed-4191-b7f8-83fa46daa26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 16:52:47,400 - __main__ - INFO - ============================== 开始数据预处理 ==============================\n",
      "2025-05-18 16:52:47,401 - __main__ - INFO - 尝试加载 Tokenizer...\n",
      "2025-05-18 16:52:47,401 - __main__ - INFO - 从本地路径 '/root/.cache/huggingface/hub/models--hfl--chinese-roberta-wwm-ext-large/snapshots/a25cc9e05974bd9687e528edd516f2cfdb3f5db9' 加载 Tokenizer...\n",
      "2025-05-18 16:52:47,427 - __main__ - INFO - Tokenizer 从本地路径 '/root/.cache/huggingface/hub/models--hfl--chinese-roberta-wwm-ext-large/snapshots/a25cc9e05974bd9687e528edd516f2cfdb3f5db9' 加载成功\n",
      "2025-05-18 16:52:47,428 - __main__ - INFO - 开始实例化或加载预处理后的 HuggingFace Dataset 对象...\n",
      "2025-05-18 16:52:47,429 - __main__ - INFO - 预处理数据集缓存不完整或不存在，将重新处理所有数据集...\n",
      "2025-05-18 16:52:47,429 - __main__ - INFO - 开始为 '训练集' 创建 HuggingFace Dataset 对象...\n",
      "2025-05-18 16:52:48,176 - __main__ - INFO - '训练集' Dataset 对象创建成功，包含 752471 条记录\n",
      "2025-05-18 16:52:48,177 - __main__ - INFO - 开始对 '训练集' 进行并行预处理 (使用 12 个进程)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8112a085a7184a1badde9cb5cd74c2a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "预处理 训练集 (num_proc=12):   0%|          | 0/752471 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 16:52:59,689 - __main__ - INFO - '训练集' 预处理完成\n",
      "2025-05-18 16:52:59,694 - __main__ - INFO - 保存预处理后的训练集到: ./files/processed_data_cache/train_dataset_cache\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7e6f7a60fd45e19659f74bbcab1617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/752471 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 16:53:00,030 - __main__ - INFO - 开始为 '验证集' 创建 HuggingFace Dataset 对象...\n",
      "2025-05-18 16:53:00,083 - __main__ - INFO - '验证集' Dataset 对象创建成功，包含 80000 条记录\n",
      "2025-05-18 16:53:00,083 - __main__ - INFO - 开始对 '验证集' 进行并行预处理 (使用 12 个进程)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44cea5c1082b47518b6401fda97c373a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "预处理 验证集 (num_proc=12):   0%|          | 0/80000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 16:53:02,532 - __main__ - INFO - '验证集' 预处理完成\n",
      "2025-05-18 16:53:02,533 - __main__ - INFO - 保存预处理后的验证集到: ./files/processed_data_cache/dev_dataset_cache\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d95536c42528464da7db776346757211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/80000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 16:53:02,587 - __main__ - INFO - 开始为 '测试集' 创建 HuggingFace Dataset 对象...\n",
      "2025-05-18 16:53:02,649 - __main__ - INFO - '测试集' Dataset 对象创建成功，包含 83599 条记录\n",
      "2025-05-18 16:53:02,650 - __main__ - INFO - 开始对 '测试集' 进行并行预处理 (使用 12 个进程)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b91a6630818f4d47a4374a3a993159fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "预处理 测试集 (num_proc=12):   0%|          | 0/83599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 16:53:05,059 - __main__ - INFO - '测试集' 预处理完成\n",
      "2025-05-18 16:53:05,060 - __main__ - INFO - 保存预处理后的测试集到: ./files/processed_data_cache/test_dataset_cache\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f086aabbe75f4a4ba87acea3380e9fa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/83599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 16:53:05,108 - __main__ - INFO - 所有数据集预处理完成并已保存到磁盘缓存\n",
      "2025-05-18 16:53:05,108 - __main__ - INFO - HuggingFace Dataset 对象实例化/加载成功\n",
      "2025-05-18 16:53:05,109 - __main__ - INFO - 设置数据集格式为 PyTorch Tensors...\n",
      "2025-05-18 16:53:05,110 - __main__ - INFO - 数据集格式设置完成\n",
      "2025-05-18 16:53:05,110 - __main__ - INFO - -------------------- 检查处理后的训练样本 --------------------\n",
      "2025-05-18 16:53:05,113 - __main__ - INFO -   样本类型: <class 'dict'>\n",
      "2025-05-18 16:53:05,113 - __main__ - INFO -   样本键: ['input_ids', 'attention_mask', 'labels']\n",
      "2025-05-18 16:53:05,114 - __main__ - INFO -     'input_ids': shape=torch.Size([28]), dtype=torch.int64\n",
      "2025-05-18 16:53:05,114 - __main__ - INFO -     'attention_mask': shape=torch.Size([28]), dtype=torch.int64\n",
      "2025-05-18 16:53:05,114 - __main__ - INFO -     'labels': shape=torch.Size([]), dtype=torch.int64\n",
      "2025-05-18 16:53:05,115 - __main__ - INFO -   解码后的 Input IDs (预览): '网 易 第 三 季 度 业 绩 低 于 分 析 师 预 期'\n",
      "2025-05-18 16:53:05,115 - __main__ - INFO -   对应的原始文本 (预览): '网易第三季度业绩低于分析师预期'\n",
      "2025-05-18 16:53:05,115 - __main__ - INFO - ============================== 数据预处理结束 ==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 主要功能:\n",
    "# 1. 加载预训练模型的Tokenizer\n",
    "# 2. 定义文本预处理函数，将文本转换为模型可接受的输入格式 (input_ids, attention_mask)，并将标签文本转换为标签ID\n",
    "# 3. 将原始数据列表转换为 Hugging Face Dataset 对象\n",
    "# 4. 使用 Dataset.map() 方法对所有数据进行批处理和并行预处理\n",
    "# 5. 实现预处理后数据的磁盘缓存与加载机制，避免重复处理\n",
    "# 6. 将处理后的 Dataset 对象设置为 PyTorch Tensor 格式，以便后续 DataLoader 使用\n",
    "# 7. (可选) 打印一个处理后的样本进行检查\n",
    "# ==============================================================================\n",
    "log.info(\"=\"*30 + \" 开始数据预处理 \" + \"=\"*30)\n",
    "\n",
    "# --- 6.1 加载 Tokenizer (Load Tokenizer) ---\n",
    "\n",
    "tokenizer = None\n",
    "log.info(f\"尝试加载 Tokenizer...\")\n",
    "if LOCAL_MODEL_PATH and os.path.exists(LOCAL_MODEL_PATH):\n",
    "    try:\n",
    "        log.info(f\"从本地路径 '{LOCAL_MODEL_PATH}' 加载 Tokenizer...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_PATH)\n",
    "        log.info(f\"Tokenizer 从本地路径 '{LOCAL_MODEL_PATH}' 加载成功\")\n",
    "    except Exception as e_local:\n",
    "        log.warning(f\"从本地路径 '{LOCAL_MODEL_PATH}' 加载 Tokenizer 失败: {e_local}\")\n",
    "        log.info(f\"将尝试从 Hugging Face Hub 加载 Tokenizer: '{PRETRAINED_MODEL_NAME}'\")\n",
    "        tokenizer = None # 重置以确保尝试从网络加载\n",
    "\n",
    "if tokenizer is None: # 如果本地加载失败或未提供本地路径\n",
    "    try:\n",
    "        log.info(f\"从 Hugging Face Hub ({PRETRAINED_MODEL_NAME}) 加载 Tokenizer...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "        log.info(f\"Tokenizer '{PRETRAINED_MODEL_NAME}' 从 Hugging Face Hub 加载成功\")\n",
    "    except Exception as e_hub:\n",
    "        log.critical(f\"从 Hugging Face Hub 加载 Tokenizer '{PRETRAINED_MODEL_NAME}' 失败: {e_hub}\")\n",
    "        log.critical(\"无法加载Tokenizer，程序将终止\")\n",
    "        exit(1)\n",
    "\n",
    "# --- 6.2 定义预处理函数 (Define Preprocessing Function) ---\n",
    "def preprocess_function(\n",
    "    examples: dict, # Dataset.map 会传入一个批次的样本，其结构为 {'text': [str], 'label_text': [str]} 或 {'text': [str], 'id': [int]}\n",
    "    tokenizer_instance: AutoTokenizer,\n",
    "    max_len: int,\n",
    "    label_map: dict = None,\n",
    "    is_test_set: bool = False\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    对一批文本样本进行tokenize和编码\n",
    "    此函数将被 Dataset.map 调用\n",
    "\n",
    "    参数:\n",
    "        examples (dict): 包含文本和其他字段的字典 (Dataset.map传入)\n",
    "        tokenizer_instance (AutoTokenizer): 已初始化的Tokenizer\n",
    "        max_len (int): Tokenize后的最大序列长度\n",
    "        label_map (dict, optional): 标签文本到ID的映射对于非测试集是必需的\n",
    "        is_test_set (bool): 是否为测试集，如果是则没有label\n",
    "\n",
    "    返回:\n",
    "        dict: 包含 'input_ids', 'attention_mask' 以及 (如果适用) 'labels' 或 'id' 的字典\n",
    "    \"\"\"\n",
    "    # 对文本进行批处理编码\n",
    "    # examples['text'] 是一个文本列表\n",
    "    tokenized_batch = tokenizer_instance(\n",
    "        examples['text'],\n",
    "        max_length=max_len,\n",
    "        padding='max_length', # 填充到max_len\n",
    "        truncation=True,      # 超出max_len则截断\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\"   # 返回PyTorch张量，后续Dataset.set_format会处理\n",
    "    )\n",
    "\n",
    "    processed_batch = {\n",
    "        'input_ids': tokenized_batch['input_ids'],\n",
    "        'attention_mask': tokenized_batch['attention_mask']\n",
    "    }\n",
    "\n",
    "    if not is_test_set: # 训练集/验证集情况\n",
    "        if label_map is None:\n",
    "            raise ValueError(\"对于训练/验证集，label_map 不能为空\")\n",
    "        if 'label_text' not in examples:\n",
    "            log.warning(\"警告：输入样本批次中缺少 'label_text' 键，但当前不是测试集模式\")\n",
    "            if not examples.get('label_text'): # 检查是否为空列表或 None\n",
    "                 raise ValueError(\"训练/验证集样本缺少 'label_text' 或内容为空\")\n",
    "\n",
    "        # 将标签文本转换为ID\n",
    "        try:\n",
    "            label_ids = [label_map[label] for label in examples['label_text']]\n",
    "            processed_batch['labels'] = torch.tensor(label_ids, dtype=torch.long)\n",
    "        except KeyError as e_key:\n",
    "            log.error(f\"标签映射错误：标签 '{e_key}' 不在 label_map 中请检查训练数据和标签映射的构建\")\n",
    "            raise\n",
    "        except TypeError as e_type: # 例如 examples['label_text'] 不是可迭代对象或 label_map 不是字典\n",
    "            log.error(f\"处理标签时发生类型错误: {e_type}. examples['label_text']: {examples.get('label_text')}, label_map: {type(label_map)}\")\n",
    "            raise\n",
    "    else: # 测试集情况\n",
    "        if 'id' in examples: # Dataset.map会传递原始列\n",
    "            processed_batch['id'] = examples['id']\n",
    "        else: log.debug(\"测试集样本中缺少 'id' 字段\")\n",
    "\n",
    "    return processed_batch\n",
    "\n",
    "# --- 6.3 定义数据集创建与预处理函数 (Define Dataset Creation & Preprocessing Function) ---\n",
    "def create_and_process_dataset(\n",
    "    raw_data_list: list,\n",
    "    tokenizer_instance: AutoTokenizer,\n",
    "    max_len: int,\n",
    "    dataset_display_name: str,\n",
    "    is_test_set: bool = False,\n",
    "    label_map: dict = None, # 仅在 is_test_set=False 时需要\n",
    "    num_processing_workers: int = 4 # 并行处理进程数\n",
    ") -> Dataset:\n",
    "    \"\"\"\n",
    "    将原始数据列表转换为 Hugging Face Dataset 对象，并对其进行预处理\n",
    "\n",
    "    参数:\n",
    "        raw_data_list (list): 包含原始样本字典的列表\n",
    "        tokenizer_instance (AutoTokenizer): 已初始化的Tokenizer\n",
    "        max_len (int): Tokenize后的最大序列长度\n",
    "        dataset_display_name (str): 用于日志和进度的据集名称 (例如 \"训练集\")\n",
    "        is_test_set (bool): 是否为测试集\n",
    "        label_map (dict, optional): 标签文本到ID的映射\n",
    "        num_processing_workers (int): Dataset.map 使用的进程数\n",
    "\n",
    "    返回:\n",
    "        Dataset: 处理后的 Hugging Face Dataset 对象\n",
    "    \"\"\"\n",
    "    if not raw_data_list:\n",
    "        log.warning(f\"原始数据列表 '{dataset_display_name}' 为空，将返回一个空 Dataset\")\n",
    "        return Dataset.from_list([]) # 返回空数据集\n",
    "\n",
    "    log.info(f\"开始为 '{dataset_display_name}' 创建 HuggingFace Dataset 对象...\")\n",
    "    hf_dataset = Dataset.from_list(raw_data_list)\n",
    "    log.info(f\"'{dataset_display_name}' Dataset 对象创建成功，包含 {len(hf_dataset)} 条记录\")\n",
    "\n",
    "    bound_preprocess_function = partial(\n",
    "        preprocess_function,\n",
    "        tokenizer_instance=tokenizer_instance,\n",
    "        max_len=max_len,\n",
    "        label_map=label_map,\n",
    "        is_test_set=is_test_set\n",
    "    )\n",
    "\n",
    "    log.info(f\"开始对 '{dataset_display_name}' 进行并行预处理 (使用 {num_processing_workers} 个进程)...\")\n",
    "    # 使用map方法进行并行预处理\n",
    "    columns_to_remove = ['text']\n",
    "    if not is_test_set:\n",
    "        columns_to_remove.append('label_text')\n",
    "    columns_to_remove_actual = [col for col in columns_to_remove if col in hf_dataset.column_names]\n",
    "\n",
    "    processed_dataset = hf_dataset.map(\n",
    "        bound_preprocess_function,\n",
    "        batched=True,          # 启用批处理\n",
    "        batch_size=1000,       \n",
    "        num_proc=num_processing_workers, # 并行处理的进程数\n",
    "        remove_columns=columns_to_remove_actual, # 移除原始列\n",
    "        desc=f\"预处理 {dataset_display_name}\"\n",
    "    )\n",
    "    log.info(f\"'{dataset_display_name}' 预处理完成\")\n",
    "    return processed_dataset\n",
    "\n",
    "# --- 6.4 实例化并缓存/加载 HuggingFace Datasets (Instantiate & Cache/Load Datasets) ---\n",
    "\n",
    "log.info(\"开始实例化或加载预处理后的 HuggingFace Dataset 对象...\")\n",
    "try:\n",
    "    # 检查所有缓存文件是否都存在\n",
    "    all_cache_exists = (\n",
    "        os.path.exists(CACHE_TRAIN_PATH) and\n",
    "        os.path.exists(CACHE_DEV_PATH) and\n",
    "        os.path.exists(CACHE_TEST_PATH)\n",
    "    )\n",
    "\n",
    "    if all_cache_exists:\n",
    "        log.info(\"所有预处理好的数据集缓存均存在，正在从磁盘加载...\")\n",
    "        train_dataset = Dataset.load_from_disk(CACHE_TRAIN_PATH)\n",
    "        dev_dataset = Dataset.load_from_disk(CACHE_DEV_PATH)\n",
    "        test_dataset = Dataset.load_from_disk(CACHE_TEST_PATH)\n",
    "        log.info(\"从磁盘加载预处理数据集成功\")\n",
    "    else:\n",
    "        log.info(\"预处理数据集缓存不完整或不存在，将重新处理所有数据集...\")\n",
    "\n",
    "        # 创建并预处理训练集\n",
    "        train_dataset = create_and_process_dataset(\n",
    "            raw_train_data, tokenizer, MAX_SEQ_LENGTH,\n",
    "            dataset_display_name=\"训练集\", is_test_set=False, label_map=label_to_id,\n",
    "            num_processing_workers=NUM_PROC_FOR_MAP\n",
    "        )\n",
    "        log.info(f\"保存预处理后的训练集到: {CACHE_TRAIN_PATH}\")\n",
    "        train_dataset.save_to_disk(CACHE_TRAIN_PATH)\n",
    "\n",
    "        # 创建并预处理验证集\n",
    "        dev_dataset = create_and_process_dataset(\n",
    "            raw_dev_data, tokenizer, MAX_SEQ_LENGTH,\n",
    "            dataset_display_name=\"验证集\", is_test_set=False, label_map=label_to_id,\n",
    "            num_processing_workers=NUM_PROC_FOR_MAP\n",
    "        )\n",
    "        log.info(f\"保存预处理后的验证集到: {CACHE_DEV_PATH}\")\n",
    "        dev_dataset.save_to_disk(CACHE_DEV_PATH)\n",
    "\n",
    "        # 创建并预处理测试集\n",
    "        test_dataset = create_and_process_dataset(\n",
    "            raw_test_data, tokenizer, MAX_SEQ_LENGTH,\n",
    "            dataset_display_name=\"测试集\", is_test_set=True, label_map=None, # 测试集不需要label_map\n",
    "            num_processing_workers=NUM_PROC_FOR_MAP\n",
    "        )\n",
    "        log.info(f\"保存预处理后的测试集到: {CACHE_TEST_PATH}\")\n",
    "        test_dataset.save_to_disk(CACHE_TEST_PATH)\n",
    "\n",
    "        log.info(\"所有数据集预处理完成并已保存到磁盘缓存\")\n",
    "\n",
    "    log.info(\"HuggingFace Dataset 对象实例化/加载成功\")\n",
    "\n",
    "    # --- 6.5 设置Dataset格式为PyTorch Tensors (Set Dataset Format to PyTorch Tensors) ---\n",
    "    log.info(\"设置数据集格式为 PyTorch Tensors...\")\n",
    "    \n",
    "    # 训练集和验证集需要 'input_ids', 'attention_mask', 'labels'\n",
    "    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    dev_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    \n",
    "    # 测试集需要 'input_ids', 'attention_mask', 以及原始的 'id'\n",
    "    test_columns_for_torch = ['input_ids', 'attention_mask']\n",
    "    \n",
    "    if 'id' in test_dataset.column_names: # 检查 'id' 列是否存在\n",
    "        test_columns_for_torch.append('id')\n",
    "    else:\n",
    "        log.warning(\"测试数据集中未找到 'id' 列，将不包含在PyTorch格式中如果需要原始ID进行后续关联，请检查预处理逻辑\")\n",
    "    test_dataset.set_format(type='torch', columns=test_columns_for_torch)\n",
    "    log.info(\"数据集格式设置完成\")\n",
    "\n",
    "except NameError as e_name:\n",
    "    log.critical(f\"创建 Dataset 时发生 NameError (可能是变量未定义): {e_name}\")\n",
    "    log.critical(\"请确保以下变量已在之前的单元格中正确加载和定义: \"\n",
    "                 \"'raw_train_data', 'raw_dev_data', 'raw_test_data', \"\n",
    "                 \"'tokenizer', 'label_to_id', 'MAX_SEQ_LENGTH', \"\n",
    "                 \"'CACHE_TRAIN_PATH', 'CACHE_DEV_PATH', 'CACHE_TEST_PATH', 'NUM_PROC_FOR_MAP'.\")\n",
    "    exit(1)\n",
    "except Exception as e_general:\n",
    "    log.critical(f\"创建或处理 HuggingFace Dataset 对象时发生未知严重错误: {e_general}\", exc_info=True) \n",
    "    exit(1)\n",
    "\n",
    "\n",
    "# --- 6.6 (可选) 检查处理后的数据 (Optional: Inspect Processed Data) ---\n",
    "log.info(\"-\" * 20 + \" 检查处理后的训练样本 \" + \"-\" * 20)\n",
    "if len(train_dataset) > 0:\n",
    "    sample = train_dataset[0]\n",
    "    log.info(f\"  样本类型: {type(sample)}\")\n",
    "    log.info(f\"  样本键: {list(sample.keys())}\")\n",
    "    for key, value in sample.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            log.info(f\"    '{key}': shape={value.shape}, dtype={value.dtype}\")\n",
    "        else:\n",
    "            log.info(f\"    '{key}': type={type(value)}, value={value}\") # 对于非Tensor值也打印出来\n",
    "\n",
    "    # 尝试解码 Input IDs 进行验证\n",
    "    try:\n",
    "        decoded_text_preview = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n",
    "        log.info(f\"  解码后的 Input IDs (预览): '{decoded_text_preview}'\")\n",
    "        if raw_train_data and isinstance(raw_train_data[0], dict) and 'text' in raw_train_data[0]:\n",
    "             log.info(f\"  对应的原始文本 (预览): '{raw_train_data[0]['text']}'\")\n",
    "    except Exception as e_decode:\n",
    "        log.warning(f\"解码样本Input IDs时出错: {e_decode}\")\n",
    "else:\n",
    "    log.warning(\"处理后的训练集为空，无法打印样本进行检查\")\n",
    "\n",
    "log.info(\"=\"*30 + \" 数据预处理结束 \" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "# --- 确保关键变量已定义 ---\n",
    "assert 'tokenizer' in locals(), \"tokenizer 未定义\"\n",
    "assert 'train_dataset' in locals(), \"train_dataset 未定义\"\n",
    "assert 'dev_dataset' in locals(), \"dev_dataset 未定义\"\n",
    "assert 'test_dataset' in locals(), \"test_dataset 未定义\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393bdeef-a555-4b82-8a10-49494f6a9839",
   "metadata": {},
   "source": [
    "## 模型加载 (Model Loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b4f88ca-8a52-48dd-8952-7ae861602520",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 16:53:05,127 - __main__ - INFO - ============================== 开始模型加载 ==============================\n",
      "2025-05-18 16:53:05,127 - __main__ - INFO - 检查模型加载所需的前置变量...\n",
      "2025-05-18 16:53:05,128 - __main__ - INFO - 所有模型加载所需的前置变量均已定义\n",
      "2025-05-18 16:53:05,128 - __main__ - INFO - 开始加载序列分类模型...\n",
      "2025-05-18 16:53:05,129 - __main__ - INFO - 尝试从本地路径 '/root/.cache/huggingface/hub/models--hfl--chinese-roberta-wwm-ext-large/snapshots/a25cc9e05974bd9687e528edd516f2cfdb3f5db9' 加载模型 (类别数: 14)...\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /root/.cache/huggingface/hub/models--hfl--chinese-roberta-wwm-ext-large/snapshots/a25cc9e05974bd9687e528edd516f2cfdb3f5db9 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-05-18 16:53:07,455 - __main__ - INFO - 模型成功从本地路径 '/root/.cache/huggingface/hub/models--hfl--chinese-roberta-wwm-ext-large/snapshots/a25cc9e05974bd9687e528edd516f2cfdb3f5db9' 加载\n",
      "2025-05-18 16:53:07,787 - __main__ - INFO - 模型已成功移至设备: cuda.\n",
      "2025-05-18 16:53:07,792 - __main__ - INFO - 模型总参数量: 325,536,782\n",
      "2025-05-18 16:53:07,792 - __main__ - INFO - 模型可训练参数量: 325,536,782\n",
      "2025-05-18 16:53:07,793 - __main__ - INFO - 模型 '/root/.cache/huggingface/hub/models--hfl--chinese-roberta-wwm-ext-large/snapshots/a25cc9e05974bd9687e528edd516f2cfdb3f5db9' (基于 'hfl/chinese-roberta-wwm-ext-large' 结构) 加载并配置完成\n",
      "2025-05-18 16:53:07,793 - __main__ - INFO - ============================== 模型加载结束 ==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 主要功能:\n",
    "# 1. 使用 AutoModelForSequenceClassification.from_pretrained() 加载预训练的序列分类模型\n",
    "#    - 关键参数: num_labels=num_classes，用于初始化与任务类别数匹配的分类头\n",
    "# 2. 将加载的模型移动到指定的计算设备 (CPU/GPU)\n",
    "# 3. 对模型加载过程中可能发生的错误 (如路径错误、网络问题、配置不匹配) 进行捕获和日志记录\n",
    "# ==============================================================================\n",
    "log.info(\"=\"*30 + \" 开始模型加载 \" + \"=\"*30)\n",
    "\n",
    "# --- 7.1 检查前置条件 (Check Prerequisites) ---\n",
    "required_vars_for_model_loading = [\n",
    "    'num_classes',        # 模型分类头的类别数\n",
    "    'device',             # 模型将加载到的设备\n",
    "    'LOCAL_MODEL_PATH',   # 本地模型路径配置\n",
    "    'PRETRAINED_MODEL_NAME' # 预训练模型名称配置\n",
    "]\n",
    "log.info(\"检查模型加载所需的前置变量...\")\n",
    "for var_name in required_vars_for_model_loading:\n",
    "    if var_name not in locals() and var_name not in globals(): # 检查局部和全局作用域\n",
    "        log.critical(f\"模型加载失败：关键前置变量 '{var_name}' 未定义，请检查之前的单元格\")\n",
    "        exit(1) \n",
    "log.info(\"所有模型加载所需的前置变量均已定义\")\n",
    "\n",
    "# --- 7.2 模型加载 (Load the Model) ---\n",
    "log.info(\"开始加载序列分类模型...\")\n",
    "\n",
    "model = None\n",
    "model_successfully_loaded_from_path = \"\"\n",
    "\n",
    "# 优先尝试从本地路径加载\n",
    "if LOCAL_MODEL_PATH and os.path.exists(LOCAL_MODEL_PATH): # 确保本地路径非空且存在\n",
    "    log.info(f\"尝试从本地路径 '{LOCAL_MODEL_PATH}' 加载模型 (类别数: {num_classes})...\")\n",
    "    try:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            LOCAL_MODEL_PATH,\n",
    "            num_labels=num_classes,\n",
    "            ignore_mismatched_sizes=False # 如果模型头的尺寸不匹配num_labels，报错\n",
    "        )\n",
    "        model_successfully_loaded_from_path = LOCAL_MODEL_PATH\n",
    "        log.info(f\"模型成功从本地路径 '{LOCAL_MODEL_PATH}' 加载\")\n",
    "    except OSError as e_os_local: \n",
    "        log.warning(f\"从本地路径 '{LOCAL_MODEL_PATH}' 加载模型时发生 OSError: {e_os_local}\")\n",
    "        log.info(f\"将尝试从 Hugging Face Hub 使用 '{PRETRAINED_MODEL_NAME}' 加载模型\")\n",
    "        model = None # 重置 model，确保后续尝试从 Hub 加载\n",
    "    except ValueError as e_val_local: # 通常是 num_labels 与模型头不匹配\n",
    "        log.warning(f\"从本地路径 '{LOCAL_MODEL_PATH}' 加载模型时发生 ValueError: {e_val_local}\")\n",
    "        log.warning(\"这可能意味着本地模型是一个已微调过的模型，其分类头与当前任务的 num_classes 不匹配\")\n",
    "        log.info(f\"将尝试从 Hugging Face Hub 使用 '{PRETRAINED_MODEL_NAME}' 重新初始化模型头并加载\")\n",
    "        model = None\n",
    "    except Exception as e_local_other: # 其他本地加载错误\n",
    "        log.warning(f\"从本地路径 '{LOCAL_MODEL_PATH}' 加载模型时发生未知错误: {e_local_other}\")\n",
    "        log.info(f\"将尝试从 Hugging Face Hub 使用 '{PRETRAINED_MODEL_NAME}' 加载模型\")\n",
    "        model = None\n",
    "\n",
    "if model is None:\n",
    "    log.info(f\"尝试从 Hugging Face Hub 使用 '{PRETRAINED_MODEL_NAME}' 加载模型 (类别数: {num_classes})...\")\n",
    "    try:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            PRETRAINED_MODEL_NAME,\n",
    "            num_labels=num_classes,\n",
    "            ignore_mismatched_sizes=False \n",
    "        )\n",
    "        model_successfully_loaded_from_path = PRETRAINED_MODEL_NAME\n",
    "        log.info(f\"模型成功从 Hugging Face Hub ('{PRETRAINED_MODEL_NAME}') 加载\")\n",
    "    except OSError as e_os_hub:\n",
    "        log.critical(f\"从 Hugging Face Hub ('{PRETRAINED_MODEL_NAME}') 加载模型时发生 OSError: {e_os_hub}\")\n",
    "        log.critical(\"可能由网络问题、模型名称错误等导致,程序无法继续\")\n",
    "        exit(1)\n",
    "    except ValueError as e_val_hub:\n",
    "        log.critical(f\"从 Hugging Face Hub ('{PRETRAINED_MODEL_NAME}') 加载模型时发生 ValueError: {e_val_hub}\")\n",
    "        log.critical(f\"可能原因：num_labels ({num_classes}) 与 '{PRETRAINED_MODEL_NAME}' 的预训练分类头（如果存在）不匹配或者模型配置问题，程序无法继续\")\n",
    "        exit(1)\n",
    "    except Exception as e_hub_other:\n",
    "        log.critical(f\"从 Hugging Face Hub ('{PRETRAINED_MODEL_NAME}') 加载模型时发生未知严重错误: {e_hub_other}\", exc_info=True)\n",
    "        exit(1)\n",
    "\n",
    "# --- 7.3 将模型移至设备并记录信息 (Move Model to Device & Log Info) ---\n",
    "try:\n",
    "    model.to(device)\n",
    "    log.info(f\"模型已成功移至设备: {device}.\")\n",
    "except Exception as e_device:\n",
    "    log.critical(f\"将模型移至设备 {device} 时发生错误: {e_device}\", exc_info=True)\n",
    "    exit(1)\n",
    "\n",
    "# (可选) 打印模型结构摘要信息\n",
    "num_model_params = sum(p.numel() for p in model.parameters())\n",
    "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "log.info(f\"模型总参数量: {num_model_params:,}\")\n",
    "log.info(f\"模型可训练参数量: {num_trainable_params:,}\")\n",
    "\n",
    "\n",
    "log.info(f\"模型 '{model_successfully_loaded_from_path}' (基于 '{PRETRAINED_MODEL_NAME}' 结构) 加载并配置完成\")\n",
    "log.info(\"=\"*30 + \" 模型加载结束 \" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "# --- 确保关键变量已定义 ---\n",
    "assert 'model' in locals() and model is not None, \"模型 (model) 未成功加载或未定义\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53ec692-0c97-4c38-beb1-69f0214cce39",
   "metadata": {},
   "source": [
    "## DataLoader 配置 (DataLoader Configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af786db3-b905-4687-8310-ddec95500f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 16:53:07,811 - __main__ - INFO - ============================== 开始配置 DataLoaders ==============================\n",
      "2025-05-18 16:53:07,812 - __main__ - INFO - 检查 DataLoader 配置所需的前置变量...\n",
      "2025-05-18 16:53:07,812 - __main__ - INFO - 所有 DataLoader 配置所需的核心前置变量均已定义\n",
      "2025-05-18 16:53:07,813 - __main__ - INFO - DataLoader 配置参数: Batch Size=256, Num Workers=6\n",
      "2025-05-18 16:53:07,813 - __main__ - INFO - 创建 DataCollatorWithPadding...\n",
      "2025-05-18 16:53:07,814 - __main__ - INFO - DataCollatorWithPadding 创建成功\n",
      "2025-05-18 16:53:07,814 - __main__ - INFO - 为训练集 DataLoader 设置 pin_memory=True\n",
      "2025-05-18 16:53:07,815 - __main__ - INFO - 创建训练集 DataLoader (Batch Size: 256, Shuffle: True, Num Workers: 6, Pin Memory: True)\n",
      "2025-05-18 16:53:07,815 - __main__ - INFO - 训练集 DataLoader 创建成功\n",
      "2025-05-18 16:53:07,816 - __main__ - INFO - 创建验证集 DataLoader (Batch Size: 256, Shuffle: False, Num Workers: 6, Pin Memory: True)\n",
      "2025-05-18 16:53:07,816 - __main__ - INFO - 验证集 DataLoader 创建成功\n",
      "2025-05-18 16:53:07,817 - __main__ - INFO - 创建测试集 DataLoader (Batch Size: 256, Shuffle: False, Num Workers: 6, Pin Memory: True)\n",
      "2025-05-18 16:53:07,817 - __main__ - INFO - 测试集 DataLoader 创建成功\n",
      "2025-05-18 16:53:07,818 - __main__ - INFO - 所有 DataLoader 配置完成\n",
      "2025-05-18 16:53:07,818 - __main__ - INFO - -------------------- 检查训练 DataLoader 输出 --------------------\n",
      "2025-05-18 16:53:08,669 - __main__ - INFO - 成功从 train_data_loader 获取一个批次\n",
      "2025-05-18 16:53:08,670 - __main__ - INFO - 批次包含的键: ['input_ids', 'attention_mask', 'labels']\n",
      "2025-05-18 16:53:08,671 - __main__ - INFO -   - 'input_ids': shape=torch.Size([256, 28]), dtype=torch.int64, device=cpu\n",
      "2025-05-18 16:53:08,671 - __main__ - INFO -   - 'attention_mask': shape=torch.Size([256, 28]), dtype=torch.int64, device=cpu\n",
      "2025-05-18 16:53:08,672 - __main__ - INFO -   - 'labels': shape=torch.Size([256]), dtype=torch.int64, device=cpu\n",
      "2025-05-18 16:53:08,673 - __main__ - INFO - DataLoader 配置已准备就绪，后续步骤可以定义优化器、学习率调度器等\n",
      "2025-05-18 16:53:08,673 - __main__ - INFO - ============================== DataLoaders 配置结束 ==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 主要功能:\n",
    "# 1. 检查前置步骤定义的关键变量和配置参数\n",
    "# 2. 创建 DataCollatorWithPadding: 用于在每个批次内部动态填充序列，\n",
    "#    使其长度一致，以便模型处理\n",
    "# 3. 实例化 PyTorch DataLoader 对象:\n",
    "#    - 为训练集 (train_data_loader): 启用数据打乱 (shuffle=True)\n",
    "#    - 为验证集 (dev_data_loader): 不打乱数据 (shuffle=False)\n",
    "#    - (可选) 为测试集 (test_data_loader): 不打乱数据 (shuffle=False)\n",
    "#    - 所有 DataLoader 都使用配置的 batch_size, num_workers, 和 collate_fn\n",
    "# 4. (可选) 从训练 DataLoader 中获取一个样本批次进行检查，验证其结构和内容\n",
    "# ==============================================================================\n",
    "log.info(\"=\"*30 + \" 开始配置 DataLoaders \" + \"=\"*30)\n",
    "\n",
    "# --- 8.1 检查前置条件 (Check Prerequisites) ---\n",
    "required_vars_for_dataloader = [\n",
    "    'tokenizer', 'train_dataset', 'dev_dataset', # test_dataset 是可选的\n",
    "    'BATCH_SIZE', 'NUM_WORKERS'\n",
    "]\n",
    "log.info(\"检查 DataLoader 配置所需的前置变量...\")\n",
    "for var_name in required_vars_for_dataloader:\n",
    "    if var_name not in locals() and var_name not in globals():\n",
    "        # 特别处理可选的 test_dataset\n",
    "        if var_name == 'test_dataset':\n",
    "            log.info(f\"可选的前置变量 '{var_name}' 未定义，将跳过测试集 DataLoader 的创建\")\n",
    "            test_dataset = None # 确保它被定义为None，以便后续逻辑正确处理\n",
    "            continue\n",
    "        log.critical(f\"DataLoader 配置失败：关键前置变量 '{var_name}' 未定义，请检查之前的单元格\")\n",
    "        exit(1)\n",
    "        \n",
    "if 'test_dataset' not in locals() and 'test_dataset' not in globals():\n",
    "    test_dataset = None # 确保 test_dataset 有定义，即使是 None\n",
    "\n",
    "log.info(\"所有 DataLoader 配置所需的核心前置变量均已定义\")\n",
    "log.info(f\"DataLoader 配置参数: Batch Size={BATCH_SIZE}, Num Workers={NUM_WORKERS}\")\n",
    "\n",
    "# --- 8.2 定义数据整理器 (Define Data Collator) ---\n",
    "# DataCollatorWithPadding 会接收一个批次的数据（字典列表）\n",
    "# 并使用传入的 tokenizer 来确定如何填充 (padding) input_ids, attention_mask等,使它们在批次内具有相同的长度\n",
    "# 即使之前的Dataset.map已经做了padding='max_length'到固定长度,\n",
    "# DataCollatorWithPadding 仍然确保批次正确组合\n",
    "log.info(\"创建 DataCollatorWithPadding...\")\n",
    "try:\n",
    "    collator = DataCollatorWithPadding(\n",
    "        tokenizer=tokenizer,\n",
    "        padding='longest' # 动态填充到批次内最长序列的长度\n",
    "    )\n",
    "    log.info(\"DataCollatorWithPadding 创建成功\")\n",
    "except Exception as e:\n",
    "    log.critical(f\"创建 DataCollatorWithPadding 失败: {e}\", exc_info=True)\n",
    "    exit(1)\n",
    "\n",
    "# --- 8.3 实例化训练集 DataLoader (Instantiate Training DataLoader) ---\n",
    "# `pin_memory=True` 可以加速CPU到GPU的数据传输\n",
    "# 如果内存不足或在某些特定环境（如CPU-only训练或某些虚拟机），可能需要设为 False\n",
    "pin_memory_enabled = torch.cuda.is_available() # 仅当CUDA可用时考虑启用\n",
    "log.info(f\"为训练集 DataLoader 设置 pin_memory={pin_memory_enabled}\")\n",
    "\n",
    "try:\n",
    "    log.info(f\"创建训练集 DataLoader (Batch Size: {BATCH_SIZE}, Shuffle: True, Num Workers: {NUM_WORKERS}, Pin Memory: {pin_memory_enabled})\")\n",
    "    train_data_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=collator,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=pin_memory_enabled\n",
    "    )\n",
    "    log.info(\"训练集 DataLoader 创建成功\")\n",
    "except Exception as e:\n",
    "    log.critical(f\"创建训练集 DataLoader 失败: {e}\", exc_info=True)\n",
    "    exit(1)\n",
    "\n",
    "# --- 8.4 实例化验证集 DataLoader (Instantiate Validation DataLoader) ---\n",
    "try:\n",
    "    log.info(f\"创建验证集 DataLoader (Batch Size: {BATCH_SIZE}, Shuffle: False, Num Workers: {NUM_WORKERS}, Pin Memory: {pin_memory_enabled})\")\n",
    "    dev_data_loader = DataLoader(\n",
    "        dataset=dev_dataset,\n",
    "        batch_size=BATCH_SIZE, # 通常验证/测试时可以使用更大的batch size（如果内存允许）\n",
    "        shuffle=False,\n",
    "        collate_fn=collator,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=pin_memory_enabled\n",
    "    )\n",
    "    log.info(\"验证集 DataLoader 创建成功\")\n",
    "except Exception as e:\n",
    "    log.critical(f\"创建验证集 DataLoader 失败: {e}\", exc_info=True)\n",
    "    exit(1)\n",
    "\n",
    "# --- 8.5 (可选) 实例化测试集 DataLoader (Optional: Instantiate Test DataLoader) ---\n",
    "test_data_loader = None # 初始化为 None\n",
    "if test_dataset is not None and len(test_dataset) > 0:\n",
    "    try:\n",
    "        log.info(f\"创建测试集 DataLoader (Batch Size: {BATCH_SIZE}, Shuffle: False, Num Workers: {NUM_WORKERS}, Pin Memory: {pin_memory_enabled})\")\n",
    "        test_data_loader = DataLoader(\n",
    "            dataset=test_dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            collate_fn=collator,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=pin_memory_enabled\n",
    "        )\n",
    "        log.info(\"测试集 DataLoader 创建成功\")\n",
    "    except Exception as e:\n",
    "        log.error(f\"创建测试集 DataLoader 失败: {e}\", exc_info=True) # 使用error而不是critical，因为测试集可能是可选的\n",
    "        log.warning(\"后续测试步骤可能无法执行\")\n",
    "        test_data_loader = None # 显式设为 None\n",
    "else:\n",
    "    if test_dataset is None:\n",
    "        log.info(\"未找到 'test_dataset' 或其为 None，跳过创建测试集 DataLoader\")\n",
    "    elif len(test_dataset) == 0:\n",
    "        log.warning(\"'test_dataset' 为空，跳过创建测试集 DataLoader\")\n",
    "    test_data_loader = None\n",
    "\n",
    "log.info(\"所有 DataLoader 配置完成\")\n",
    "\n",
    "# --- 8.6 (可选) 检查 DataLoader 输出 (Optional: Inspect DataLoader Output) ---\n",
    "if train_data_loader is not None:\n",
    "    log.info(\"-\" * 20 + \" 检查训练 DataLoader 输出 \" + \"-\" * 20)\n",
    "    try:\n",
    "        sample_batch = next(iter(train_data_loader))\n",
    "        log.info(f\"成功从 train_data_loader 获取一个批次\")\n",
    "        log.info(f\"批次包含的键: {list(sample_batch.keys())}\")\n",
    "        for key, value in sample_batch.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                log.info(f\"  - '{key}': shape={value.shape}, dtype={value.dtype}, device={value.device}\")\n",
    "            else: # 例如 'id' 列表（如果存在且未转为Tensor）\n",
    "                log.info(f\"  - '{key}': type={type(value)}, 示例值 (前5个): {value[:5] if isinstance(value, list) else value}\")\n",
    "        # 预期输出的关键张量:\n",
    "        # - 'input_ids': shape=[BATCH_SIZE, sequence_length], dtype=torch.int64\n",
    "        # - 'attention_mask': shape=[BATCH_SIZE, sequence_length], dtype=torch.int64\n",
    "        # - 'labels' (for train/dev): shape=[BATCH_SIZE], dtype=torch.int64\n",
    "        # - 'id' (for test, if kept): 可能是列表或Tensor，取决于 collate_fn 和 Dataset 格式\n",
    "    except StopIteration:\n",
    "        log.warning(\"无法从 train_data_loader 获取批次，可能是训练数据集为空\")\n",
    "    except Exception as e:\n",
    "        log.error(f\"尝试从 train_data_loader 获取或检查批次时出错: {e}\", exc_info=True)\n",
    "        if train_dataset is not None and len(train_dataset) > 0:\n",
    "            log.info(f\"打印一个原始训练样本 (HuggingFace Dataset 格式，应用set_format后): {train_dataset[0]}\")\n",
    "else:\n",
    "    log.warning(\"train_data_loader 未成功创建或为 None，跳过批次检查\")\n",
    "\n",
    "log.info(\"DataLoader 配置已准备就绪，后续步骤可以定义优化器、学习率调度器等\")\n",
    "log.info(\"=\"*30 + \" DataLoaders 配置结束 \" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "# --- 确保关键变量已定义 ---\n",
    "assert 'collator' in locals(), \"collator 未定义\"\n",
    "assert 'train_data_loader' in locals() and train_data_loader is not None, \"train_data_loader 未成功创建或未定义\"\n",
    "assert 'dev_data_loader' in locals() and dev_data_loader is not None, \"dev_data_loader 未成功创建或未定义\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1fe3ff-f7a0-4549-b1d3-3862f88194a4",
   "metadata": {},
   "source": [
    "## 优化器、学习率调度器与评估指标配置(Optimizer, Learning Rate Scheduler & Metrics Configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd4ae57c-9b29-4097-96fe-ffc64e5e200c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 16:53:08,690 - __main__ - INFO - ============================== 开始配置优化器、学习率调度器与评估指标 ==============================\n",
      "2025-05-18 16:53:08,691 - __main__ - INFO - 检查优化器等配置所需的前置变量...\n",
      "2025-05-18 16:53:08,691 - __main__ - INFO - 所有优化器等配置所需的前置变量均已定义\n",
      "2025-05-18 16:53:08,692 - __main__ - INFO - 训练超参数: Learning Rate=2e-05, Epochs=6, Warmup Proportion=0.1, Weight Decay=0.01\n",
      "2025-05-18 16:53:08,692 - __main__ - INFO - 定义优化器 (AdamW)...\n",
      "2025-05-18 16:53:08,694 - __main__ - INFO - AdamW 优化器创建成功Learning Rate: 2e-05, Weight Decay: 0.01, Eps: 1e-8\n",
      "2025-05-18 16:53:08,695 - __main__ - INFO - 定义学习率调度器 (get_linear_schedule_with_warmup)...\n",
      "2025-05-18 16:53:08,695 - __main__ - INFO - 总训练步数 (for LR Scheduler): 17640\n",
      "2025-05-18 16:53:08,696 - __main__ - INFO - 预热步数 (Warmup Steps): 1764\n",
      "2025-05-18 16:53:08,696 - __main__ - INFO - 学习率调度器创建成功\n",
      "2025-05-18 16:53:08,696 - __main__ - INFO - 定义评估指标 (Accuracy 和 F1-Score 使用 evaluate.load)...\n",
      "2025-05-18 16:53:08,706 - __main__ - INFO - Accuracy 指标加载成功\n",
      "2025-05-18 16:53:08,716 - __main__ - INFO - F1-Score 指标加载成功\n",
      "2025-05-18 16:53:08,717 - __main__ - INFO - 评估指标加载过程完成\n",
      "2025-05-18 16:53:08,721 - __main__ - INFO - 超参数已记录到 TensorBoard: ./files/logs/tensorboard_runs/20250518-165241\n",
      "2025-05-18 16:53:08,721 - __main__ - INFO - 优化器、学习率调度器、评估指标配置完成\n",
      "2025-05-18 16:53:08,721 - __main__ - INFO - ============================== 优化器等配置结束 ==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 主要功能:\n",
    "# 1. 检查前置步骤定义的关键变量和配置参数\n",
    "# 2. 定义训练超参数 (Learning Rate, Epochs, Warmup Proportion, Weight Decay)\n",
    "# 3. 创建优化器 (AdamW)\n",
    "# 4. 根据训练步数和预热比例计算预热步数，并创建学习率调度器\n",
    "#    (get_linear_schedule_with_warmup)\n",
    "# 5. 加载评估指标 (Accuracy, F1-Score) 使用 `evaluate` 库\n",
    "# 6. 将配置的超参数记录到 TensorBoard\n",
    "# ==============================================================================\n",
    "log.info(\"=\"*30 + \" 开始配置优化器、学习率调度器与评估指标 \" + \"=\"*30)\n",
    "\n",
    "# --- 9.1 检查前置条件 (Check Prerequisites) ---\n",
    "required_vars_for_optimizer_setup = [\n",
    "    'model', 'train_data_loader',\n",
    "    'LEARNING_RATE', 'NUM_EPOCHS', 'WARMUP_PROPORTION', 'WEIGHT_DECAY'\n",
    "]\n",
    "log.info(\"检查优化器等配置所需的前置变量...\")\n",
    "for var_name in required_vars_for_optimizer_setup:\n",
    "    if var_name not in locals() and var_name not in globals():\n",
    "        log.critical(f\"配置失败：关键前置变量 '{var_name}' 未定义，请检查之前的单元格\")\n",
    "        exit(1)\n",
    "log.info(\"所有优化器等配置所需的前置变量均已定义\")\n",
    "\n",
    "log.info(f\"训练超参数: Learning Rate={LEARNING_RATE}, Epochs={NUM_EPOCHS}, \"\n",
    "         f\"Warmup Proportion={WARMUP_PROPORTION}, Weight Decay={WEIGHT_DECAY}\")\n",
    "\n",
    "# --- 9.2 定义优化器 (Define Optimizer) ---\n",
    "log.info(\"定义优化器 (AdamW)...\")\n",
    "try:\n",
    "    optimizer = AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), # 只优化需要梯度的参数\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        eps=1e-8 # AdamW的epsilon参数，增加数值稳定性，Hugging Face Trainer常用此值\n",
    "    )\n",
    "    log.info(f\"AdamW 优化器创建成功Learning Rate: {LEARNING_RATE}, Weight Decay: {WEIGHT_DECAY}, Eps: 1e-8\")\n",
    "except Exception as e:\n",
    "    log.critical(f\"创建 AdamW 优化器失败: {e}\", exc_info=True)\n",
    "    exit(1)\n",
    "\n",
    "# --- 9.3 定义学习率调度器 (Define Learning Rate Scheduler) ---\n",
    "log.info(\"定义学习率调度器 (get_linear_schedule_with_warmup)...\")\n",
    "\n",
    "# 检查训练数据加载器和Epoch数\n",
    "if train_data_loader is None or len(train_data_loader) == 0:\n",
    "    log.critical(\"训练数据加载器 (train_data_loader) 为空或未定义，无法计算总训练步数\")\n",
    "    exit(1)\n",
    "if NUM_EPOCHS <= 0:\n",
    "    log.critical(f\"训练轮数 (NUM_EPOCHS) 必须为正数，当前为: {NUM_EPOCHS}\")\n",
    "    exit(1)\n",
    "if not (0.0 <= WARMUP_PROPORTION <= 1.0):\n",
    "    log.critical(f\"学习率预热比例 (WARMUP_PROPORTION) 必须在 [0.0, 1.0] 之间，当前为: {WARMUP_PROPORTION}\")\n",
    "    exit(1)\n",
    "\n",
    "num_training_steps = NUM_EPOCHS * len(train_data_loader)\n",
    "num_warmup_steps = int(WARMUP_PROPORTION * num_training_steps)\n",
    "\n",
    "if num_warmup_steps > num_training_steps:\n",
    "    log.warning(f\"计算得到的预热步数 ({num_warmup_steps}) 大于总训练步数 ({num_training_steps})，将预热步数调整为总训练步数\")\n",
    "    num_warmup_steps = num_training_steps\n",
    "if num_training_steps == 0 :\n",
    "    log.critical(\"总训练步数为0，无法创建学习率调度器\")\n",
    "    exit(1)\n",
    "\n",
    "log.info(f\"总训练步数 (for LR Scheduler): {num_training_steps}\")\n",
    "log.info(f\"预热步数 (Warmup Steps): {num_warmup_steps}\")\n",
    "\n",
    "try:\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    log.info(\"学习率调度器创建成功\")\n",
    "except Exception as e:\n",
    "    log.critical(f\"创建学习率调度器失败: {e}\", exc_info=True)\n",
    "    exit(1)\n",
    "\n",
    "# --- 9.4 定义评估指标 (Define Evaluation Metrics) ---\n",
    "log.info(\"定义评估指标 (Accuracy 和 F1-Score 使用 evaluate.load)...\")\n",
    "accuracy_metric = None\n",
    "f1_metric = None\n",
    "try:\n",
    "    # 尝试加载 accuracy 指标\n",
    "    try:\n",
    "        accuracy_metric = evaluate.load(\"accuracy\")\n",
    "        log.info(\"Accuracy 指标加载成功\")\n",
    "    except ModuleNotFoundError:\n",
    "        log.error(\"Hugging Face 'evaluate' 库或 'accuracy' 指标模块未找到，请确保已正确安装: pip install evaluate scikit-learn\") \n",
    "        log.warning(\"将跳过 Accuracy 指标的计算\")\n",
    "    except Exception as e_acc:\n",
    "        log.error(f\"加载 Accuracy 指标失败: {e_acc}\")\n",
    "        log.warning(\"将跳过 Accuracy 指标的计算\")\n",
    "\n",
    "    # 尝试加载 f1 指标\n",
    "    try:\n",
    "        f1_metric = evaluate.load(\"f1\")\n",
    "        log.info(\"F1-Score 指标加载成功\")\n",
    "    except ModuleNotFoundError:\n",
    "        log.error(\"Hugging Face 'evaluate' 库或 'f1' 指标模块未找到，请确保已正确安装: pip install evaluate scikit-learn\") \n",
    "        log.warning(\"将跳过 F1-Score 指标的计算\")\n",
    "    except Exception as e_f1:\n",
    "        log.error(f\"加载 F1-Score 指标失败: {e_f1}\")\n",
    "        log.warning(\"将跳过 F1-Score 指标的计算\")\n",
    "\n",
    "    if accuracy_metric is None and f1_metric is None:\n",
    "        log.warning(\"所有评估指标均加载失败，训练仍可进行，但不会计算验证集指标\")\n",
    "    else:\n",
    "        log.info(\"评估指标加载过程完成\")\n",
    "\n",
    "except Exception as e_load_main: # 捕获 evaluate.load 外层可能发生的其他问题\n",
    "    log.error(f\"加载评估指标过程中发生意外错误: {e_load_main}\", exc_info=True)\n",
    "    accuracy_metric = None\n",
    "    f1_metric = None\n",
    "    log.warning(\"所有评估指标均设置为 None\")\n",
    "\n",
    "\n",
    "# --- 9.5 将超参数记录到 TensorBoard ---\n",
    "# 此处记录的指标只是占位符，实际指标值在训练循环中计算并记录\n",
    "# 你也可以在训练开始前就这样记录一次配置\n",
    "try:\n",
    "    hparams = {\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'num_epochs': NUM_EPOCHS,\n",
    "        'batch_size': BATCH_SIZE, \n",
    "        'warmup_proportion': WARMUP_PROPORTION,\n",
    "        'weight_decay': WEIGHT_DECAY,\n",
    "        'max_seq_length': MAX_SEQ_LENGTH, \n",
    "        'model_name': PRETRAINED_MODEL_NAME \n",
    "    }\n",
    "    # Tensorboard 的 add_hparams 通常期望有一个指标字典，即使是初始值\n",
    "    # 这里的 metric_dict 是为了满足 API 要求，实际的指标值会在训练中更新\n",
    "    metric_dict_placeholder = {\n",
    "        'hparam/accuracy': 0,\n",
    "        'hparam/f1': 0\n",
    "    }\n",
    "    writer.add_hparams(hparams, metric_dict_placeholder)\n",
    "    log.info(f\"超参数已记录到 TensorBoard: {TENSORBOARD_LOG_DIR}\")\n",
    "except Exception as e_tb:\n",
    "    log.warning(f\"记录超参数到 TensorBoard 时发生错误: {e_tb}\")\n",
    "\n",
    "\n",
    "log.info(\"优化器、学习率调度器、评估指标配置完成\")\n",
    "log.info(\"=\"*30 + \" 优化器等配置结束 \" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "# --- 确保关键变量已定义 ---\n",
    "assert 'optimizer' in locals() and optimizer is not None, \"优化器 (optimizer) 未成功创建或未定义\"\n",
    "assert 'lr_scheduler' in locals() and lr_scheduler is not None, \"学习率调度器 (lr_scheduler) 未成功创建或未定义\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abe42ac-83f5-40bc-9b3f-4186a1fc05c9",
   "metadata": {},
   "source": [
    "## 模型训练与验证循环 (Model Training & Validation Loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9929fb-59f2-4b31-9b48-e83de9daa22a",
   "metadata": {},
   "source": [
    "### eval函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9657b823-0dc9-4e94-9625-1f2da2ee7cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 16:53:51,499 - __main__ - INFO - ============================== 开始模型训练与验证 ==============================\n",
      "2025-05-18 16:53:51,500 - __main__ - INFO - 检查训练与验证所需的前置变量...\n",
      "2025-05-18 16:53:51,501 - __main__ - INFO - 所有训练与验证所需的前置变量均已定义。\n",
      "2025-05-18 16:53:51,502 - __main__ - INFO - 早停机制已启用: Patience=10, Metric='val_loss', Min Delta=0.001\n",
      "2025-05-18 16:53:51,502 - __main__ - INFO - 早停监控模式: min (指标: val_loss)\n",
      "2025-05-18 16:53:51,503 - __main__ - INFO - 按步数验证策略已启用：每训练 588 步 (约占epoch的 20.0%) 进行一次验证。\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 主要功能:\n",
    "# 1. 实现完整的训练周期 (多 Epochs)\n",
    "# 2. 在每个 Epoch 内:\n",
    "#    a. 训练阶段 (Training):\n",
    "#       - 设置模型为训练模式 (model.train())。\n",
    "#       - 迭代训练数据加载器 (train_data_loader)。\n",
    "#       - 执行前向传播、计算损失、反向传播、更新优化器和学习率。\n",
    "#       - 记录训练损失到日志和 TensorBoard。\n",
    "#    b. 定期验证阶段 (Intra-Epoch/End-of-Epoch Evaluation):\n",
    "#       - 根据配置的 EVAL_STRATEGY 和 EVAL_FREQUENCY_FRAC_EPOCH，\n",
    "#         在训练过程中或每个 Epoch 结束后执行验证。\n",
    "#       - 设置模型为评估模式 (model.eval())。\n",
    "#       - 迭代验证数据加载器 (dev_data_loader)。\n",
    "#       - 计算验证集上的损失、准确率、F1分数等指标。\n",
    "#       - 记录验证指标到日志和 TensorBoard。\n",
    "# 3. 模型保存:\n",
    "#    - 根据验证集上的主要评估指标 (如准确率) 保存表现最佳的模型权重。\n",
    "# 4. 早停机制 (Early Stopping):\n",
    "#    - 监控验证集上的指定指标 (EARLY_STOPPING_METRIC)。\n",
    "#    - 如果指标在连续 EARLY_STOPPING_PATIENCE 次评估中没有明显改善\n",
    "#      (超过 EARLY_STOPPING_MIN_DELTA)，则提前终止训练。\n",
    "# 5. 记录整体训练耗时和最终的最佳指标。\n",
    "# ==============================================================================\n",
    "log.info(\"=\"*30 + \" 开始模型训练与验证 \" + \"=\"*30)\n",
    "log.debug(f\"evaluate_model: Global accuracy_metric type: {type(accuracy_metric)}, value: {accuracy_metric is not None}\")\n",
    "log.debug(f\"evaluate_model: Global f1_metric type: {type(f1_metric)}, value: {f1_metric is not None}\")\n",
    "# --- 10.1 检查前置条件 (Check Prerequisites) ---\n",
    "required_vars_for_training = [\n",
    "    'model', 'optimizer', 'lr_scheduler',\n",
    "    'train_data_loader', 'dev_data_loader',\n",
    "    'NUM_EPOCHS', 'device', 'log', 'writer', \n",
    "    'BEST_MODEL_PATH',\n",
    "    'EVAL_STRATEGY', \n",
    "    'EARLY_STOPPING_PATIENCE','EARLY_STOPPING_METRIC','EARLY_STOPPING_MIN_DELTA'\n",
    "]\n",
    "if EVAL_STRATEGY == \"steps\":\n",
    "    required_vars_for_training.append('EVAL_FREQUENCY_FRAC_EPOCH')\n",
    "\n",
    "log.info(\"检查训练与验证所需的前置变量...\")\n",
    "for var_name in required_vars_for_training:\n",
    "    if var_name not in locals() and var_name not in globals():\n",
    "        log.critical(f\"训练失败：关键前置变量 '{var_name}' 未定义，请检查之前的单元格。\")\n",
    "        exit(1)\n",
    "log.info(\"所有训练与验证所需的前置变量均已定义。\")\n",
    "\n",
    "# --- 10.2 初始化训练状态变量 (Initialize Training State Variables) ---\n",
    "best_val_metric_for_saving = -float('inf') # 用于保存模型的最佳指标值 (假设是准确率，越大越好)\n",
    "                                           # 如果是loss，则应为 float('inf')\n",
    "current_best_model_info = {\"epoch\": 0, \"step\":0, \"val_accuracy\": 0.0, \"val_f1\": 0.0, \"val_loss\": float('inf')}\n",
    "\n",
    "\n",
    "global_step = 0  # 用于 TensorBoard 的全局步数计数\n",
    "training_completed_normally = False # 标记训练是否正常完成所有epoch\n",
    "\n",
    "# 早停机制变量初始化\n",
    "if EARLY_STOPPING_ENABLED:\n",
    "    log.info(f\"早停机制已启用: Patience={EARLY_STOPPING_PATIENCE}, Metric='{EARLY_STOPPING_METRIC}', Min Delta={EARLY_STOPPING_MIN_DELTA}\")\n",
    "    epochs_no_improve = 0 # 连续多少次评估没有改善\n",
    "    # 根据监控的指标确定初始最佳值和比较模式\n",
    "    if EARLY_STOPPING_METRIC.endswith('loss'):\n",
    "        best_early_stopping_metric_val = float('inf')\n",
    "        early_stopping_mode = 'min'\n",
    "    else: # accuracy, f1\n",
    "        best_early_stopping_metric_val = -float('inf')\n",
    "        early_stopping_mode = 'max'\n",
    "    # 如果在配置中显式设置了 EARLY_STOPPING_MODE，则覆盖自动推断\n",
    "    if 'EARLY_STOPPING_MODE' in locals() and EARLY_STOPPING_MODE in ['min', 'max']:\n",
    "        early_stopping_mode = EARLY_STOPPING_MODE\n",
    "        if early_stopping_mode == 'min':\n",
    "            best_early_stopping_metric_val = float('inf')\n",
    "        else:\n",
    "            best_early_stopping_metric_val = -float('inf')\n",
    "    log.info(f\"早停监控模式: {early_stopping_mode} (指标: {EARLY_STOPPING_METRIC})\")\n",
    "\n",
    "\n",
    "# 计算按步数验证的频率\n",
    "eval_every_n_steps = 0\n",
    "if EVAL_STRATEGY == \"steps\":\n",
    "    if not (0 < EVAL_FREQUENCY_FRAC_EPOCH <= 1.0):\n",
    "        log.warning(f\"EVAL_FREQUENCY_FRAC_EPOCH ({EVAL_FREQUENCY_FRAC_EPOCH}) 不在 (0, 1] 范围内，将调整为每个epoch结束时验证。\")\n",
    "        EVAL_STRATEGY = \"epoch\" # 回退到按epoch验证\n",
    "    else:\n",
    "        eval_every_n_steps = max(1, int(len(train_data_loader) * EVAL_FREQUENCY_FRAC_EPOCH))\n",
    "        log.info(f\"按步数验证策略已启用：每训练 {eval_every_n_steps} 步 (约占epoch的 {EVAL_FREQUENCY_FRAC_EPOCH*100:.1f}%) 进行一次验证。\")\n",
    "else:\n",
    "    log.info(\"按Epoch结束验证策略已启用。\")\n",
    "\n",
    "\n",
    "# --- 10.3 验证函数定义 (Define Evaluation Function) ---\n",
    "\n",
    "def evaluate_model(current_model, dataloader, current_device, eval_step_info=\"\"):\n",
    "    log.info(f\"开始验证... {eval_step_info}\")\n",
    "    current_model.eval() # 设置模型为评估模式\n",
    "\n",
    "    total_eval_loss = 0\n",
    "\n",
    "    log.debug(f\"evaluate_model: Global accuracy_metric type: {type(accuracy_metric)}, value: {accuracy_metric is not None}\")\n",
    "    log.debug(f\"evaluate_model: Global f1_metric type: {type(f1_metric)}, value: {f1_metric is not None}\")\n",
    "\n",
    "\n",
    "    can_evaluate_accuracy = accuracy_metric is not None # 直接使用全局变量\n",
    "    can_evaluate_f1 = f1_metric is not None             # 直接使用全局变量\n",
    "\n",
    "    if not can_evaluate_accuracy and not can_evaluate_f1:\n",
    "        log.warning(\"所有评估指标均未初始化或加载失败，无法计算详细验证指标。仅计算验证损失。\")\n",
    "        # 注意：这里的警告信息可以更精确，因为它现在反映的是全局变量的状态\n",
    "\n",
    "    val_progress_bar = tqdm(dataloader, desc=f\"Validation {eval_step_info}\", leave=False, position=0)\n",
    "    with torch.no_grad(): # 无需计算梯度\n",
    "        for batch in val_progress_bar:\n",
    "            input_ids = batch['input_ids'].to(current_device)\n",
    "            attention_mask = batch['attention_mask'].to(current_device)\n",
    "            labels = batch['labels'].to(current_device)\n",
    "\n",
    "            outputs = current_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            total_eval_loss += loss.item()\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            if can_evaluate_accuracy:\n",
    "                accuracy_metric.add_batch(predictions=predictions.cpu(), references=labels.cpu()) # 使用全局 accuracy_metric\n",
    "            if can_evaluate_f1:\n",
    "                f1_metric.add_batch(predictions=predictions.cpu(), references=labels.cpu())       # 使用全局 f1_metric\n",
    "\n",
    "    val_progress_bar.close()\n",
    "    avg_val_loss = total_eval_loss / len(dataloader) if len(dataloader) > 0 else float('inf')\n",
    "    log.info(f\"验证完成. 平均验证损失: {avg_val_loss:.4f}\")\n",
    "\n",
    "    val_accuracy = -1.0\n",
    "    val_f1_weighted = -1.0\n",
    "\n",
    "    if can_evaluate_accuracy:\n",
    "        try:\n",
    "            acc_result = accuracy_metric.compute() # 使用全局 accuracy_metric\n",
    "            val_accuracy = acc_result['accuracy']\n",
    "            log.info(f\"  验证准确率 (Accuracy): {val_accuracy:.4f}\")\n",
    "        except Exception as e:\n",
    "            log.error(f\"计算验证 Accuracy 时出错: {e}\")\n",
    "    else:\n",
    "        log.debug(\"Accuracy 指标未加载或不可用，跳过计算。\") # 更新日志信息\n",
    "\n",
    "    if can_evaluate_f1:\n",
    "        try:\n",
    "            f1_result = f1_metric.compute(average=\"weighted\") # 使用全局 f1_metric\n",
    "            val_f1_weighted = f1_result['f1']\n",
    "            log.info(f\"  验证加权 F1-Score: {val_f1_weighted:.4f}\")\n",
    "        except Exception as e:\n",
    "            log.error(f\"计算验证 F1-Score 时出错: {e}\")\n",
    "    else:\n",
    "        log.debug(\"F1-Score 指标未加载或不可用，跳过计算。\") # 更新日志信息\n",
    "\n",
    "    return avg_val_loss, val_accuracy, val_f1_weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544023c7-9ded-48cb-87e1-58814a1178d6",
   "metadata": {},
   "source": [
    "### 训练主循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e330fe49-2dd1-427a-818d-a2430fcab5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 16:53:56,385 - __main__ - INFO - --- Epoch 1/6 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc32c4f11594e07bdb681b279912576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 Training:   0%|          | 0/2940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 17:01:24,592 - __main__ - INFO - 开始验证... Epoch 1, Step 588/2940\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d31b51c7e184bbea801e7f794ded477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 1, Step 588/2940:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 17:02:51,956 - __main__ - INFO - 验证完成. 平均验证损失: 0.2121\n",
      "2025-05-18 17:02:52,044 - __main__ - INFO -   验证准确率 (Accuracy): 0.9372\n",
      "2025-05-18 17:02:52,151 - __main__ - INFO -   验证加权 F1-Score: 0.9371\n",
      "2025-05-18 17:02:52,153 - __main__ - INFO - 🎉 新的最佳模型 (基于Accuracy)! Accuracy: 0.9372 at Epoch 1, Step 588/2940. 保存模型到 ./files/saved_models/best_roberta_model.pt\n",
      "2025-05-18 17:03:23,418 - __main__ - INFO - 早停指标改善: val_loss 从 inf -> 0.2121. 重置Patience计数器。\n",
      "2025-05-18 17:10:50,672 - __main__ - INFO - 开始验证... Epoch 1, Step 1176/2940\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15a9593458a4309b40e9c1de86cde43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 1, Step 1176/2940:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 17:12:18,069 - __main__ - INFO - 验证完成. 平均验证损失: 0.1575\n",
      "2025-05-18 17:12:18,167 - __main__ - INFO -   验证准确率 (Accuracy): 0.9511\n",
      "2025-05-18 17:12:18,282 - __main__ - INFO -   验证加权 F1-Score: 0.9512\n",
      "2025-05-18 17:12:18,285 - __main__ - INFO - 🎉 新的最佳模型 (基于Accuracy)! Accuracy: 0.9511 at Epoch 1, Step 1176/2940. 保存模型到 ./files/saved_models/best_roberta_model.pt\n",
      "2025-05-18 17:12:19,630 - __main__ - INFO - 早停指标改善: val_loss 从 0.2121 -> 0.1575. 重置Patience计数器。\n",
      "2025-05-18 17:19:47,743 - __main__ - INFO - 开始验证... Epoch 1, Step 1764/2940\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89411ea2fb6745d28039c27437fba57f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 1, Step 1764/2940:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 17:29:43,644 - __main__ - INFO - 开始验证... Epoch 1, Step 2352/2940\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f966f2af718a44c9aaa559e64c2f37db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 1, Step 2352/2940:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 17:31:10,460 - __main__ - INFO - 验证完成. 平均验证损失: 0.1220\n",
      "2025-05-18 17:31:10,547 - __main__ - INFO -   验证准确率 (Accuracy): 0.9607\n",
      "2025-05-18 17:31:10,656 - __main__ - INFO -   验证加权 F1-Score: 0.9607\n",
      "2025-05-18 17:31:10,659 - __main__ - INFO - 🎉 新的最佳模型 (基于Accuracy)! Accuracy: 0.9607 at Epoch 1, Step 2352/2940. 保存模型到 ./files/saved_models/best_roberta_model.pt\n",
      "2025-05-18 17:31:12,590 - __main__ - INFO - 早停指标改善: val_loss 从 0.1400 -> 0.1220. 重置Patience计数器。\n",
      "2025-05-18 17:39:08,717 - __main__ - INFO - 开始验证... Epoch 1, Step 2940/2940\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b82ddb62eca44773a45c77b95a0916a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 1, Step 2940/2940:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 17:40:36,115 - __main__ - INFO - 验证完成. 平均验证损失: 0.1011\n",
      "2025-05-18 17:40:36,263 - __main__ - INFO -   验证准确率 (Accuracy): 0.9673\n",
      "2025-05-18 17:40:36,446 - __main__ - INFO -   验证加权 F1-Score: 0.9673\n",
      "2025-05-18 17:40:36,449 - __main__ - INFO - 🎉 新的最佳模型 (基于Accuracy)! Accuracy: 0.9673 at Epoch 1, Step 2940/2940. 保存模型到 ./files/saved_models/best_roberta_model.pt\n",
      "2025-05-18 17:40:38,248 - __main__ - INFO - 早停指标改善: val_loss 从 0.1220 -> 0.1011. 重置Patience计数器。\n",
      "2025-05-18 17:40:38,388 - __main__ - INFO - Epoch 1 训练阶段完成. 平均训练损失: 0.3092\n",
      "2025-05-18 17:40:38,389 - __main__ - INFO - Epoch 1 总耗时: 2802.00 秒\n",
      "2025-05-18 17:40:38,389 - __main__ - INFO - --- Epoch 2/6 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad989d4a90c439d9a39bab07b373dda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 Training:   0%|          | 0/2940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 17:48:08,298 - __main__ - INFO - 开始验证... Epoch 2, Step 588/2940\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b4bae573979404a8167c7de64c3e6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 2, Step 588/2940:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: \n",
      "can only test a child processException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionErrorcan only test a child process: \n",
      "2025-05-18 17:49:35,714 - __main__ - INFO - 验证完成. 平均验证损失: 0.0918\n",
      "2025-05-18 17:49:35,800 - __main__ - INFO -   验证准确率 (Accuracy): 0.9709\n",
      "2025-05-18 17:49:35,910 - __main__ - INFO -   验证加权 F1-Score: 0.9709\n",
      "2025-05-18 17:49:35,913 - __main__ - INFO - 🎉 新的最佳模型 (基于Accuracy)! Accuracy: 0.9709 at Epoch 2, Step 588/2940. 保存模型到 ./files/saved_models/best_roberta_model.pt\n",
      "2025-05-18 17:49:37,562 - __main__ - INFO - 早停指标改善: val_loss 从 0.1011 -> 0.0918. 重置Patience计数器。\n",
      "2025-05-18 17:57:34,415 - __main__ - INFO - 开始验证... Epoch 2, Step 1176/2940\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2644367436364b2699ed2b098256ef1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 2, Step 1176/2940:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 17:59:01,039 - __main__ - INFO - 验证完成. 平均验证损失: 0.0819\n",
      "2025-05-18 17:59:01,126 - __main__ - INFO -   验证准确率 (Accuracy): 0.9735\n",
      "2025-05-18 17:59:01,238 - __main__ - INFO -   验证加权 F1-Score: 0.9734\n",
      "2025-05-18 17:59:01,241 - __main__ - INFO - 🎉 新的最佳模型 (基于Accuracy)! Accuracy: 0.9735 at Epoch 2, Step 1176/2940. 保存模型到 ./files/saved_models/best_roberta_model.pt\n",
      "2025-05-18 17:59:02,905 - __main__ - INFO - 早停指标改善: val_loss 从 0.0918 -> 0.0819. 重置Patience计数器。\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "2025-05-18 18:17:50,687 - __main__ - INFO - 验证完成. 平均验证损失: 0.0685\n",
      "2025-05-18 18:17:50,773 - __main__ - INFO -   验证准确率 (Accuracy): 0.9775\n",
      "2025-05-18 18:17:50,882 - __main__ - INFO -   验证加权 F1-Score: 0.9775\n",
      "2025-05-18 18:17:50,885 - __main__ - INFO - 🎉 新的最佳模型 (基于Accuracy)! Accuracy: 0.9775 at Epoch 2, Step 2352/2940. 保存模型到 ./files/saved_models/best_roberta_model.pt\n",
      "2025-05-18 18:17:52,465 - __main__ - INFO - 早停指标改善: val_loss 从 0.0737 -> 0.0685. 重置Patience计数器。\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "2025-05-18 18:25:19,628 - __main__ - INFO - 开始验证... Epoch 2, Step 2940/2940\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ab3068207e842f9889d4e92ada9ffeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 2, Step 2940/2940:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 18:26:46,369 - __main__ - INFO - 验证完成. 平均验证损失: 0.0617\n",
      "2025-05-18 18:26:46,453 - __main__ - INFO -   验证准确率 (Accuracy): 0.9798\n",
      "2025-05-18 18:26:46,578 - __main__ - INFO -   验证加权 F1-Score: 0.9799\n",
      "2025-05-18 18:26:46,581 - __main__ - INFO - 🎉 新的最佳模型 (基于Accuracy)! Accuracy: 0.9798 at Epoch 2, Step 2940/2940. 保存模型到 ./files/saved_models/best_roberta_model.pt\n",
      "2025-05-18 18:27:17,883 - __main__ - INFO - 早停指标改善: val_loss 从 0.0685 -> 0.0617. 重置Patience计数器。\n",
      "2025-05-18 18:27:18,013 - __main__ - INFO - Epoch 2 训练阶段完成. 平均训练损失: 0.1079\n",
      "2025-05-18 18:27:18,014 - __main__ - INFO - Epoch 2 总耗时: 2799.63 秒\n",
      "2025-05-18 18:27:18,015 - __main__ - INFO - --- Epoch 3/6 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "179c6d382cc744058497909b8ba57e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 Training:   0%|          | 0/2940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 18:35:45,146 - __main__ - INFO - 开始验证... Epoch 3, Step 588/2940\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2f76175254045f9bcfd4a709407a4b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 3, Step 588/2940:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 18:37:11,327 - __main__ - INFO - 验证完成. 平均验证损失: 0.0581\n",
      "2025-05-18 18:37:11,421 - __main__ - INFO -   验证准确率 (Accuracy): 0.9804\n",
      "2025-05-18 18:37:11,541 - __main__ - INFO -   验证加权 F1-Score: 0.9804\n",
      "2025-05-18 18:37:11,544 - __main__ - INFO - 🎉 新的最佳模型 (基于Accuracy)! Accuracy: 0.9804 at Epoch 3, Step 588/2940. 保存模型到 ./files/saved_models/best_roberta_model.pt\n",
      "2025-05-18 18:37:13,227 - __main__ - INFO - 早停指标改善: val_loss 从 0.0617 -> 0.0581. 重置Patience计数器。\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "2025-05-18 18:45:07,384 - __main__ - INFO - 开始验证... Epoch 3, Step 1176/2940\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f9b1464750043febf4a96831572caa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 3, Step 1176/2940:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 18:46:34,552 - __main__ - INFO - 验证完成. 平均验证损失: 0.0497\n",
      "2025-05-18 18:46:34,640 - __main__ - INFO -   验证准确率 (Accuracy): 0.9835\n",
      "2025-05-18 18:46:34,750 - __main__ - INFO -   验证加权 F1-Score: 0.9835\n",
      "2025-05-18 18:46:34,753 - __main__ - INFO - 🎉 新的最佳模型 (基于Accuracy)! Accuracy: 0.9835 at Epoch 3, Step 1176/2940. 保存模型到 ./files/saved_models/best_roberta_model.pt\n",
      "2025-05-18 18:46:36,629 - __main__ - INFO - 早停指标改善: val_loss 从 0.0581 -> 0.0497. 重置Patience计数器。\n",
      "2025-05-18 18:54:02,407 - __main__ - INFO - 开始验证... Epoch 3, Step 1764/2940\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ba8b1b0a09443b9b35de80683ed8ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 3, Step 1764/2940:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 18:55:29,599 - __main__ - INFO - 验证完成. 平均验证损失: 0.0443\n",
      "2025-05-18 18:55:29,686 - __main__ - INFO -   验证准确率 (Accuracy): 0.9853\n",
      "2025-05-18 18:55:29,796 - __main__ - INFO -   验证加权 F1-Score: 0.9853\n",
      "2025-05-18 18:55:29,799 - __main__ - INFO - 🎉 新的最佳模型 (基于Accuracy)! Accuracy: 0.9853 at Epoch 3, Step 1764/2940. 保存模型到 ./files/saved_models/best_roberta_model.pt\n",
      "2025-05-18 18:55:31,447 - __main__ - INFO - 早停指标改善: val_loss 从 0.0497 -> 0.0443. 重置Patience计数器。\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "2025-05-18 19:03:55,575 - __main__ - INFO - 开始验证... Epoch 3, Step 2352/2940\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8402b54eff884e31a27eaaec417c7820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 3, Step 2352/2940:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 19:05:22,090 - __main__ - INFO - 验证完成. 平均验证损失: 0.0418\n",
      "2025-05-18 19:05:22,177 - __main__ - INFO -   验证准确率 (Accuracy): 0.9861\n",
      "2025-05-18 19:05:22,287 - __main__ - INFO -   验证加权 F1-Score: 0.9861\n",
      "2025-05-18 19:05:22,290 - __main__ - INFO - 🎉 新的最佳模型 (基于Accuracy)! Accuracy: 0.9861 at Epoch 3, Step 2352/2940. 保存模型到 ./files/saved_models/best_roberta_model.pt\n",
      "2025-05-18 19:05:53,903 - __main__ - INFO - 早停指标改善: val_loss 从 0.0443 -> 0.0418. 重置Patience计数器。\n",
      "2025-05-18 19:13:18,902 - __main__ - INFO - 开始验证... Epoch 3, Step 2940/2940\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "503b544b7aa44c559095d82f9d8ad6eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 3, Step 2940/2940:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 19:14:45,421 - __main__ - INFO - 验证完成. 平均验证损失: 0.0371\n",
      "2025-05-18 19:14:45,507 - __main__ - INFO -   验证准确率 (Accuracy): 0.9879\n",
      "2025-05-18 19:14:45,616 - __main__ - INFO -   验证加权 F1-Score: 0.9879\n",
      "2025-05-18 19:14:45,618 - __main__ - INFO - 🎉 新的最佳模型 (基于Accuracy)! Accuracy: 0.9879 at Epoch 3, Step 2940/2940. 保存模型到 ./files/saved_models/best_roberta_model.pt\n",
      "2025-05-18 19:14:47,255 - __main__ - INFO - 早停指标改善: val_loss 从 0.0418 -> 0.0371. 重置Patience计数器。\n",
      "2025-05-18 19:14:47,385 - __main__ - INFO - Epoch 3 训练阶段完成. 平均训练损失: 0.0681\n",
      "2025-05-18 19:14:47,386 - __main__ - INFO - Epoch 3 总耗时: 2849.37 秒\n",
      "2025-05-18 19:14:47,387 - __main__ - INFO - --- Epoch 4/6 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51acef64a578473aad4921c7b65fe479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4 Training:   0%|          | 0/2940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: Exception ignored in: Exception ignored in: Exception ignored in: Exception ignored in: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0><function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0><function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0><function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0><function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0><function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "                        self._shutdown_workers()self._shutdown_workers()self._shutdown_workers()self._shutdown_workers()self._shutdown_workers()self._shutdown_workers()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "                        if w.is_alive():if w.is_alive():if w.is_alive():if w.is_alive():if w.is_alive():\n",
      "if w.is_alive():\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "                                      ^  ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^\n",
      "^  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "\n",
      "\n",
      "^  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "      File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'          File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      " \n",
      "\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "    \n",
      "                                                         ^   ^^^^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^\n",
      "\n",
      "\n",
      "AssertionError\n",
      "AssertionErrorAssertionError^AssertionErrorAssertionError: : : : ^: can only test a child processcan only test a child processcan only test a child processcan only test a child process^\n",
      "can only test a child process\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0><function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()    \n",
      "self._shutdown_workers()  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "\n",
      "      File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "if w.is_alive():    \n",
      "if w.is_alive(): \n",
      "            ^ ^^^^Exception ignored in: Exception ignored in: Exception ignored in: ^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0><function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0><function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>^<function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>^\n",
      "\n",
      "\n",
      "^^\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "^^Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "^  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "^  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "^        ^    ^    self._shutdown_workers()self._shutdown_workers()self._shutdown_workers()^^self._shutdown_workers()\n",
      "\n",
      "\n",
      "^\n",
      "^  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "^  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    ^      File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "        if w.is_alive():\n",
      "if w.is_alive():if w.is_alive():    if w.is_alive():\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "\n",
      "\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "       \n",
      " assert self._parent_pid == os.getpid(), 'can only test a child process'    \n",
      "                              ^  ^^^^  ^^^^  ^^^^  ^^^ ^^ ^^^^^ ^^^^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^\n",
      "^^  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "^  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "\n",
      "^^    ^  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    ^^assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'^    ^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "^^     \n",
      " ^^ assert self._parent_pid == os.getpid(), 'can only test a child process'  ^^ \n",
      "  ^^    ^^    ^ ^   ^ ^   ^ ^   ^ ^   ^ ^   ^ ^   ^^^ ^ ^^^ ^^^^^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^AssertionError^\n",
      "^^: ^AssertionError^^^^can only test a child process: ^^^^\n",
      "can only test a child process^^^^\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^AssertionError^^^: \n",
      "^\n",
      "can only test a child processAssertionError^AssertionError\n",
      ": \n",
      ": can only test a child processAssertionErrorcan only test a child process\n",
      ": \n",
      "can only test a child process\n",
      "2025-05-18 19:22:44,383 - __main__ - INFO - 开始验证... Epoch 4, Step 588/2940\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b70d375a9e44dbaf3a3a808bde62d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 4, Step 588/2940:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 19:24:11,439 - __main__ - INFO - 验证完成. 平均验证损失: 0.0292\n",
      "2025-05-18 19:24:11,524 - __main__ - INFO -   验证准确率 (Accuracy): 0.9906\n",
      "2025-05-18 19:24:11,631 - __main__ - INFO -   验证加权 F1-Score: 0.9906\n",
      "2025-05-18 19:24:11,634 - __main__ - INFO - 🎉 新的最佳模型 (基于Accuracy)! Accuracy: 0.9906 at Epoch 4, Step 588/2940. 保存模型到 ./files/saved_models/best_roberta_model.pt\n",
      "2025-05-18 19:24:12,943 - __main__ - INFO - 早停指标改善: val_loss 从 0.0371 -> 0.0292. 重置Patience计数器。\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "2025-05-18 19:33:06,182 - __main__ - INFO - 验证完成. 平均验证损失: 0.0267\n",
      "2025-05-18 19:33:06,267 - __main__ - INFO -   验证准确率 (Accuracy): 0.9917\n",
      "2025-05-18 19:33:06,375 - __main__ - INFO -   验证加权 F1-Score: 0.9916\n",
      "2025-05-18 19:33:06,377 - __main__ - INFO - 🎉 新的最佳模型 (基于Accuracy)! Accuracy: 0.9917 at Epoch 4, Step 1176/2940. 保存模型到 ./files/saved_models/best_roberta_model.pt\n",
      "2025-05-18 19:33:07,685 - __main__ - INFO - 早停指标改善: val_loss 从 0.0292 -> 0.0267. 重置Patience计数器。\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "2025-05-18 19:41:02,534 - __main__ - INFO - 开始验证... Epoch 4, Step 1764/2940\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddc7167f294d409bb5450aadafda5305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 4, Step 1764/2940:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "2025-05-18 19:59:51,221 - __main__ - INFO - 开始验证... Epoch 4, Step 2940/2940\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd5abfeca1ee4cda948906370276b622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 4, Step 2940/2940:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 20:01:18,428 - __main__ - INFO - 验证完成. 平均验证损失: 0.0181\n",
      "2025-05-18 20:01:18,516 - __main__ - INFO -   验证准确率 (Accuracy): 0.9949\n",
      "2025-05-18 20:01:18,623 - __main__ - INFO -   验证加权 F1-Score: 0.9949\n",
      "2025-05-18 20:01:18,626 - __main__ - INFO - 🎉 新的最佳模型 (基于Accuracy)! Accuracy: 0.9949 at Epoch 4, Step 2940/2940. 保存模型到 ./files/saved_models/best_roberta_model.pt\n",
      "2025-05-18 20:01:20,257 - __main__ - INFO - 早停指标改善: val_loss 从 0.0222 -> 0.0181. 重置Patience计数器。\n",
      "2025-05-18 20:01:20,404 - __main__ - INFO - Epoch 4 训练阶段完成. 平均训练损失: 0.0415\n",
      "2025-05-18 20:01:20,406 - __main__ - INFO - Epoch 4 总耗时: 2793.02 秒\n",
      "2025-05-18 20:01:20,406 - __main__ - INFO - --- Epoch 5/6 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eaac55248ff4bb5ba3712a3e58e37b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5 Training:   0%|          | 0/2940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 20:09:47,510 - __main__ - INFO - 开始验证... Epoch 5, Step 588/2940\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "176e0a1fdb1a4ead9fcb0fbb24ec1bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 5, Step 588/2940:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 20:11:14,010 - __main__ - INFO - 验证完成. 平均验证损失: 0.0156\n",
      "2025-05-18 20:11:14,098 - __main__ - INFO -   验证准确率 (Accuracy): 0.9951\n",
      "2025-05-18 20:11:14,205 - __main__ - INFO -   验证加权 F1-Score: 0.9951\n",
      "2025-05-18 20:11:14,208 - __main__ - INFO - 🎉 新的最佳模型 (基于Accuracy)! Accuracy: 0.9951 at Epoch 5, Step 588/2940. 保存模型到 ./files/saved_models/best_roberta_model.pt\n",
      "2025-05-18 20:11:15,824 - __main__ - INFO - 早停指标改善: val_loss 从 0.0181 -> 0.0156. 重置Patience计数器。\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "2025-05-18 20:19:10,396 - __main__ - INFO - 开始验证... Epoch 5, Step 1176/2940\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbabc7f098114d3c904ddaa1813a1913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 5, Step 1176/2940:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 20:20:36,903 - __main__ - INFO - 验证完成. 平均验证损失: 0.0151\n",
      "2025-05-18 20:20:36,991 - __main__ - INFO -   验证准确率 (Accuracy): 0.9956\n",
      "2025-05-18 20:20:37,097 - __main__ - INFO -   验证加权 F1-Score: 0.9956\n",
      "2025-05-18 20:20:37,100 - __main__ - INFO - 🎉 新的最佳模型 (基于Accuracy)! Accuracy: 0.9956 at Epoch 5, Step 1176/2940. 保存模型到 ./files/saved_models/best_roberta_model.pt\n",
      "2025-05-18 20:20:38,488 - __main__ - INFO - 早停指标未改善 (val_loss: 0.0151 vs best: 0.0156). Patience: 1/10\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "2025-05-18 20:28:05,200 - __main__ - INFO - 开始验证... Epoch 5, Step 1764/2940\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ca4d0837e84c53ae1a8972f27914c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 5, Step 1764/2940:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4f5a976de0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "2025-05-18 20:29:32,656 - __main__ - INFO - 验证完成. 平均验证损失: 0.0128\n",
      "2025-05-18 20:29:32,743 - __main__ - INFO -   验证准确率 (Accuracy): 0.9963\n",
      "2025-05-18 20:29:32,863 - __main__ - INFO -   验证加权 F1-Score: 0.9963\n",
      "2025-05-18 20:29:32,866 - __main__ - INFO - 🎉 新的最佳模型 (基于Accuracy)! Accuracy: 0.9963 at Epoch 5, Step 1764/2940. 保存模型到 ./files/saved_models/best_roberta_model.pt\n",
      "2025-05-18 20:29:34,700 - __main__ - INFO - 早停指标改善: val_loss 从 0.0156 -> 0.0128. 重置Patience计数器。\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "2025-05-18 21:15:41,175 - __main__ - INFO - 开始验证... Epoch 6, Step 1764/2940\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af15182ef9f64299854cd57bbaad18a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 6, Step 1764/2940:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 21:17:08,202 - __main__ - INFO - 验证完成. 平均验证损失: 0.0078\n",
      "2025-05-18 21:17:08,287 - __main__ - INFO -   验证准确率 (Accuracy): 0.9978\n",
      "2025-05-18 21:17:08,394 - __main__ - INFO -   验证加权 F1-Score: 0.9977\n",
      "2025-05-18 21:17:08,397 - __main__ - INFO - 🎉 新的最佳模型 (基于Accuracy)! Accuracy: 0.9978 at Epoch 6, Step 1764/2940. 保存模型到 ./files/saved_models/best_roberta_model.pt\n",
      "2025-05-18 21:17:09,810 - __main__ - INFO - 早停指标改善: val_loss 从 0.0092 -> 0.0078. 重置Patience计数器。\n",
      "2025-05-18 21:24:37,235 - __main__ - INFO - 开始验证... Epoch 6, Step 2352/2940\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aef579a4bcc41cf92ecd36b69c57600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 6, Step 2352/2940:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 21:26:04,636 - __main__ - INFO - 验证完成. 平均验证损失: 0.0074\n",
      "2025-05-18 21:26:04,723 - __main__ - INFO -   验证准确率 (Accuracy): 0.9979\n",
      "2025-05-18 21:26:04,832 - __main__ - INFO -   验证加权 F1-Score: 0.9979\n",
      "2025-05-18 21:26:04,835 - __main__ - INFO - 🎉 新的最佳模型 (基于Accuracy)! Accuracy: 0.9979 at Epoch 6, Step 2352/2940. 保存模型到 ./files/saved_models/best_roberta_model.pt\n",
      "2025-05-18 21:26:36,638 - __main__ - INFO - 早停指标未改善 (val_loss: 0.0074 vs best: 0.0078). Patience: 1/10\n",
      "2025-05-18 21:34:02,566 - __main__ - INFO - 开始验证... Epoch 6, Step 2940/2940\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f21b1487033b4defa8b033ba0eccb9f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 6, Step 2940/2940:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 21:35:29,945 - __main__ - INFO - 验证完成. 平均验证损失: 0.0070\n",
      "2025-05-18 21:35:30,030 - __main__ - INFO -   验证准确率 (Accuracy): 0.9980\n",
      "2025-05-18 21:35:30,133 - __main__ - INFO -   验证加权 F1-Score: 0.9980\n",
      "2025-05-18 21:35:30,136 - __main__ - INFO - 🎉 新的最佳模型 (基于Accuracy)! Accuracy: 0.9980 at Epoch 6, Step 2940/2940. 保存模型到 ./files/saved_models/best_roberta_model.pt\n",
      "2025-05-18 21:35:31,870 - __main__ - INFO - 早停指标未改善 (val_loss: 0.0070 vs best: 0.0078). Patience: 2/10\n",
      "2025-05-18 21:35:31,990 - __main__ - INFO - Epoch 6 训练阶段完成. 平均训练损失: 0.0141\n",
      "2025-05-18 21:35:31,991 - __main__ - INFO - Epoch 6 总耗时: 2827.02 秒\n",
      "2025-05-18 21:35:31,991 - __main__ - INFO - 所有计划的 Epoch 训练与验证完成。\n",
      "2025-05-18 21:35:31,992 - __main__ - INFO - 训练期间记录的最佳模型信息: Epoch=6, Step=2940, Val Accuracy=0.9980, Val F1=0.9980, Val Loss=0.0070\n",
      "2025-05-18 21:35:31,992 - __main__ - INFO - 最佳模型参数已保存至: ./files/saved_models/best_roberta_model.pt (对应验证准确率: 0.9980)\n",
      "2025-05-18 21:35:31,992 - __main__ - INFO - 总训练耗时: 04小时 41分钟 35.61秒\n",
      "2025-05-18 21:35:31,993 - __main__ - INFO - TensorBoard 日志已保存到: ./files/logs/tensorboard_runs/20250518-165241\n",
      "2025-05-18 21:35:31,994 - __main__ - INFO - ============================== 模型训练与验证结束 ==============================\n",
      "\n",
      "2025-05-18 21:35:31,994 - __main__ - INFO - 第一次微调完成，准备进行内存清理...\n",
      "2025-05-18 21:35:31,995 - __main__ - INFO - 已删除第一次微调的优化器 (optimizer)。\n",
      "2025-05-18 21:35:31,999 - __main__ - INFO - 已删除第一次微调的学习率调度器 (lr_scheduler)。\n",
      "2025-05-18 21:35:31,999 - __main__ - INFO - 已删除原始训练数据加载器 (train_data_loader)。\n",
      "2025-05-18 21:35:32,000 - __main__ - INFO - 已删除原始训练数据集对象 (train_dataset)。\n"
     ]
    }
   ],
   "source": [
    "# --- 10.4 训练主循环 (Main Training Loop) ---\n",
    "global_start_time = time.time()\n",
    "try:\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        log.info(f\"--- Epoch {epoch}/{NUM_EPOCHS} ---\")\n",
    "        model.train()\n",
    "        total_train_loss_epoch = 0\n",
    "        train_progress_bar = tqdm(train_data_loader, desc=f\"Epoch {epoch} Training\", leave=True, position=0)\n",
    "\n",
    "        for step, batch in enumerate(train_progress_bar):\n",
    "            global_step += 1 # 更新全局步数\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # (可选) 梯度裁剪\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step() # 每个step更新学习率\n",
    "\n",
    "            total_train_loss_epoch += loss.item()\n",
    "            current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "            # 更新tqdm的描述信息\n",
    "            train_progress_bar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'avg_loss_epoch': f'{total_train_loss_epoch / (step + 1):.4f}',\n",
    "                'lr': f'{current_lr:.2e}'\n",
    "            })\n",
    "\n",
    "            # 记录训练损失到TensorBoard (可以按需调整频率，例如每N步)\n",
    "            if global_step % (max(1, len(train_data_loader) // 100)) == 0: # 大约记录100次/epoch\n",
    "                 writer.add_scalar('Loss/train_step', loss.item(), global_step)\n",
    "                 writer.add_scalar('LearningRate/step', current_lr, global_step)\n",
    "\n",
    "\n",
    "            # --- 定期验证 (Intra-Epoch Evaluation) ---\n",
    "            if EVAL_STRATEGY == \"steps\" and (step + 1) % eval_every_n_steps == 0:\n",
    "                step_info = f\"Epoch {epoch}, Step {step+1}/{len(train_data_loader)}\"\n",
    "                val_loss, val_accuracy, val_f1_weighted = evaluate_model(model, dev_data_loader, device, step_info)\n",
    "                model.train() # 确保验证后模型回到训练模式\n",
    "\n",
    "                # 记录验证指标到 TensorBoard\n",
    "                writer.add_scalar('Loss/validation_step', val_loss, global_step)\n",
    "                if val_accuracy != -1.0: writer.add_scalar('Accuracy/validation_step', val_accuracy, global_step)\n",
    "                if val_f1_weighted != -1.0: writer.add_scalar('F1_weighted/validation_step', val_f1_weighted, global_step)\n",
    "\n",
    "                # 检查是否保存模型 (基于val_accuracy)\n",
    "                if val_accuracy > best_val_metric_for_saving:\n",
    "                    best_val_metric_for_saving = val_accuracy\n",
    "                    current_best_model_info = {\"epoch\": epoch, \"step\":step+1, \"val_accuracy\": val_accuracy, \"val_f1\": val_f1_weighted, \"val_loss\": val_loss}\n",
    "                    log.info(f\"🎉 新的最佳模型 (基于Accuracy)! Accuracy: {val_accuracy:.4f} at {step_info}. 保存模型到 {BEST_MODEL_PATH}\")\n",
    "                    torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "                else:\n",
    "                    log.info(f\"当前验证 Accuracy {val_accuracy:.4f} 未超过历史最佳 {best_val_metric_for_saving:.4f} (at {step_info}).\")\n",
    "\n",
    "\n",
    "                # 早停机制检查\n",
    "                if EARLY_STOPPING_ENABLED:\n",
    "                    current_metric_for_early_stop = -1.0\n",
    "                    if EARLY_STOPPING_METRIC == 'val_accuracy': current_metric_for_early_stop = val_accuracy\n",
    "                    elif EARLY_STOPPING_METRIC == 'val_f1_weighted': current_metric_for_early_stop = val_f1_weighted\n",
    "                    elif EARLY_STOPPING_METRIC == 'val_loss': current_metric_for_early_stop = val_loss\n",
    "                    else: # 默认用val_accuracy\n",
    "                        log.warning(f\"未知的早停指标 '{EARLY_STOPPING_METRIC}', 将使用 'val_accuracy'.\")\n",
    "                        current_metric_for_early_stop = val_accuracy\n",
    "\n",
    "                    improved = False\n",
    "                    if early_stopping_mode == 'max':\n",
    "                        if current_metric_for_early_stop > best_early_stopping_metric_val + EARLY_STOPPING_MIN_DELTA:\n",
    "                            improved = True\n",
    "                    else: # min mode\n",
    "                        if current_metric_for_early_stop < best_early_stopping_metric_val - EARLY_STOPPING_MIN_DELTA:\n",
    "                            improved = True\n",
    "                    \n",
    "                    if improved:\n",
    "                        log.info(f\"早停指标改善: {EARLY_STOPPING_METRIC} 从 {best_early_stopping_metric_val:.4f} -> {current_metric_for_early_stop:.4f}. 重置Patience计数器。\")\n",
    "                        best_early_stopping_metric_val = current_metric_for_early_stop\n",
    "                        epochs_no_improve = 0\n",
    "                    else:\n",
    "                        epochs_no_improve += 1\n",
    "                        log.info(f\"早停指标未改善 ({EARLY_STOPPING_METRIC}: {current_metric_for_early_stop:.4f} vs best: {best_early_stopping_metric_val:.4f}). Patience: {epochs_no_improve}/{EARLY_STOPPING_PATIENCE}\")\n",
    "                    \n",
    "                    if epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n",
    "                        log.info(f\"🛑 早停触发! {EARLY_STOPPING_METRIC} 已连续 {epochs_no_improve} 次评估未改善，训练终止。\")\n",
    "                        training_completed_normally = False # 标记为非正常结束\n",
    "                        raise StopIteration(\"Early stopping triggered\") # 使用 StopIteration 来跳出所有循环\n",
    "\n",
    "        train_progress_bar.close() # 关闭当前epoch的训练进度条\n",
    "        avg_epoch_train_loss = total_train_loss_epoch / len(train_data_loader) if len(train_data_loader) > 0 else float('inf')\n",
    "        log.info(f\"Epoch {epoch} 训练阶段完成. 平均训练损失: {avg_epoch_train_loss:.4f}\")\n",
    "        writer.add_scalar('Loss/train_epoch', avg_epoch_train_loss, epoch) # TensorBoard记录epoch平均训练损失\n",
    "\n",
    "        # --- Epoch结束时验证 (End-of-Epoch Evaluation) ---\n",
    "        # 如果是按epoch验证，或者按steps验证但当前epoch的最后一步不是验证步，则执行\n",
    "        perform_epoch_end_eval = (EVAL_STRATEGY == \"epoch\") or \\\n",
    "                                 (EVAL_STRATEGY == \"steps\" and len(train_data_loader) % eval_every_n_steps != 0)\n",
    "\n",
    "        if perform_epoch_end_eval:\n",
    "            epoch_end_info = f\"Epoch {epoch} End\"\n",
    "            val_loss, val_accuracy, val_f1_weighted = evaluate_model(model, dev_data_loader, device, epoch_end_info)\n",
    "            # 记录验证指标到 TensorBoard\n",
    "            writer.add_scalar('Loss/validation_epoch', val_loss, epoch)\n",
    "            if val_accuracy != -1.0: writer.add_scalar('Accuracy/validation_epoch', val_accuracy, epoch)\n",
    "            if val_f1_weighted != -1.0: writer.add_scalar('F1_weighted/validation_epoch', val_f1_weighted, epoch)\n",
    "\n",
    "            # 检查是否保存模型 (基于val_accuracy)\n",
    "            if val_accuracy > best_val_metric_for_saving:\n",
    "                best_val_metric_for_saving = val_accuracy\n",
    "                current_best_model_info = {\"epoch\": epoch, \"step\":\"end_of_epoch\", \"val_accuracy\": val_accuracy, \"val_f1\": val_f1_weighted, \"val_loss\": val_loss}\n",
    "                log.info(f\"🎉 新的最佳模型 (基于Accuracy)! Accuracy: {val_accuracy:.4f} at {epoch_end_info}. 保存模型到 {BEST_MODEL_PATH}\")\n",
    "                torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "            else:\n",
    "                log.info(f\"当前验证 Accuracy {val_accuracy:.4f} 未超过历史最佳 {best_val_metric_for_saving:.4f} (at {epoch_end_info}).\")\n",
    "\n",
    "            # 早停机制检查 (如果按epoch验证)\n",
    "            if EARLY_STOPPING_ENABLED and EVAL_STRATEGY == \"epoch\": # 只在按epoch验证时，在epoch结束检查早停\n",
    "                current_metric_for_early_stop = -1.0\n",
    "                if EARLY_STOPPING_METRIC == 'val_accuracy': current_metric_for_early_stop = val_accuracy\n",
    "                elif EARLY_STOPPING_METRIC == 'val_f1_weighted': current_metric_for_early_stop = val_f1_weighted\n",
    "                elif EARLY_STOPPING_METRIC == 'val_loss': current_metric_for_early_stop = val_loss\n",
    "                else:\n",
    "                    log.warning(f\"未知的早停指标 '{EARLY_STOPPING_METRIC}', 将使用 'val_accuracy'.\")\n",
    "                    current_metric_for_early_stop = val_accuracy\n",
    "\n",
    "                improved = False\n",
    "                if early_stopping_mode == 'max':\n",
    "                    if current_metric_for_early_stop > best_early_stopping_metric_val + EARLY_STOPPING_MIN_DELTA:\n",
    "                        improved = True\n",
    "                else: # min mode\n",
    "                    if current_metric_for_early_stop < best_early_stopping_metric_val - EARLY_STOPPING_MIN_DELTA:\n",
    "                        improved = True\n",
    "\n",
    "                if improved:\n",
    "                    log.info(f\"早停指标改善: {EARLY_STOPPING_METRIC} 从 {best_early_stopping_metric_val:.4f} -> {current_metric_for_early_stop:.4f}. 重置Patience计数器。\")\n",
    "                    best_early_stopping_metric_val = current_metric_for_early_stop\n",
    "                    epochs_no_improve = 0\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "                    log.info(f\"早停指标未改善 ({EARLY_STOPPING_METRIC}: {current_metric_for_early_stop:.4f} vs best: {best_early_stopping_metric_val:.4f}). Patience: {epochs_no_improve}/{EARLY_STOPPING_PATIENCE}\")\n",
    "\n",
    "                if epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n",
    "                    log.info(f\"🛑 早停触发! {EARLY_STOPPING_METRIC} 已连续 {epochs_no_improve} 次评估未改善，训练终止。\")\n",
    "                    training_completed_normally = False\n",
    "                    raise StopIteration(\"Early stopping triggered\")\n",
    "\n",
    "        epoch_duration = time.time() - epoch_start_time\n",
    "        log.info(f\"Epoch {epoch} 总耗时: {epoch_duration:.2f} 秒\")\n",
    "        writer.add_scalar('Time/epoch_duration_seconds', epoch_duration, epoch)\n",
    "\n",
    "    training_completed_normally = True # 如果循环正常结束，标记为true\n",
    "\n",
    "except StopIteration as e_stop: # 捕获早停信号\n",
    "    if str(e_stop) == \"Early stopping triggered\":\n",
    "        log.info(\"训练因早停机制提前结束。\")\n",
    "    else:\n",
    "        log.error(f\"训练意外因 StopIteration 终止: {e_stop}\", exc_info=True) # 其他 StopIteration\n",
    "except KeyboardInterrupt:\n",
    "    log.warning(\"训练被用户手动中断 (KeyboardInterrupt)。\")\n",
    "    training_completed_normally = False\n",
    "except Exception as e_train:\n",
    "    log.critical(f\"训练过程中发生严重错误: {e_train}\", exc_info=True)\n",
    "    training_completed_normally = False\n",
    "finally:\n",
    "    # --- 10.5 训练结束总结 (Training Completion Summary) ---\n",
    "    if training_completed_normally:\n",
    "        log.info(\"所有计划的 Epoch 训练与验证完成。\")\n",
    "    else:\n",
    "        log.info(\"训练未完成所有计划的 Epoch。\")\n",
    "\n",
    "    log.info(f\"训练期间记录的最佳模型信息: Epoch={current_best_model_info['epoch']}, Step={current_best_model_info['step']}, \"\n",
    "             f\"Val Accuracy={current_best_model_info['val_accuracy']:.4f}, \"\n",
    "             f\"Val F1={current_best_model_info['val_f1']:.4f}, \"\n",
    "             f\"Val Loss={current_best_model_info['val_loss']:.4f}\")\n",
    "    if os.path.exists(BEST_MODEL_PATH) and best_val_metric_for_saving > -float('inf'):\n",
    "        log.info(f\"最佳模型参数已保存至: {BEST_MODEL_PATH} (对应验证准确率: {best_val_metric_for_saving:.4f})\")\n",
    "    else:\n",
    "        log.warning(f\"未成功保存任何最佳模型到 {BEST_MODEL_PATH} (可能是因为验证指标从未改善或未进行有效评估)。\")\n",
    "\n",
    "    total_training_time = time.time() - global_start_time\n",
    "    hours, rem = divmod(total_training_time, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    log.info(f\"总训练耗时: {int(hours):02d}小时 {int(minutes):02d}分钟 {seconds:.2f}秒\")\n",
    "    writer.close() # 关闭 TensorBoard writer\n",
    "    log.info(f\"TensorBoard 日志已保存到: {TENSORBOARD_LOG_DIR}\")\n",
    "\n",
    "log.info(\"=\"*30 + \" 模型训练与验证结束 \" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "# ... 第一次微调训练循环结束 ...\n",
    "\n",
    "log.info(\"第一次微调完成，准备进行内存清理...\")\n",
    "\n",
    "if 'optimizer' in locals():\n",
    "    del optimizer\n",
    "    log.info(\"已删除第一次微调的优化器 (optimizer)。\")\n",
    "if 'lr_scheduler' in locals():\n",
    "    del lr_scheduler\n",
    "    log.info(\"已删除第一次微调的学习率调度器 (lr_scheduler)。\")\n",
    "\n",
    "# 2. 删除不再需要的原始训练数据和加载器\n",
    "if 'train_data_loader' in locals(): \n",
    "    del train_data_loader\n",
    "    log.info(\"已删除原始训练数据加载器 (train_data_loader)。\")\n",
    "if 'train_dataset' in locals(): \n",
    "    del train_dataset\n",
    "    log.info(\"已删除原始训练数据集对象 (train_dataset)。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c887b26-cacf-45d7-b01a-c4a1b1bbf0dc",
   "metadata": {},
   "source": [
    "### 清缓存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "183b26c9-d2e7-45a9-bd57-93d9b5d8aa38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 21:35:32,008 - __main__ - INFO - 当前CUDA显存已分配: 2502.73 MB\n",
      "2025-05-18 21:35:32,009 - __main__ - INFO - 当前CUDA显存已缓存: 16290.00 MB\n",
      "2025-05-18 21:35:32,258 - __main__ - INFO - 已尝试清空CUDA缓存。\n",
      "2025-05-18 21:35:32,259 - __main__ - INFO - 清空缓存后CUDA显存已分配: 2502.73 MB\n",
      "2025-05-18 21:35:32,259 - __main__ - INFO - 清空缓存后CUDA显存已缓存: 3322.00 MB\n"
     ]
    }
   ],
   "source": [
    "# 3. 清空CUDA缓存\n",
    "if torch.cuda.is_available():\n",
    "    log.info(f\"当前CUDA显存已分配: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "    log.info(f\"当前CUDA显存已缓存: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "    torch.cuda.empty_cache()\n",
    "    log.info(\"已尝试清空CUDA缓存。\")\n",
    "    log.info(f\"清空缓存后CUDA显存已分配: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "    log.info(f\"清空缓存后CUDA显存已缓存: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "\n",
    "# 确保 dev_data_loader, model, tokenizer, id_to_label, num_classes, device, writer 等二次微调仍然需要的对象没有被错误删除"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ec2a0e-e0f7-4800-92a5-b170ee8a8984",
   "metadata": {},
   "source": [
    "## 测试集预测结果分析与数据增强样本生成(Test Set Prediction Analysis & Data Augmentation Sample Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55d90532-29fe-485c-b77c-39820ca34fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 21:36:02,317 - __main__ - INFO - ============================== 开始测试集预测分析与数据增强样本生成 ==============================\n",
      "2025-05-18 21:36:02,318 - __main__ - INFO - 检查分析模块所需的前置变量...\n",
      "2025-05-18 21:36:02,319 - __main__ - INFO - 所有分析模块所需的前置变量均已定义\n",
      "2025-05-18 21:36:02,320 - __main__ - INFO - 加载最佳模型权重从: ./files/saved_models/best_roberta_model.pt\n",
      "/tmp/ipykernel_16737/1235877767.py:52: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=device))\n",
      "2025-05-18 21:36:03,128 - __main__ - INFO - 最佳模型权重加载成功\n",
      "2025-05-18 21:36:03,131 - __main__ - INFO - 开始对所有测试数据进行 Top-1 预测...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbdd91214b1946b29fc0dda3099c0829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Full Test Prediction (Top-1):   0%|          | 0/327 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 21:37:34,207 - __main__ - INFO - Top-1 预测处理完成，共获得 83599 条预测结果\n",
      "2025-05-18 21:37:34,635 - __main__ - INFO - 全量测试集 Top-1 预测结果已保存至 JSON: ./files/analysis_results/all_test_analysis/test_top1_predictions.json\n",
      "2025-05-18 21:37:34,635 - __main__ - INFO - JSON结果示例 (第一条):\n",
      "2025-05-18 21:37:34,636 - __main__ - INFO - {\n",
      "  \"original_id\": 0,\n",
      "  \"text\": \"北京君太百货璀璨秋色 满100省353020元\",\n",
      "  \"predicted_label\": \"房产\",\n",
      "  \"probability\": 0.999\n",
      "}\n",
      "2025-05-18 21:37:34,636 - __main__ - INFO - 开始统计和绘制 Top-1 预测概率分布图...\n",
      "2025-05-18 21:38:05,847 - __main__ - INFO - 概率分布柱状图已保存至: ./files/analysis_results/all_test_analysis/test_prob_distribution.png\n",
      "2025-05-18 21:38:05,969 - __main__ - INFO - 概率分布图已尝试记录到 TensorBoard\n",
      "2025-05-18 21:38:05,981 - __main__ - INFO - 概率分布数据已保存至 CSV: ./files/analysis_results/all_test_analysis/test_prob_distribution_data.csv\n",
      "2025-05-18 21:38:05,982 - __main__ - INFO - 开始根据 Top-1 预测概率生成数据增强文件 (选取前 80% 高置信度样本)...\n",
      "2025-05-18 21:38:05,996 - __main__ - INFO - 将选取 66879 条样本用于数据增强\n",
      "2025-05-18 21:38:05,997 - __main__ - INFO -   这些样本的 Top-1 预测概率范围从 1.000 到 0.999\n",
      "2025-05-18 21:38:06,047 - __main__ - INFO - 成功将 66879 条预测写入数据增强文件: ./files/raw_data/train_add.txt\n",
      "2025-05-18 21:38:06,047 - __main__ - INFO - 数据增强文件内容示例 (前3行):\n",
      "2025-05-18 21:38:06,048 - __main__ - INFO -   专业级单反相机 佳能7D单机售价9280元\t科技\n",
      "2025-05-18 21:38:06,048 - __main__ - INFO -   星展银行起诉内地客户 银行强硬客户无奈\t股票\n",
      "2025-05-18 21:38:06,049 - __main__ - INFO -   内城土地稀缺 对开发商提出更高要求(组图)\t房产\n",
      "2025-05-18 21:38:06,049 - __main__ - INFO - ============================== 测试集预测分析与数据增强样本生成结束 ==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 主要功能:\n",
    "# 1. 检查前置条件，加载最佳模型权重\n",
    "# 2. 对整个测试集执行预测，获取每个样本的 top-1 预测标签及其概率\n",
    "# 3. 将 top-1 预测结果 (原始ID, 文本, 预测标签, 概率) 保存为 JSON 文件\n",
    "# 4. 统计预测概率分布:\n",
    "#    a. 以指定的概率分度值 (PROB_DIST_PLOT_STEP) 从 1 到最低概率构建 x 轴\n",
    "#    b. 计算在每个概率阈值下，有多少比例的样本其 top-1 预测概率高于该阈值 (y 轴)\n",
    "#    c. 绘制并保存概率分布柱状图\n",
    "#    d. 将概率分布的具体数值 (概率阈值, 样本占比) 保存为 CSV 文件\n",
    "# 5. 数据增强:\n",
    "#    a. 选取 top-1 预测概率排在前 TOP_PERCENT_FOR_AUGMENTATION (例如80%) 的样本\n",
    "#    b. 将这些高置信度样本以 \"标题\\t预测的标签\" 格式保存到指定路径下的\n",
    "#       TRAIN_ADD_FILE_NAME 文件中，用于潜在的数据增强\n",
    "#    c. 记录用于数据增强的样本中，最低的 top-1 预测概率值\n",
    "# ==============================================================================\n",
    "log.info(\"=\"*30 + \" 开始测试集预测分析与数据增强样本生成 \" + \"=\"*30)\n",
    "\n",
    "# --- 11.1 检查前置条件与配置加载 (Check Prerequisites & Load Configs) ---\n",
    "required_vars_for_analysis = [\n",
    "    'model', 'id_to_label', 'log', 'device', 'writer', \n",
    "    'test_data_loader', 'raw_test_data', \n",
    "    'BEST_MODEL_PATH', \n",
    "    'RAW_DATA_DIR',\n",
    "    'TRAIN_ADD_FILE_PATH', \n",
    "    'PROB_DIST_PLOT_STEP', \n",
    "    'TOP1_JSON_PATH',\n",
    "    'TOP_PERCENT_FOR_AUGMENTATION' \n",
    "]\n",
    "log.info(\"检查分析模块所需的前置变量...\")\n",
    "for var_name in required_vars_for_analysis:\n",
    "    if var_name not in locals() and var_name not in globals():\n",
    "        log.critical(f\"分析模块执行失败：关键前置变量 '{var_name}' 未定义，请检查之前的单元格\")\n",
    "        exit(1)\n",
    "\n",
    "if test_data_loader is None or len(test_data_loader) == 0:\n",
    "    log.critical(\"测试数据加载器 (test_data_loader) 为空或未定义，无法进行预测分析\")\n",
    "    exit(1)\n",
    "if not raw_test_data:\n",
    "    log.critical(\"原始测试数据 (raw_test_data) 为空，无法获取文本内容\")\n",
    "    exit(1)\n",
    "\n",
    "log.info(\"所有分析模块所需的前置变量均已定义\")\n",
    "\n",
    "\n",
    "# --- 11.2 加载最佳模型权重 (Load Best Model Weights) ---\n",
    "log.info(f\"加载最佳模型权重从: {BEST_MODEL_PATH}\")\n",
    "if not os.path.exists(BEST_MODEL_PATH):\n",
    "    log.critical(f\"模型权重文件不存在: {BEST_MODEL_PATH}，无法进行预测\")\n",
    "    exit(1)\n",
    "try:\n",
    "    model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=device))\n",
    "    model.to(device)\n",
    "    log.info(\"最佳模型权重加载成功\")\n",
    "except Exception as e:\n",
    "    log.critical(f\"加载模型权重失败: {e}\", exc_info=True)\n",
    "    exit(1)\n",
    "\n",
    "# --- 11.3 执行全量预测并收集 Top-1 结果 (Perform Full Prediction & Collect Top-1 Results) ---\n",
    "model.eval() # 设置模型为评估模式\n",
    "all_top1_results = [] # 存储每个样本的 top-1 预测信息\n",
    "\n",
    "log.info(\"开始对所有测试数据进行 Top-1 预测...\")\n",
    "predict_progress_bar = tqdm(test_data_loader, desc=\"Full Test Prediction (Top-1)\", leave=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(predict_progress_bar):\n",
    "        if 'id' not in batch:\n",
    "            log.warning(f\"批次 {batch_idx} 中缺少 'id' 键，跳过此批次中的部分数据关联\")\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=-1) # (batch_size, num_classes)\n",
    "\n",
    "        # 获取 Top-1 预测的概率和索引\n",
    "        top1_probs, top1_indices = torch.max(probabilities, dim=-1) # (batch_size), (batch_size)\n",
    "\n",
    "        batch_original_ids = batch.get('id', torch.arange(input_ids.size(0)) + batch_idx * test_data_loader.batch_size).cpu().tolist()\n",
    "\n",
    "\n",
    "        for i in range(top1_probs.size(0)):\n",
    "            original_id = batch_original_ids[i]\n",
    "            prob_val = top1_probs[i].item()\n",
    "            label_idx = top1_indices[i].item()\n",
    "            predicted_label_text = id_to_label.get(label_idx, \"未知标签\") # 使用 .get() 避免KeyError\n",
    "\n",
    "            try:\n",
    "                # 确保 original_id 是有效的 raw_test_data 索引\n",
    "                if 0 <= original_id < len(raw_test_data):\n",
    "                     original_text = raw_test_data[original_id]['text']\n",
    "                else:\n",
    "                    log.warning(f\"original_id {original_id} 超出 raw_test_data 范围 ({len(raw_test_data)}), 无法获取原始文本\")\n",
    "                    original_text = \"原始文本不可用\"\n",
    "            except (IndexError, TypeError) as e_text: # 如果raw_test_data结构不对或id有问题\n",
    "                log.warning(f\"通过 original_id {original_id} 获取原始文本失败: {e_text}\")\n",
    "                original_text = \"获取原始文本失败\"\n",
    "\n",
    "\n",
    "            all_top1_results.append({\n",
    "                \"original_id\": original_id,\n",
    "                \"text\": original_text,\n",
    "                \"predicted_label\": predicted_label_text,\n",
    "                \"probability\": round(prob_val, 3) # 保留三位小数\n",
    "            })\n",
    "predict_progress_bar.close()\n",
    "log.info(f\"Top-1 预测处理完成，共获得 {len(all_top1_results)} 条预测结果\")\n",
    "\n",
    "if not all_top1_results:\n",
    "    log.critical(\"未能生成任何 Top-1 预测结果，后续分析无法进行\")\n",
    "    exit(1)\n",
    "\n",
    "# --- 11.4 保存 Top-1 预测 JSON 结果 (Save Top-1 Prediction JSON Results) ---\n",
    "try:\n",
    "    with open(TOP1_JSON_PATH, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_top1_results, f, ensure_ascii=False, indent=2)\n",
    "    log.info(f\"全量测试集 Top-1 预测结果已保存至 JSON: {TOP1_JSON_PATH}\")\n",
    "    if all_top1_results:\n",
    "        log.info(\"JSON结果示例 (第一条):\")\n",
    "        log.info(json.dumps(all_top1_results[0], ensure_ascii=False, indent=2))\n",
    "except Exception as e:\n",
    "    log.error(f\"保存 Top-1 预测 JSON 结果时发生错误: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "# --- 11.5 统计并绘制概率分布图 (Analyze and Plot Probability Distribution) ---\n",
    "log.info(\"开始统计和绘制 Top-1 预测概率分布图...\")\n",
    "top1_probabilities = np.array([res['probability'] for res in all_top1_results])\n",
    "\n",
    "if len(top1_probabilities) == 0:\n",
    "    log.warning(\"没有 Top-1 概率数据可供分析，跳过概率分布图绘制\")\n",
    "else:\n",
    "    min_prob = np.min(top1_probabilities) if len(top1_probabilities) > 0 else 0.0\n",
    "    # x轴：从1.0到最低概率，以PROB_DIST_PLOT_STEP为步长\n",
    "    # 确保包含min_prob和1.0，并且步长合理\n",
    "    prob_thresholds = np.arange(1.0, min_prob - PROB_DIST_PLOT_STEP, -PROB_DIST_PLOT_STEP)\n",
    "    prob_thresholds = np.clip(prob_thresholds, 0.0, 1.0) # 确保在[0,1]内\n",
    "    prob_thresholds = np.unique(prob_thresholds)[::-1] # 去重并保持降序，确保1.0在最前\n",
    "    if prob_thresholds[0] < 1.0: # 确保1.0作为第一个阈值\n",
    "        prob_thresholds = np.insert(prob_thresholds, 0, 1.0)\n",
    "\n",
    "\n",
    "    sample_proportions = [] # y轴：高于该概率阈值的样本占比\n",
    "    for threshold in prob_thresholds:\n",
    "        count_above_threshold = np.sum(top1_probabilities >= threshold)\n",
    "        proportion = count_above_threshold / len(top1_probabilities) if len(top1_probabilities) > 0 else 0\n",
    "        sample_proportions.append(proportion)\n",
    "\n",
    "    # 绘制柱状图\n",
    "    try:\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        # 为了柱状图美观，x轴标签可能需要调整，如果阈值太多\n",
    "        bar_positions = np.arange(len(prob_thresholds))\n",
    "        plt.bar(bar_positions, sample_proportions, width=0.8, color='skyblue')\n",
    "\n",
    "        plt.xlabel(\"Top-1 Prediction Probability Threshold (P)\")\n",
    "        plt.ylabel(f\"Proportion of Samples with Top-1 Prob >= P (Total Samples: {len(top1_probabilities)})\")\n",
    "        plt.title(\"Distribution of Top-1 Prediction Probabilities on Test Set\")\n",
    "        \n",
    "        # 调整x轴刻度标签，避免重叠\n",
    "        tick_indices = np.linspace(0, len(prob_thresholds) - 1, num=min(15, len(prob_thresholds)), dtype=int) # 最多显示15个刻度\n",
    "        plt.xticks(bar_positions[tick_indices], [f\"{prob_thresholds[i]:.3f}\" for i in tick_indices], rotation=45, ha=\"right\")\n",
    "        \n",
    "        plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.grid(axis='y', linestyle='--')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(PROB_DIST_PLOT_PATH)\n",
    "        log.info(f\"概率分布柱状图已保存至: {PROB_DIST_PLOT_PATH}\")\n",
    "        plt.close() # 关闭图像，释放内存\n",
    "\n",
    "        # 将图像记录到TensorBoard\n",
    "        try:\n",
    "            image = Image.open(PROB_DIST_PLOT_PATH)\n",
    "            image_tensor = torch.tensor(np.array(image)).permute(2,0,1) # HWC to CHW\n",
    "            writer.add_image('Analysis/ProbabilityDistribution', image_tensor, global_step=0) # global_step可以设为0或某个特定值\n",
    "            log.info(\"概率分布图已尝试记录到 TensorBoard\")\n",
    "        except Exception as e_tb_img:\n",
    "            log.warning(f\"记录概率分布图到 TensorBoard 失败: {e_tb_img}\")\n",
    "\n",
    "\n",
    "    except Exception as e_plot:\n",
    "        log.error(f\"绘制或保存概率分布图时发生错误: {e_plot}\", exc_info=True)\n",
    "\n",
    "    # 保存概率分布数据到CSV\n",
    "    try:\n",
    "        with open(PROB_DIST_CSV_PATH, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            csv_writer = csv.writer(csvfile)\n",
    "            csv_writer.writerow(['Probability_Threshold', 'Proportion_Samples_Above_Threshold'])\n",
    "            for threshold, proportion in zip(prob_thresholds, sample_proportions):\n",
    "                csv_writer.writerow([f\"{threshold:.3f}\", f\"{proportion:.4f}\"])\n",
    "        log.info(f\"概率分布数据已保存至 CSV: {PROB_DIST_CSV_PATH}\")\n",
    "    except Exception as e_csv:\n",
    "        log.error(f\"保存概率分布数据到 CSV 时发生错误: {e_csv}\", exc_info=True)\n",
    "\n",
    "\n",
    "# --- 11.6 生成数据增强文件 (Generate Data Augmentation File) ---\n",
    "log.info(f\"开始根据 Top-1 预测概率生成数据增强文件 (选取前 {TOP_PERCENT_FOR_AUGMENTATION*100:.0f}% 高置信度样本)...\")\n",
    "\n",
    "# 按概率降序排序所有 Top-1 结果\n",
    "sorted_top1_results = sorted(all_top1_results, key=lambda x: x['probability'], reverse=True)\n",
    "\n",
    "num_to_select_for_aug = int(len(sorted_top1_results) * TOP_PERCENT_FOR_AUGMENTATION)\n",
    "if num_to_select_for_aug == 0 and len(sorted_top1_results) > 0: # 确保至少选一个，如果比例太小但有数据\n",
    "    log.warning(f\"根据比例 {TOP_PERCENT_FOR_AUGMENTATION} 计算得到的数据增强样本数为0，将至少选择一个样本（如果存在）\")\n",
    "    # num_to_select_for_aug = 1 # 或者保持为0，不生成文件\n",
    "\n",
    "augmented_samples = sorted_top1_results[:num_to_select_for_aug]\n",
    "\n",
    "if augmented_samples:\n",
    "    min_prob_for_augmentation = augmented_samples[-1]['probability'] # 获取选中样本中的最低概率\n",
    "    log.info(f\"将选取 {len(augmented_samples)} 条样本用于数据增强\")\n",
    "    log.info(f\"  这些样本的 Top-1 预测概率范围从 {augmented_samples[0]['probability']:.3f} 到 {min_prob_for_augmentation:.3f}\")\n",
    "\n",
    "    try:\n",
    "        count_written = 0\n",
    "        with open(TRAIN_ADD_FILE_PATH, 'w', encoding='utf-8') as f_aug:\n",
    "            for sample in augmented_samples:\n",
    "                title = sample['text'].replace('\\t', ' ').replace('\\n', ' ') # 替换制表符和换行符\n",
    "                predicted_label = sample['predicted_label']\n",
    "                f_aug.write(f\"{title}\\t{predicted_label}\\n\")\n",
    "                count_written +=1\n",
    "        log.info(f\"成功将 {count_written} 条预测写入数据增强文件: {TRAIN_ADD_FILE_PATH}\")\n",
    "        if count_written > 0:\n",
    "            log.info(\"数据增强文件内容示例 (前3行):\")\n",
    "            with open(TRAIN_ADD_FILE_PATH, 'r', encoding='utf-8') as f_preview:\n",
    "                for i, line in enumerate(f_preview):\n",
    "                    if i < 3:\n",
    "                        log.info(f\"  {line.strip()}\")\n",
    "                    else:\n",
    "                        break\n",
    "    except Exception as e_aug:\n",
    "        log.error(f\"写入数据增强文件 {TRAIN_ADD_FILE_PATH} 时发生错误: {e_aug}\", exc_info=True)\n",
    "else:\n",
    "    log.info(\"没有足够的样本满足数据增强条件，或未生成任何预测结果，未创建数据增强文件\")\n",
    "log.info(\"=\"*30 + \" 测试集预测分析与数据增强样本生成结束 \" + \"=\"*30 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43804e5-ec16-49e9-b21e-3103cccedf58",
   "metadata": {},
   "source": [
    "## 增强训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a492e9b8-e546-4908-9694-02451ad55704",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 22:00:51,050 - __main__ - INFO - ============================== 开始使用增强数据进行二次微调 ==============================\n",
      "2025-05-18 22:00:51,052 - __main__ - INFO - 检查二次微调所需的前置变量...\n",
      "2025-05-18 22:00:51,052 - __main__ - INFO - 所有二次微调所需的前置变量均已定义。\n",
      "2025-05-18 22:00:51,053 - __main__ - INFO - 开始加载数据增强文件: ./files/raw_data/train_add.txt\n",
      "2025-05-18 22:00:51,054 - __main__ - INFO - 开始从文件加载数据: ./files/raw_data/train_add.txt\n",
      "2025-05-18 22:00:51,112 - __main__ - INFO - 从 ./files/raw_data/train_add.txt 成功加载 66879 条数据\n",
      "2025-05-18 22:00:51,142 - __main__ - INFO - 成功从 ./files/raw_data/train_add.txt 加载 66879 条增强训练数据。\n",
      "2025-05-18 22:00:51,143 - __main__ - INFO - 开始预处理增强训练数据...\n",
      "2025-05-18 22:00:51,143 - __main__ - INFO - 发现预处理好的增强训练数据缓存，正在从磁盘加载: ./files/processed_data_cache/aug_train_dataset_cache\n",
      "2025-05-18 22:00:51,149 - __main__ - INFO - 从磁盘加载预处理增强训练数据成功。\n",
      "2025-05-18 22:00:51,150 - __main__ - INFO - 增强训练数据集格式已设置为 PyTorch Tensors。\n",
      "2025-05-18 22:00:51,150 - __main__ - INFO - 创建增强训练数据的 DataLoader...\n",
      "2025-05-18 22:00:51,151 - __main__ - INFO - 增强训练数据 DataLoader 创建成功 (Batch Size: 256, Num Workers: 0)。\n",
      "2025-05-18 22:00:51,151 - __main__ - INFO - 加载第一阶段微调后的最佳模型权重从: ./files/saved_models/best_roberta_model.pt\n",
      "/tmp/ipykernel_16737/2372349392.py:119: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(BEST_AUG_MODEL_PATH, map_location=device))\n",
      "2025-05-18 22:00:52,177 - __main__ - INFO - 成功加载来自 './files/saved_models/best_roberta_model.pt' 的模型权重。\n",
      "2025-05-18 22:00:52,178 - __main__ - INFO - 为二次微调配置新的优化器和学习率调度器...\n",
      "2025-05-18 22:00:52,178 - __main__ - INFO - 二次微调超参数: Learning Rate=2e-05, Epochs=3, Warmup Proportion=0.1, Weight Decay=0.01\n",
      "2025-05-18 22:00:52,180 - __main__ - INFO - 二次微调 AdamW 优化器创建成功。\n",
      "2025-05-18 22:00:52,181 - __main__ - INFO - 二次微调总训练步数: 786, 预热步数: 78\n",
      "2025-05-18 22:00:52,184 - __main__ - INFO - 二次微调学习率调度器创建成功。\n",
      "2025-05-18 22:00:52,185 - __main__ - INFO - 开始二次微调训练与验证循环...\n",
      "2025-05-18 22:00:52,185 - __main__ - INFO - 二次微调按步数验证：每 262 步 (约占epoch的 100.0%) 进行一次验证。\n",
      "2025-05-18 22:00:52,187 - __main__ - INFO - --- 二次微调 Epoch 1/3 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb7e5beb89654ad18c4990022ca2641c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aug Epoch 1 Training:   0%|          | 0/262 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 22:04:41,166 - __main__ - INFO - 开始验证... Aug Epoch 1, Step 262/262\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1c0b56638db43818cd3ac100bfa6847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Aug Epoch 1, Step 262/262:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 22:06:07,358 - __main__ - INFO - 验证完成. 平均验证损失: 0.0322\n",
      "2025-05-18 22:06:07,448 - __main__ - INFO -   验证准确率 (Accuracy): 0.9897\n",
      "2025-05-18 22:06:07,554 - __main__ - INFO -   验证加权 F1-Score: 0.9897\n",
      "2025-05-18 22:06:07,557 - __main__ - INFO - 🎉 新的最佳增强模型 (基于Accuracy)! Accuracy: 0.9897 at Aug Epoch 1, Step 262/262. 保存模型到 ./files/saved_models/best_augmented_model.pt\n",
      "2025-05-18 22:06:09,297 - __main__ - INFO - 二次微调 Epoch 1 训练阶段完成. 平均训练损失: 0.0066\n",
      "2025-05-18 22:06:09,298 - __main__ - INFO - 二次微调 Epoch 1 总耗时: 317.11 秒\n",
      "2025-05-18 22:06:09,334 - __main__ - INFO - --- 二次微调 Epoch 2/3 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dcb9637fb6e475ab74c1308e2e8261b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aug Epoch 2 Training:   0%|          | 0/262 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 22:09:59,621 - __main__ - INFO - 开始验证... Aug Epoch 2, Step 262/262\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "798722e9025f4268b067d00c1fcdc24f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Aug Epoch 2, Step 262/262:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 22:11:26,277 - __main__ - INFO - 验证完成. 平均验证损失: 0.0344\n",
      "2025-05-18 22:11:26,368 - __main__ - INFO -   验证准确率 (Accuracy): 0.9896\n",
      "2025-05-18 22:11:26,478 - __main__ - INFO -   验证加权 F1-Score: 0.9896\n",
      "2025-05-18 22:11:26,481 - __main__ - INFO - 当前验证 Accuracy 0.9896 未超过二次微调历史最佳 0.9897 (at Aug Epoch 2, Step 262/262).\n",
      "2025-05-18 22:11:26,483 - __main__ - INFO - 二次微调 Epoch 2 训练阶段完成. 平均训练损失: 0.0030\n",
      "2025-05-18 22:11:26,484 - __main__ - INFO - 二次微调 Epoch 2 总耗时: 317.15 秒\n",
      "2025-05-18 22:11:26,484 - __main__ - INFO - --- 二次微调 Epoch 3/3 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c6f53ed8814123b8b465f6268c8bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aug Epoch 3 Training:   0%|          | 0/262 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 22:15:16,680 - __main__ - INFO - 开始验证... Aug Epoch 3, Step 262/262\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b5111cb2c19421a9e07afabef5c2a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Aug Epoch 3, Step 262/262:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 22:16:43,389 - __main__ - INFO - 验证完成. 平均验证损失: 0.0322\n",
      "2025-05-18 22:16:43,482 - __main__ - INFO -   验证准确率 (Accuracy): 0.9903\n",
      "2025-05-18 22:16:43,591 - __main__ - INFO -   验证加权 F1-Score: 0.9904\n",
      "2025-05-18 22:16:43,594 - __main__ - INFO - 🎉 新的最佳增强模型 (基于Accuracy)! Accuracy: 0.9903 at Aug Epoch 3, Step 262/262. 保存模型到 ./files/saved_models/best_augmented_model.pt\n",
      "2025-05-18 22:16:44,901 - __main__ - INFO - 二次微调 Epoch 3 训练阶段完成. 平均训练损失: 0.0010\n",
      "2025-05-18 22:16:44,902 - __main__ - INFO - 二次微调 Epoch 3 总耗时: 318.42 秒\n",
      "2025-05-18 22:16:44,902 - __main__ - INFO - 所有计划的二次微调 Epoch 训练与验证完成。\n",
      "2025-05-18 22:16:44,903 - __main__ - INFO - 二次微调期间记录的最佳模型信息: Epoch=3, Step=262, Val Accuracy=0.9903, Val F1=0.9904, Val Loss=0.0322\n",
      "2025-05-18 22:16:44,903 - __main__ - INFO - 最佳二次微调模型参数已保存至: ./files/saved_models/best_augmented_model.pt (对应验证准确率: 0.9903)\n",
      "2025-05-18 22:16:44,904 - __main__ - INFO - 总二次微调耗时: 00小时 15分钟 52.72秒\n",
      "2025-05-18 22:16:44,904 - __main__ - INFO - ============================== 使用增强数据进行二次微调结束 ==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 单元格 12: 使用增强数据进行二次微调 (Second Stage Fine-tuning with Augmented Data)\n",
    "# 承接之前生成的数据增强文件 (TRAIN_ADD_FILE_PATH)，\n",
    "# 以及第一阶段微调得到的最佳模型 (BEST_MODEL_PATH)。\n",
    "#\n",
    "# 主要功能:\n",
    "# 1. 检查前置条件和配置文件。\n",
    "# 2. 加载由高置信度预测生成的增强训练数据。\n",
    "# 3. 对增强数据进行预处理 (Tokenization, Dataset creation)，可选择使用新的缓存路径。\n",
    "# 4. 创建用于增强数据训练的 DataLoader。\n",
    "# 5. 加载第一阶段微调后的最佳模型权重。\n",
    "# 6. (可选) 为二次微调配置新的（通常更小的）学习率、优化器和学习率调度器。\n",
    "# 7. 执行训练和验证循环：\n",
    "#    - 使用增强数据进行训练。\n",
    "#    - 在原始验证集 (dev_data_loader) 上进行评估。\n",
    "#    - 无早停机制。\n",
    "#    - 记录训练过程到 TensorBoard。\n",
    "# 8. (可选) 保存二次微调后的最佳模型。\n",
    "# ==============================================================================\n",
    "log.info(\"=\"*30 + \" 开始使用增强数据进行二次微调 \" + \"=\"*30)\n",
    "\n",
    "# --- 12.1 检查前置条件与配置加载 (Check Prerequisites & Load Configs) ---\n",
    "required_vars_for_aug_finetuning = [\n",
    "    'model', 'tokenizer', 'id_to_label', 'num_classes', 'device', 'log', 'writer',\n",
    "    'dev_data_loader', # 使用原始验证集进行评估\n",
    "    'AUG_TRAIN_FILE_PATH', # 增强数据文件路径 (原TRAIN_ADD_FILE_PATH)\n",
    "    'CACHE_AUG_TRAIN_PATH', # 增强数据缓存路径 (新)\n",
    "    'BEST_MODEL_PATH',      # 初次微调的最佳模型\n",
    "    'BEST_AUG_MODEL_PATH',  # 二次微调模型保存路径 (新)\n",
    "    'AUG_LEARNING_RATE', 'AUG_NUM_EPOCHS', 'AUG_WARMUP_PROPORTION', 'AUG_WEIGHT_DECAY',\n",
    "    'BATCH_SIZE', 'NUM_WORKERS', 'MAX_SEQ_LENGTH', 'NUM_PROC_FOR_MAP',\n",
    "    'EVAL_STRATEGY', # 复用或定义 AUG_EVAL_STRATEGY\n",
    "]\n",
    "if EVAL_STRATEGY == \"steps\": # 如果按步验证，也需要频率参数\n",
    "    required_vars_for_aug_finetuning.append('EVAL_FREQUENCY_FRAC_EPOCH') # 复用或定义 AUG_EVAL_FREQUENCY_FRAC_EPOCH\n",
    "\n",
    "log.info(\"检查二次微调所需的前置变量...\")\n",
    "for var_name in required_vars_for_aug_finetuning:\n",
    "    if var_name not in locals() and var_name not in globals():\n",
    "        log.critical(f\"二次微调失败：关键前置变量 '{var_name}' 未定义。请检查之前的单元格和配置。\")\n",
    "        exit(1)\n",
    "log.info(\"所有二次微调所需的前置变量均已定义。\")\n",
    "\n",
    "# --- 12.2 加载数据增强文件 (Load Augmented Data File) ---\n",
    "log.info(f\"开始加载数据增强文件: {AUG_TRAIN_FILE_PATH}\")\n",
    "if not os.path.exists(AUG_TRAIN_FILE_PATH):\n",
    "    log.critical(f\"数据增强文件 {AUG_TRAIN_FILE_PATH} 不存在。无法进行二次微调。\")\n",
    "    exit(1)\n",
    "\n",
    "# 使用之前定义的 load_data_from_file 函数加载，格式为 \"text\\tlabel\"\n",
    "try:\n",
    "    raw_aug_train_data = load_data_from_file(AUG_TRAIN_FILE_PATH, is_test_set=False)\n",
    "    if not raw_aug_train_data:\n",
    "        log.critical(f\"从 {AUG_TRAIN_FILE_PATH} 加载的数据为空。无法进行二次微调。\")\n",
    "        exit(1)\n",
    "    log.info(f\"成功从 {AUG_TRAIN_FILE_PATH} 加载 {len(raw_aug_train_data)} 条增强训练数据。\")\n",
    "except Exception as e:\n",
    "    log.critical(f\"加载数据增强文件 {AUG_TRAIN_FILE_PATH} 失败: {e}\", exc_info=True)\n",
    "    exit(1)\n",
    "\n",
    "# --- 12.3 预处理增强数据 (Preprocess Augmented Data) ---\n",
    "# 使用与之前类似的预处理流程，但使用新的缓存路径\n",
    "log.info(\"开始预处理增强训练数据...\")\n",
    "aug_train_dataset = None\n",
    "try:\n",
    "    if os.path.exists(CACHE_AUG_TRAIN_PATH):\n",
    "        log.info(f\"发现预处理好的增强训练数据缓存，正在从磁盘加载: {CACHE_AUG_TRAIN_PATH}\")\n",
    "        aug_train_dataset = Dataset.load_from_disk(CACHE_AUG_TRAIN_PATH)\n",
    "        log.info(\"从磁盘加载预处理增强训练数据成功。\")\n",
    "    else:\n",
    "        log.info(f\"未找到增强训练数据缓存，正在重新处理: {CACHE_AUG_TRAIN_PATH}\")\n",
    "        aug_train_dataset = create_and_process_dataset( # 使用单元格7定义的函数\n",
    "            raw_data_list=raw_aug_train_data,\n",
    "            tokenizer_instance=tokenizer,\n",
    "            max_len=MAX_SEQ_LENGTH,\n",
    "            dataset_display_name=\"增强训练集\",\n",
    "            is_test_set=False,\n",
    "            label_map=label_to_id, # 使用原始的label_to_id映射\n",
    "            num_processing_workers=NUM_PROC_FOR_MAP\n",
    "        )\n",
    "        log.info(f\"保存预处理后的增强训练集到: {CACHE_AUG_TRAIN_PATH}\")\n",
    "        aug_train_dataset.save_to_disk(CACHE_AUG_TRAIN_PATH)\n",
    "        log.info(\"增强训练数据预处理完成并已保存到磁盘缓存。\")\n",
    "\n",
    "    # 设置Dataset格式为PyTorch Tensors\n",
    "    aug_train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    log.info(\"增强训练数据集格式已设置为 PyTorch Tensors。\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.critical(f\"预处理增强训练数据时发生严重错误: {e}\", exc_info=True)\n",
    "    exit(1)\n",
    "\n",
    "# --- 12.4 创建增强数据的 DataLoader (Create DataLoader for Augmented Data) ---\n",
    "log.info(\"创建增强训练数据的 DataLoader...\")\n",
    "pin_memory_enabled_aug = torch.cuda.is_available()\n",
    "try:\n",
    "    aug_train_data_loader = DataLoader(\n",
    "        dataset=aug_train_dataset,\n",
    "        batch_size=BATCH_SIZE, # 可以使用与初次训练相同的BATCH_SIZE，或单独配置\n",
    "        shuffle=True,\n",
    "        collate_fn=collator, # 复用之前创建的collator\n",
    "        #num_workers=NUM_WORKERS, \n",
    "        num_workers = 0,\n",
    "        pin_memory=pin_memory_enabled_aug\n",
    "    )\n",
    "    log.info(f\"增强训练数据 DataLoader 创建成功 (Batch Size: {BATCH_SIZE}, Num Workers: {0})。\")\n",
    "    if len(aug_train_data_loader) == 0:\n",
    "        log.critical(\"增强训练数据 DataLoader 为空，无法进行二次微调。\")\n",
    "        exit(1)\n",
    "except Exception as e:\n",
    "    log.critical(f\"创建增强训练数据 DataLoader 失败: {e}\", exc_info=True)\n",
    "    exit(1)\n",
    "\n",
    "# --- 12.5 加载已微调的模型 (Load Fine-tuned Model) ---\n",
    "log.info(f\"加载第一阶段微调后的最佳模型权重从: {BEST_MODEL_PATH}\")\n",
    "# model 对象应该仍然是之前训练/加载的那个实例，我们直接加载状态字典\n",
    "try:\n",
    "    #model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=device))\n",
    "    model.load_state_dict(torch.load(BEST_AUG_MODEL_PATH, map_location=device))\n",
    "    model.to(device) # 确保模型在正确的设备上\n",
    "    log.info(f\"成功加载来自 '{BEST_MODEL_PATH}' 的模型权重。\")\n",
    "except Exception as e:\n",
    "    log.critical(f\"加载模型权重 {BEST_MODEL_PATH} 失败: {e}\", exc_info=True)\n",
    "    exit(1)\n",
    "\n",
    "# --- 12.6 配置二次微调的优化器和学习率调度器 (Configure Optimizer & Scheduler for Augmentation Fine-tuning) ---\n",
    "log.info(\"为二次微调配置新的优化器和学习率调度器...\")\n",
    "log.info(f\"二次微调超参数: Learning Rate={AUG_LEARNING_RATE}, Epochs={AUG_NUM_EPOCHS}, \"\n",
    "         f\"Warmup Proportion={AUG_WARMUP_PROPORTION}, Weight Decay={AUG_WEIGHT_DECAY}\")\n",
    "try:\n",
    "    aug_optimizer = AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=AUG_LEARNING_RATE,\n",
    "        weight_decay=AUG_WEIGHT_DECAY,\n",
    "        eps=1e-8\n",
    "    )\n",
    "    log.info(\"二次微调 AdamW 优化器创建成功。\")\n",
    "\n",
    "    aug_num_training_steps = AUG_NUM_EPOCHS * len(aug_train_data_loader)\n",
    "    aug_num_warmup_steps = int(AUG_WARMUP_PROPORTION * aug_num_training_steps)\n",
    "    if aug_num_training_steps == 0:\n",
    "        log.critical(\"二次微调的总训练步数为0，无法创建学习率调度器。\")\n",
    "        exit(1)\n",
    "\n",
    "    log.info(f\"二次微调总训练步数: {aug_num_training_steps}, 预热步数: {aug_num_warmup_steps}\")\n",
    "    aug_lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer=aug_optimizer,\n",
    "        num_warmup_steps=aug_num_warmup_steps,\n",
    "        num_training_steps=aug_num_training_steps\n",
    "    )\n",
    "    log.info(\"二次微调学习率调度器创建成功。\")\n",
    "except Exception as e:\n",
    "    log.critical(f\"创建二次微调优化器或学习率调度器失败: {e}\", exc_info=True)\n",
    "    exit(1)\n",
    "\n",
    "# --- 12.7 执行二次微调训练与验证循环 (Execute Second Stage Fine-tuning Loop) ---\n",
    "log.info(\"开始二次微调训练与验证循环...\")\n",
    "\n",
    "best_aug_val_metric_for_saving = -float('inf') # 用于保存二次微调模型的最佳指标值\n",
    "current_best_aug_model_info = {\"epoch\": 0, \"step\":0, \"val_accuracy\": 0.0, \"val_f1\": 0.0, \"val_loss\": float('inf')}\n",
    "global_aug_step = 0 # 用于二次微调的 TensorBoard 全局步数\n",
    "\n",
    "# 根据 EVAL_STRATEGY 计算验证频率 (如果使用基于steps的验证)\n",
    "aug_eval_every_n_steps = 0\n",
    "# current_eval_strategy = getattr('AUG_EVAL_STRATEGY', EVAL_STRATEGY) # 允许为增强训练单独配置策略\n",
    "# current_eval_freq_frac = getattr('AUG_EVAL_FREQUENCY_FRAC_EPOCH', EVAL_FREQUENCY_FRAC_EPOCH)\n",
    "current_eval_strategy = globals().get('AUG_EVAL_STRATEGY', EVAL_STRATEGY)\n",
    "current_eval_freq_frac = globals().get('AUG_EVAL_FREQUENCY_FRAC_EPOCH', EVAL_FREQUENCY_FRAC_EPOCH)\n",
    "if current_eval_strategy == \"steps\":\n",
    "    if not (0 < current_eval_freq_frac <= 1.0):\n",
    "        log.warning(f\"二次微调的 EVAL_FREQUENCY_FRAC_EPOCH ({current_eval_freq_frac}) 无效，将按epoch结束时验证。\")\n",
    "        current_eval_strategy = \"epoch\"\n",
    "    else:\n",
    "        aug_eval_every_n_steps = max(1, int(len(aug_train_data_loader) * current_eval_freq_frac))\n",
    "        log.info(f\"二次微调按步数验证：每 {aug_eval_every_n_steps} 步 (约占epoch的 {current_eval_freq_frac*100:.1f}%) 进行一次验证。\")\n",
    "else:\n",
    "    log.info(\"二次微调按Epoch结束验证策略。\")\n",
    "\n",
    "\n",
    "global_aug_start_time = time.time()\n",
    "training_aug_completed_normally = False\n",
    "\n",
    "try:\n",
    "    for epoch in range(1, AUG_NUM_EPOCHS + 1):\n",
    "        epoch_aug_start_time = time.time()\n",
    "        log.info(f\"--- 二次微调 Epoch {epoch}/{AUG_NUM_EPOCHS} ---\")\n",
    "        model.train()\n",
    "        total_train_loss_aug_epoch = 0\n",
    "        aug_train_progress_bar = tqdm(aug_train_data_loader, desc=f\"Aug Epoch {epoch} Training\", leave=True, position=0)\n",
    "\n",
    "        for step, batch in enumerate(aug_train_progress_bar):\n",
    "            global_aug_step += 1\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            aug_optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # (可选) 梯度裁剪\n",
    "            aug_optimizer.step()\n",
    "            aug_lr_scheduler.step()\n",
    "\n",
    "            total_train_loss_aug_epoch += loss.item()\n",
    "            current_aug_lr = aug_optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "            aug_train_progress_bar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'avg_loss_epoch': f'{total_train_loss_aug_epoch / (step + 1):.4f}',\n",
    "                'lr': f'{current_aug_lr:.2e}'\n",
    "            })\n",
    "\n",
    "            if global_aug_step % (max(1, len(aug_train_data_loader) // 100)) == 0:\n",
    "                writer.add_scalar('Loss/AugTrain_step', loss.item(), global_aug_step)\n",
    "                writer.add_scalar('LearningRate/Aug_step', current_aug_lr, global_aug_step)\n",
    "\n",
    "            # --- 定期验证 (二次微调期间) ---\n",
    "            if current_eval_strategy == \"steps\" and (step + 1) % aug_eval_every_n_steps == 0:\n",
    "                step_info = f\"Aug Epoch {epoch}, Step {step+1}/{len(aug_train_data_loader)}\"\n",
    "                # 使用原始的 dev_data_loader 进行验证\n",
    "                val_loss, val_accuracy, val_f1_weighted = evaluate_model(model, dev_data_loader, device, step_info)\n",
    "                model.train() # 确保验证后模型回到训练模式\n",
    "\n",
    "                writer.add_scalar('Loss/AugValidation_step', val_loss, global_aug_step)\n",
    "                if val_accuracy != -1.0: writer.add_scalar('Accuracy/AugValidation_step', val_accuracy, global_aug_step)\n",
    "                if val_f1_weighted != -1.0: writer.add_scalar('F1_weighted/AugValidation_step', val_f1_weighted, global_aug_step)\n",
    "\n",
    "                if val_accuracy > best_aug_val_metric_for_saving:\n",
    "                    best_aug_val_metric_for_saving = val_accuracy\n",
    "                    current_best_aug_model_info = {\"epoch\": epoch, \"step\":step+1, \"val_accuracy\": val_accuracy, \"val_f1\": val_f1_weighted, \"val_loss\": val_loss}\n",
    "                    log.info(f\"🎉 新的最佳增强模型 (基于Accuracy)! Accuracy: {val_accuracy:.4f} at {step_info}. 保存模型到 {BEST_AUG_MODEL_PATH}\")\n",
    "                    torch.save(model.state_dict(), BEST_AUG_MODEL_PATH)\n",
    "                else:\n",
    "                    log.info(f\"当前验证 Accuracy {val_accuracy:.4f} 未超过二次微调历史最佳 {best_aug_val_metric_for_saving:.4f} (at {step_info}).\")\n",
    "\n",
    "        aug_train_progress_bar.close()\n",
    "        avg_epoch_aug_train_loss = total_train_loss_aug_epoch / len(aug_train_data_loader) if len(aug_train_data_loader) > 0 else float('inf')\n",
    "        log.info(f\"二次微调 Epoch {epoch} 训练阶段完成. 平均训练损失: {avg_epoch_aug_train_loss:.4f}\")\n",
    "        writer.add_scalar('Loss/AugTrain_epoch', avg_epoch_aug_train_loss, epoch)\n",
    "\n",
    "\n",
    "        # --- Epoch结束时验证 (二次微调期间) ---\n",
    "        perform_aug_epoch_end_eval = (current_eval_strategy == \"epoch\") or \\\n",
    "                                     (current_eval_strategy == \"steps\" and len(aug_train_data_loader) % aug_eval_every_n_steps != 0)\n",
    "\n",
    "        if perform_aug_epoch_end_eval:\n",
    "            epoch_end_info = f\"Aug Epoch {epoch} End\"\n",
    "            val_loss, val_accuracy, val_f1_weighted = evaluate_model(model, dev_data_loader, device, epoch_end_info)\n",
    "\n",
    "            writer.add_scalar('Loss/AugValidation_epoch', val_loss, epoch)\n",
    "            if val_accuracy != -1.0: writer.add_scalar('Accuracy/AugValidation_epoch', val_accuracy, epoch)\n",
    "            if val_f1_weighted != -1.0: writer.add_scalar('F1_weighted/AugValidation_epoch', val_f1_weighted, epoch)\n",
    "\n",
    "            if val_accuracy > best_aug_val_metric_for_saving:\n",
    "                best_aug_val_metric_for_saving = val_accuracy\n",
    "                current_best_aug_model_info = {\"epoch\": epoch, \"step\":\"end_of_epoch\", \"val_accuracy\": val_accuracy, \"val_f1\": val_f1_weighted, \"val_loss\": val_loss}\n",
    "                log.info(f\"🎉 新的最佳增强模型 (基于Accuracy)! Accuracy: {val_accuracy:.4f} at {epoch_end_info}. 保存模型到 {BEST_AUG_MODEL_PATH}\")\n",
    "                torch.save(model.state_dict(), BEST_AUG_MODEL_PATH)\n",
    "            else:\n",
    "                log.info(f\"当前验证 Accuracy {val_accuracy:.4f} 未超过二次微调历史最佳 {best_aug_val_metric_for_saving:.4f} (at {epoch_end_info}).\")\n",
    "\n",
    "        epoch_aug_duration = time.time() - epoch_aug_start_time\n",
    "        log.info(f\"二次微调 Epoch {epoch} 总耗时: {epoch_aug_duration:.2f} 秒\")\n",
    "        writer.add_scalar('Time/Aug_epoch_duration_seconds', epoch_aug_duration, epoch)\n",
    "\n",
    "    training_aug_completed_normally = True\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    log.warning(\"二次微调被用户手动中断 (KeyboardInterrupt)。\")\n",
    "    training_aug_completed_normally = False\n",
    "except Exception as e_train_aug:\n",
    "    log.critical(f\"二次微调过程中发生严重错误: {e_train_aug}\", exc_info=True)\n",
    "    training_aug_completed_normally = False\n",
    "finally:\n",
    "    if training_aug_completed_normally:\n",
    "        log.info(\"所有计划的二次微调 Epoch 训练与验证完成。\")\n",
    "    else:\n",
    "        log.info(\"二次微调训练未完成所有计划的 Epoch。\")\n",
    "\n",
    "    log.info(f\"二次微调期间记录的最佳模型信息: Epoch={current_best_aug_model_info['epoch']}, Step={current_best_aug_model_info['step']}, \"\n",
    "             f\"Val Accuracy={current_best_aug_model_info['val_accuracy']:.4f}, \"\n",
    "             f\"Val F1={current_best_aug_model_info['val_f1']:.4f}, \"\n",
    "             f\"Val Loss={current_best_aug_model_info['val_loss']:.4f}\")\n",
    "    if os.path.exists(BEST_AUG_MODEL_PATH) and best_aug_val_metric_for_saving > -float('inf'):\n",
    "        log.info(f\"最佳二次微调模型参数已保存至: {BEST_AUG_MODEL_PATH} (对应验证准确率: {best_aug_val_metric_for_saving:.4f})\")\n",
    "    else:\n",
    "        log.warning(f\"未成功保存任何最佳二次微调模型到 {BEST_AUG_MODEL_PATH}。\")\n",
    "\n",
    "    total_aug_training_time = time.time() - global_aug_start_time\n",
    "    hours, rem = divmod(total_aug_training_time, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    log.info(f\"总二次微调耗时: {int(hours):02d}小时 {int(minutes):02d}分钟 {seconds:.2f}秒\")\n",
    "\n",
    "log.info(\"=\"*30 + \" 使用增强数据进行二次微调结束 \" + \"=\"*30 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4637702a-c445-4f7f-9921-bfd30b23907c",
   "metadata": {},
   "source": [
    "## 最终测试集预测与结果分析(Final Test Set Prediction & Result Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5ee805-9f3d-453c-a4a0-d530ca410b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 22:20:43,706 - __main__ - INFO - ============================== 开始最终测试集预测与结果分析 (使用二次微调模型) ==============================\n",
      "2025-05-18 22:20:43,707 - __main__ - INFO - 检查最终预测与分析所需的前置变量...\n",
      "2025-05-18 22:20:43,708 - __main__ - INFO - 所有最终预测分析所需的前置变量均已定义。\n",
      "2025-05-18 22:20:43,708 - __main__ - INFO - 加载二次微调后的最佳模型权重从: ./files/saved_models/best_augmented_model.pt\n",
      "/tmp/ipykernel_16737/1463040675.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(BEST_AUG_MODEL_PATH, map_location=device))\n",
      "2025-05-18 22:20:44,468 - __main__ - INFO - 二次微调后的最佳模型权重从 ./files/saved_models/best_augmented_model.pt 加载成功。\n",
      "2025-05-18 22:20:44,479 - __main__ - INFO - 开始对所有测试数据进行最终 Top-1 预测 (使用二次微调模型)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27b8b8ed11fa4308b16cf352f9831cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Final Test Prediction (Top-1):   0%|          | 0/327 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 22:22:14,496 - __main__ - INFO - 最终 Top-1 预测处理完成，共获得 83599 条预测结果。\n",
      "2025-05-18 22:22:14,934 - __main__ - INFO - 最终测试集 Top-1 预测结果已保存至 JSON: ./files/results/final_test_top1_predictions.json\n",
      "2025-05-18 22:22:14,935 - __main__ - INFO - JSON结果示例 (第一条):\n",
      "2025-05-18 22:22:14,935 - __main__ - INFO - {\n",
      "  \"original_id\": 0,\n",
      "  \"text\": \"北京君太百货璀璨秋色 满100省353020元\",\n",
      "  \"predicted_label\": \"房产\",\n",
      "  \"probability\": 0.996\n",
      "}\n",
      "2025-05-18 22:22:14,968 - __main__ - INFO - 成功将 83599 条预测标签写入纯文本文件: ./files/results/result.txt\n",
      "2025-05-18 22:22:14,968 - __main__ - INFO - result.txt 内容示例 (前3行，如果存在):\n",
      "2025-05-18 22:22:14,969 - __main__ - INFO -   房产\n",
      "2025-05-18 22:22:14,969 - __main__ - INFO -   时政\n",
      "2025-05-18 22:22:14,969 - __main__ - INFO -   科技\n",
      "2025-05-18 22:22:14,970 - __main__ - INFO - 开始统计和绘制最终 Top-1 预测概率分布图...\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 主要功能:\n",
    "# 1. 检查前置条件，加载二次微调后的最佳模型权重。\n",
    "# 2. 对整个测试集执行预测，获取每个样本的 top-1 预测标签及其概率。\n",
    "# 3. 结果保存:\n",
    "#    a. Top-1 预测结果 (原始ID, 文本, 预测标签, 概率) 保存为 JSON 文件。\n",
    "#    b. 仅包含预测标签的文本文件 (result.txt)。\n",
    "# 4. 统计预测概率分布 (与单元格11类似，但使用最终模型的预测结果):\n",
    "#    a. 绘制并保存概率分布柱状图。\n",
    "#    b. 将概率分布的具体数值保存为 CSV 文件。\n",
    "# ==============================================================================\n",
    "log.info(\"=\"*30 + \" 开始最终测试集预测与结果分析 (使用二次微调模型) \" + \"=\"*30)\n",
    "\n",
    "# --- 13.1 检查前置条件与配置加载 (Check Prerequisites & Load Configs) ---\n",
    "required_vars_for_final_prediction = [\n",
    "     'id_to_label', 'log', 'device', 'writer',\n",
    "    'test_data_loader', 'raw_test_data',\n",
    "    'BEST_AUG_MODEL_PATH', # 二次微调后的最佳模型路径\n",
    "    'FINAL_RESULT_TXT_PATH',\n",
    "    'FINAL_PROB_DIST_CSV_PATH', # 最终分析图表和CSV保存目录\n",
    "    'FINAL_PROB_DIST_PLOT_PATH',\n",
    "    'PROB_DIST_PLOT_STEP'  # 概率分布图的分度值\n",
    "]\n",
    "log.info(\"检查最终预测与分析所需的前置变量...\")\n",
    "for var_name in required_vars_for_final_prediction:\n",
    "    if var_name not in locals() and var_name not in globals():\n",
    "        log.critical(f\"最终预测分析失败：关键前置变量 '{var_name}' 未定义。请检查之前的单元格。\")\n",
    "        exit(1)\n",
    "\n",
    "if test_data_loader is None or len(test_data_loader) == 0:\n",
    "    log.critical(\"测试数据加载器 (test_data_loader) 为空或未定义，无法进行最终预测。\")\n",
    "    exit(1)\n",
    "if not raw_test_data:\n",
    "    log.critical(\"原始测试数据 (raw_test_data) 为空，无法获取文本内容。\")\n",
    "    exit(1)\n",
    "\n",
    "log.info(\"所有最终预测分析所需的前置变量均已定义。\")\n",
    "\n",
    "\n",
    "\n",
    "# --- 13.2 加载二次微调后的最佳模型权重 (Load Best Second Stage Fine-tuned Model Weights) ---\n",
    "log.info(f\"加载二次微调后的最佳模型权重从: {BEST_AUG_MODEL_PATH}\")\n",
    "if not os.path.exists(BEST_AUG_MODEL_PATH):\n",
    "    log.warning(f\"二次微调后的最佳模型文件 {BEST_AUG_MODEL_PATH} 不存在。\")\n",
    "    log.warning(f\"将尝试使用当前内存中的 'model' 对象进行预测（可能是二次微调的最后状态）。\")\n",
    "    log.warning(f\"如果希望使用磁盘上特定的最佳增强模型，请确保 {BEST_AUG_MODEL_PATH} 存在。\")\n",
    "\n",
    "    if model is None: #\n",
    "        log.critical(\"当前 'model'为 None，且无法从磁盘加载二次微调模型。无法进行预测。\")\n",
    "        exit(1)\n",
    "    model.to(device) \n",
    "    log.info(\"已准备使用当前内存中的模型进行预测。\")\n",
    "else: # BEST_AUG_MODEL_PATH 存在\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(BEST_AUG_MODEL_PATH, map_location=device))\n",
    "        model.to(device)\n",
    "        log.info(f\"二次微调后的最佳模型权重从 {BEST_AUG_MODEL_PATH} 加载成功。\")\n",
    "    except Exception as e:\n",
    "        log.critical(f\"加载二次微调模型权重 {BEST_AUG_MODEL_PATH} 失败: {e}\", exc_info=True)\n",
    "        exit(1)\n",
    "\n",
    "# --- 13.3 执行全量预测并收集 Top-1 结果 (Perform Full Prediction & Collect Top-1 Results) ---\n",
    "model.eval() \n",
    "final_all_top1_results = [] \n",
    "\n",
    "log.info(\"开始对所有测试数据进行最终 Top-1 预测 (使用二次微调模型)...\")\n",
    "final_predict_progress_bar = tqdm(test_data_loader, desc=\"Final Test Prediction (Top-1)\", leave=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(final_predict_progress_bar):\n",
    "        if 'id' not in batch:\n",
    "            log.warning(f\"最终预测批次 {batch_idx} 中缺少 'id' 键，跳过此批次中的部分数据关联。\")\n",
    "            # 对于最终预测，原始ID对于结果的完整性很重要\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        top1_probs, top1_indices = torch.max(probabilities, dim=-1)\n",
    "        \n",
    "        # 获取原始ID，如果 'id' 不在batch中，则生成一个基于batch索引的伪ID（不推荐用于真实场景）\n",
    "        batch_original_ids = batch.get('id')\n",
    "        if batch_original_ids is None:\n",
    "            log.error(f\"最终预测批次 {batch_idx} 中 'id' 键缺失\")\n",
    "            exit(1)\n",
    "        else:\n",
    "            batch_original_ids_list = batch_original_ids.cpu().tolist()\n",
    "\n",
    "        for i in range(top1_probs.size(0)):\n",
    "            original_id = batch_original_ids_list[i]\n",
    "            prob_val = top1_probs[i].item()\n",
    "            label_idx = top1_indices[i].item()\n",
    "            predicted_label_text = id_to_label.get(label_idx, f\"未知标签ID:{label_idx}\")\n",
    "\n",
    "            original_text = \"原始文本不可用\" # 默认值\n",
    "            try:\n",
    "                # 确保 original_id 是有效的 raw_test_data 索引\n",
    "                # 这个 original_id 应该与 raw_test_data 创建时的id对应\n",
    "                if 0 <= original_id < len(raw_test_data) and isinstance(raw_test_data[original_id], dict) and 'text' in raw_test_data[original_id]:\n",
    "                     original_text = raw_test_data[original_id]['text']\n",
    "                else:\n",
    "                    # 尝试通过查找匹配的 'id' 字段（如果 raw_test_data 中的 id 不是简单索引）\n",
    "                    found_text = False\n",
    "                    for item_idx, item in enumerate(raw_test_data): # 效率较低，但作为后备\n",
    "                        if isinstance(item, dict) and item.get('id') == original_id:\n",
    "                            original_text = item['text']\n",
    "                            found_text = True\n",
    "                            break\n",
    "                    if not found_text:\n",
    "                        log.warning(f\"无法通过 original_id {original_id} (来自DataLoader) 在 raw_test_data 中找到对应文本。\")\n",
    "            except Exception as e_text:\n",
    "                log.warning(f\"通过 original_id {original_id} 获取原始文本时发生错误: {e_text}\")\n",
    "            \n",
    "            final_all_top1_results.append({\n",
    "                \"original_id\": original_id,\n",
    "                \"text\": original_text,\n",
    "                \"predicted_label\": predicted_label_text,\n",
    "                \"probability\": round(prob_val, 3) # 保留三位小数\n",
    "            })\n",
    "final_predict_progress_bar.close()\n",
    "log.info(f\"最终 Top-1 预测处理完成，共获得 {len(final_all_top1_results)} 条预测结果。\")\n",
    "\n",
    "if not final_all_top1_results:\n",
    "    log.critical(\"未能生成任何最终 Top-1 预测结果，后续分析无法进行。\")\n",
    "    exit(1)\n",
    "\n",
    "# --- 13.4 保存最终预测结果 (Save Final Prediction Results) ---\n",
    "# 13.4.1 保存 Top-1 预测 JSON 结果\n",
    "try:\n",
    "    with open(FINAL_TOP1_JSON_PATH, 'w', encoding='utf-8') as f:\n",
    "        json.dump(final_all_top1_results, f, ensure_ascii=False, indent=2)\n",
    "    log.info(f\"最终测试集 Top-1 预测结果已保存至 JSON: {FINAL_TOP1_JSON_PATH}\")\n",
    "    if final_all_top1_results: # 打印示例\n",
    "        log.info(\"JSON结果示例 (第一条):\")\n",
    "        log.info(json.dumps(final_all_top1_results[0], ensure_ascii=False, indent=2))\n",
    "except Exception as e:\n",
    "    log.error(f\"保存最终 Top-1 预测 JSON 结果时发生错误: {e}\", exc_info=True)\n",
    "\n",
    "# 13.4.2 保存仅含标签的 result.txt\n",
    "try:\n",
    "    count_written_txt = 0\n",
    "    with open(FINAL_RESULT_TXT_PATH, 'w', encoding='utf-8') as f_txt:\n",
    "        for result_item in final_all_top1_results:\n",
    "            f_txt.write(f\"{result_item['predicted_label']}\\n\")\n",
    "            count_written_txt +=1\n",
    "    log.info(f\"成功将 {count_written_txt} 条预测标签写入纯文本文件: {FINAL_RESULT_TXT_PATH}\")\n",
    "    if count_written_txt > 0:\n",
    "        log.info(\"result.txt 内容示例 (前3行，如果存在):\")\n",
    "        with open(FINAL_RESULT_TXT_PATH, 'r', encoding='utf-8') as f_preview:\n",
    "            for i, line in enumerate(f_preview):\n",
    "                if i < 3:\n",
    "                    log.info(f\"  {line.strip()}\")\n",
    "                else:\n",
    "                    break\n",
    "except Exception as e:\n",
    "    log.error(f\"保存 result.txt 文件时发生错误: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "# --- 13.5 统计并绘制最终概率分布图 (Analyze and Plot Final Probability Distribution) ---\n",
    "log.info(\"开始统计和绘制最终 Top-1 预测概率分布图...\")\n",
    "final_top1_probabilities = np.array([res['probability'] for res in final_all_top1_results])\n",
    "\n",
    "if len(final_top1_probabilities) == 0:\n",
    "    log.warning(\"没有最终 Top-1 概率数据可供分析，跳过概率分布图绘制。\")\n",
    "else:\n",
    "    min_prob_final = np.min(final_top1_probabilities) if len(final_top1_probabilities) > 0 else 0.0\n",
    "    final_prob_thresholds = np.arange(1.0, min_prob_final - PROB_DIST_PLOT_STEP, -PROB_DIST_PLOT_STEP)\n",
    "    final_prob_thresholds = np.clip(final_prob_thresholds, 0.0, 1.0)\n",
    "    final_prob_thresholds = np.unique(final_prob_thresholds)[::-1]\n",
    "    if final_prob_thresholds[0] < 1.0:\n",
    "        final_prob_thresholds = np.insert(final_prob_thresholds, 0, 1.0)\n",
    "\n",
    "    final_sample_proportions = []\n",
    "    for threshold in final_prob_thresholds:\n",
    "        count_above_threshold = np.sum(final_top1_probabilities >= threshold)\n",
    "        proportion = count_above_threshold / len(final_top1_probabilities) if len(final_top1_probabilities) > 0 else 0\n",
    "        final_sample_proportions.append(proportion)\n",
    "\n",
    "    # 绘制柱状图\n",
    "    try:\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        bar_positions_final = np.arange(len(final_prob_thresholds))\n",
    "        plt.bar(bar_positions_final, final_sample_proportions, width=0.8, color='mediumseagreen') # 不同颜色\n",
    "\n",
    "        plt.xlabel(\"Top-1 Prediction Probability Threshold (P) - Final Model\")\n",
    "        plt.ylabel(f\"Proportion of Samples with Top-1 Prob >= P (Total Samples: {len(final_top1_probabilities)})\")\n",
    "        plt.title(\"Distribution of Top-1 Prediction Probabilities on Test Set (Final Fine-tuned Model)\")\n",
    "        \n",
    "        tick_indices_final = np.linspace(0, len(final_prob_thresholds) - 1, num=min(15, len(final_prob_thresholds)), dtype=int)\n",
    "        plt.xticks(bar_positions_final[tick_indices_final], [f\"{final_prob_thresholds[i]:.3f}\" for i in tick_indices_final], rotation=45, ha=\"right\")\n",
    "        \n",
    "        plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.grid(axis='y', linestyle='--')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(FINAL_PROB_DIST_PLOT_PATH)\n",
    "        log.info(f\"最终概率分布柱状图已保存至: {FINAL_PROB_DIST_PLOT_PATH}\")\n",
    "        plt.close()\n",
    "\n",
    "        # 将图像记录到TensorBoard\n",
    "        try:\n",
    "            image = Image.open(FINAL_PROB_DIST_PLOT_PATH)\n",
    "            image_tensor_final = torch.tensor(np.array(image)).permute(2,0,1)\n",
    "            writer.add_image('Analysis/FinalProbabilityDistribution', image_tensor_final, global_step=1) # global_step可以设为1或与之前区分\n",
    "            log.info(\"最终概率分布图已尝试记录到 TensorBoard。\")\n",
    "        except Exception as e_tb_img_final:\n",
    "            log.warning(f\"记录最终概率分布图到 TensorBoard 失败: {e_tb_img_final}\")\n",
    "\n",
    "    except Exception as e_plot_final:\n",
    "        log.error(f\"绘制或保存最终概率分布图时发生错误: {e_plot_final}\", exc_info=True)\n",
    "\n",
    "    # 保存最终概率分布数据到CSV\n",
    "    try:\n",
    "        with open(FINAL_PROB_DIST_CSV_PATH, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            csv_writer = csv.writer(csvfile)\n",
    "            csv_writer.writerow(['Probability_Threshold', 'Proportion_Samples_Above_Threshold'])\n",
    "            for threshold, proportion in zip(final_prob_thresholds, final_sample_proportions):\n",
    "                csv_writer.writerow([f\"{threshold:.3f}\", f\"{proportion:.4f}\"])\n",
    "        log.info(f\"最终概率分布数据已保存至 CSV: {FINAL_PROB_DIST_CSV_PATH}\")\n",
    "    except Exception as e_csv_final:\n",
    "        log.error(f\"保存最终概率分布数据到 CSV 时发生错误: {e_csv_final}\", exc_info=True)\n",
    "\n",
    "log.info(\"=\"*30 + \" 最终测试集预测与结果分析结束 \" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d11e40c-77bb-41e1-92cf-cce9fb2891b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpu_SDT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
